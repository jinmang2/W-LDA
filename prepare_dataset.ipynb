{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "- [wikitext-103 dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n",
    "- wikitest 이름 바꾸기 ㅡㅡ 뒤질라고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_create_dir(dir):\n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir)\n",
    "    os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory:  C:\\workspace\\W-LDA\\data\\wikitest-103\n"
     ]
    }
   ],
   "source": [
    "# create directory for data\n",
    "dataset = \"wikitest-103\"\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "data_dir = os.path.join(current_dir, \"data\")\n",
    "if not os.path.exists(data_dir):\n",
    "    print('Creating directory:', data_dir)\n",
    "    os.mkdir(data_dir)\n",
    "data_dir = os.path.join(current_dir, \"data\", dataset)\n",
    "check_create_dir(data_dir)\n",
    "os.chdir(data_dir)\n",
    "print('Current directory: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse into documents\n",
    "def is_document_start(line):\n",
    "    if len(line) < 4:\n",
    "        return False\n",
    "    if line[0] is '=' and line[-1] is '=':\n",
    "        if line[2] is not '=':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def token_list_per_doc(input_dir, token_file):\n",
    "    lines_list = []\n",
    "    line_prev = ''\n",
    "    prev_line_start_doc = False\n",
    "    with open(os.path.join(input_dir, token_file), 'r', encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            line = l.strip()\n",
    "            if prev_line_start_doc and line:\n",
    "                # the previous line should not have been start of a document!\n",
    "                lines_list.pop()\n",
    "                lines_list[-1] = lines_list[-1] + ' ' + line_prev\n",
    "\n",
    "            if line:\n",
    "                if is_document_start(line) and not line_prev:\n",
    "                    lines_list.append(line)\n",
    "                    prev_line_start_doc = True\n",
    "                else:\n",
    "                    lines_list[-1] = lines_list[-1] + ' ' + line\n",
    "                    prev_line_start_doc = False\n",
    "            else:\n",
    "                prev_line_start_doc = False\n",
    "            line_prev = line\n",
    "\n",
    "    print(\"{} documents parsed!\".format(len(lines_list)))\n",
    "    return lines_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28472 documents parsed!\n",
      "60 documents parsed!\n",
      "60 documents parsed!\n"
     ]
    }
   ],
   "source": [
    "input_dir = os.path.join(data_dir, dataset)\n",
    "train_file = 'wiki.train.tokens'\n",
    "val_file = 'wiki.valid.tokens'\n",
    "test_file = 'wiki.test.tokens'\n",
    "train_doc_list = token_list_per_doc(input_dir, train_file)\n",
    "val_doc_list = token_list_per_doc(input_dir, val_file)\n",
    "test_doc_list = token_list_per_doc(input_dir, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jinma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [\n",
    "            self.wnl.lemmatize(t) for t in doc.split() \n",
    "            if (len(t) >= 2 and re.match(\"[a-z].*\", t) and \n",
    "                re.match(token_pattern, t))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing and counting, this may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 20000\n",
      "Done. Time elapsed: 401.46s\n"
     ]
    }
   ],
   "source": [
    "print('Lemmatizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', \n",
    "                             stop_words='english', tokenizer=LemmaTokenizer(), \n",
    "                             max_df=0.8, min_df=3, max_features=20000)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_doc_list)\n",
    "val_vectors = vectorizer.transform(val_doc_list)\n",
    "test_vectors = vectorizer.transform(test_doc_list)\n",
    "\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "vocab_size = len(vocab_list)\n",
    "print('vocab size:', vocab_size)\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_dtype(vectors):\n",
    "    idx = np.arange(vectors.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    vectors = vectors[idx]\n",
    "    vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "    print(type(vectors), vectors.dtype)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n",
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n",
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "train_vectors = shuffle_and_dtype(train_vectors)\n",
    "val_vectors = shuffle_and_dtype(val_vectors)\n",
    "test_vectors = shuffle_and_dtype(test_vectors)\n",
    "\n",
    "with open('vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in vocab_list:\n",
    "        f.write(item+'\\n')\n",
    "\n",
    "sparse.save_npz('wikitext-103_tra.csr.npz', train_vectors)\n",
    "sparse.save_npz('wikitext-103_val.csr.npz', val_vectors)\n",
    "sparse.save_npz('wikitext-103_test.csr.npz', test_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
