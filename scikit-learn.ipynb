{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    _token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    @property\n",
    "    def token_pattern(self):\n",
    "        return self._token_pattern\n",
    "\n",
    "    @token_pattern.setter\n",
    "    def token_pattern(self, s):\n",
    "        if isinstance(s, str):\n",
    "            self._token_pattern = re.compile(s)\n",
    "        elif isinstance(s, type(re.compile(\"\"))):\n",
    "            self._token_pattern = s\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [\n",
    "            self.wnl.lemmatize(t) for t in doc.split() \n",
    "            if (len(t) >= 2 and \n",
    "                re.match(\"[a-z].*\", t) and \n",
    "                re.match(self.token_pattern, t))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jinma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (data\\wikitext\\wikitext-103-v1\\1.0.0\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af)\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\n",
    "    path=\"./wlda/wikitext.py\", name=\"wikitext-103-v1\", cache_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(\n",
    "    input='content', analyzer='word', stop_words='english',\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    max_df=0.8, min_df=3, max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\jinma\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.8, max_features=20000, min_df=3, stop_words='english',\n",
       "                tokenizer=<__main__.LemmaTokenizer object at 0x000001AF59DA63C8>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x20000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 743 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([docs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('i')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca, indptr, indices = [], [], []\n",
    "values = array.array(str(\"i\")) # signed integer\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2535"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2=CountVectorizer(\n",
    "    input='content', analyzer='word', stop_words='english',\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    max_df=0.8, min_df=3, max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `validate_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.ngram_range # min_n, max_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `validate_vocabulary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. setting vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 3)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.max_df, vectorizer2.min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<method-wrapper '__len__' of collections.defaultdict object at 0x000001AFAA35EC28>,\n",
       "            {})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = defaultdict()\n",
    "vocabulary.default_factory = vocabulary.__len__\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "analzer = \"word\"\n",
    "# http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n",
    "stop_words = ENGLISH_STOP_WORDS\n",
    "tokenize = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_accents = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def _preprocess(doc, accent_function, lower):\n",
    "    if lower:\n",
    "        doc = doc.lower()\n",
    "    if accent_function is not None:\n",
    "        doc = accent_function(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "preprocess = partial(_preprocess, accent_function=strip_accents, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistent = set()\n",
    "for w in stop_words:\n",
    "    tokens = list(tokenize(preprocess(w)))\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            inconsistent.add(token)\n",
    "_stop_words_id = id(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not inconsistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze(doc, analyzer=None, tokenizer=None, ngrams=None,\n",
    "             preprocessor=None, decoder=None, stop_words=None):\n",
    "    if decoder is not None:\n",
    "        doc = decoder(doc)\n",
    "    if analyzer is not None:\n",
    "        doc = analyzer(doc)\n",
    "    else:\n",
    "        if preprocessor is not None:\n",
    "            doc = preprocessor(doc)\n",
    "        if tokenizer is not None:\n",
    "            doc = tokenizer(doc)\n",
    "        if ngrams is not None:\n",
    "            if stop_words is not None:\n",
    "                doc = ngrams(doc, stop_words)\n",
    "            else:\n",
    "                doc = ngrams(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'you']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"i like you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'favorite', 'food', 'is', 'sea', 'food']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'favorite',\n",
       " 'food',\n",
       " 'sea',\n",
       " 'food',\n",
       " 'my favorite',\n",
       " 'favorite food',\n",
       " 'food sea',\n",
       " 'sea food',\n",
       " 'my favorite food',\n",
       " 'favorite food sea',\n",
       " 'food sea food',\n",
       " 'my favorite food sea',\n",
       " 'favorite food sea food',\n",
       " 'my favorite food sea food']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"my favorite food is sea food\")\n",
    "print(tokens)\n",
    "_word_ngrams(tokens, stop_words=[\"is\"], ngram_range=(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _word_ngrams(tokens, stop_words=None, ngram_range=(1,1)):\n",
    "    if stop_words is not None:\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "    # handle token n-grams\n",
    "    min_n, max_n = ngram_range\n",
    "    if max_n != 1:\n",
    "        original_tokens = tokens\n",
    "        if min_n == 1:\n",
    "            tokens = list(original_tokens)\n",
    "            min_n += 1\n",
    "        else:\n",
    "            tokens = []\n",
    "        n_original_tokens = len(original_tokens)\n",
    "        \n",
    "        # bind method outside of loop to reduce overhead\n",
    "        tokens_append = tokens.append\n",
    "        space_join = \" \".join\n",
    "        \n",
    "        for n in range(min_n, min(max_n+1, n_original_tokens+1)):\n",
    "            for i in range(n_original_tokens-n+1):\n",
    "                tokens_append(space_join(original_tokens[i:i+n]))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strict'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.decode_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(doc, input_type=\"content\"):\n",
    "    if input_type == \"filename\":\n",
    "        with open(doc, \"rb\") as fh:\n",
    "            doc = fh.read()\n",
    "    elif input_type == \"file\":\n",
    "        doc = doc.read()\n",
    "    \n",
    "    if isinstance(doc, bytes):\n",
    "        doc = doc.decode(\"utf-8\", \"strict\")\n",
    "    \n",
    "    if doc is np.nan:\n",
    "        raise ValueError\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LemmaTokenizer at 0x1afa3f47160>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = partial(_analyze, ngrams=_word_ngrams,\n",
    "                  tokenizer=tokenize, preprocessor=preprocess,\n",
    "                  decoder=decode, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2._validate_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.fixed_vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_int_array():\n",
    "    return array.array(str(\"i\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "_v = defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "_v.default_factory = _v.__len__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_v[\"i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "j_indices = []\n",
    "indptr = []\n",
    "values = _make_int_array()\n",
    "indptr.append(0)\n",
    "for doc in docs:\n",
    "    feature_counter = {}\n",
    "    for feature in analyze(doc):\n",
    "        try:\n",
    "            feature_idx = vocabulary[feature]\n",
    "            if feature_idx not in feature_counter:\n",
    "                feature_counter[feature_idx] = 1\n",
    "            else:\n",
    "                feature_counter[feature_idx] += 1\n",
    "        except KeyError:\n",
    "            print(\"ERROR\")\n",
    "            continue\n",
    "    j_indices.extend(feature_counter.keys())\n",
    "    values.extend(feature_counter.values())\n",
    "    indptr.append(len(j_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indptr[-1] > np.iinfo(np.int32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_dtype = np.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203446"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
    "indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "values = np.frombuffer(values, dtype=np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19320318,), (19320318,), (29445,))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape, j_indices.shape, indptr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sp.csr_matrix((values, j_indices, indptr),\n",
    "              shape=(len(indptr) - 1, len(vocabulary)),\n",
    "              dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "_j_indices = copy.deepcopy(X.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sort_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19320318,)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.equal(_j_indices, X.indices).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2003"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<29444x2003 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19320318 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29444"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc = X.shape[0]\n",
    "n_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 3)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_df = vectorizer2.max_df\n",
    "min_df = vectorizer2.min_df\n",
    "max_df, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(min_df, numbers.Integral), isinstance(max_df, numbers.Integral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23555.2, 3)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_doc_count = max_df * n_doc\n",
    "min_doc_count = min_df\n",
    "max_doc_count, min_doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 113659, 130858, 195014])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _sort_features\n",
    "sorted_features = sorted(vocabulary.items())\n",
    "map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)\n",
    "for new_val, (term, old_val) in enumerate(sorted_features):\n",
    "    vocabulary[term] = new_val\n",
    "    map_index[old_val] = new_val\n",
    "X.indices = map_index.take(X.indices, mode=\"clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([190852,  32715,  83216, ...,  25047, 128116,  10166])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _limit_features(self, X, vocabulary, high=None, low=None,\n",
    "                    limit=None):\n",
    "    \"\"\"Remove too rare or too common features.\n",
    "    Prune features that are non zero in more samples than high or less\n",
    "    documents than low, modifying the vocabulary, and restricting it to\n",
    "    at most the limit most frequent.\n",
    "    This does not prune samples with zero features.\n",
    "    \"\"\"\n",
    "    if high is None and low is None and limit is None:\n",
    "        return X, set()\n",
    "\n",
    "    # Calculate a mask based on document frequencies\n",
    "    dfs = _document_frequency(X)\n",
    "    mask = np.ones(len(dfs), dtype=bool)\n",
    "    if high is not None:\n",
    "        mask &= dfs <= high\n",
    "    if low is not None:\n",
    "        mask &= dfs >= low\n",
    "    if limit is not None and mask.sum() > limit:\n",
    "        tfs = np.asarray(X.sum(axis=0)).ravel()\n",
    "        mask_inds = (-tfs[mask]).argsort()[:limit]\n",
    "        new_mask = np.zeros(len(dfs), dtype=bool)\n",
    "        new_mask[np.where(mask)[0][mask_inds]] = True\n",
    "        mask = new_mask\n",
    "\n",
    "    new_indices = np.cumsum(mask) - 1  # maps old indices to new\n",
    "    removed_terms = set()\n",
    "    for term, old_index in list(vocabulary.items()):\n",
    "        if mask[old_index]:\n",
    "            vocabulary[term] = new_indices[old_index]\n",
    "        else:\n",
    "            del vocabulary[term]\n",
    "            removed_terms.add(term)\n",
    "    kept_indices = np.where(mask)[0]\n",
    "    if len(kept_indices) == 0:\n",
    "        raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
    "                         \" min_df or a higher max_df.\")\n",
    "    return X[:, kept_indices], removed_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,   4, 122, ...,   3,   2,   9], dtype=int64)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate a mask based on document frequencies\n",
    "if sp.isspmatrix_csr(X):\n",
    "    dfs = np.bincount(X.indices, minlength=X.shape[1])\n",
    "else:\n",
    "    dfs = np.diff(X.indptr)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.ones(len(dfs), dtype=bool)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True, False,  True])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask &= dfs <= max_doc_count\n",
    "mask &= dfs >= min_doc_count\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum() > max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203446,)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,   4, 290, ...,   3,   3,  14], dtype=int64)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = np.asarray(X.sum(axis=0)).ravel()\n",
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([194715, 181522, 200865, ..., 163407,  66830,  24708], dtype=int64)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-tfs).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([147178,  54761,  90653, ...,  60786,  62603, 119810], dtype=int64)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_inds = (-tfs[mask]).argsort()[:max_features]\n",
    "mask_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mask = np.zeros(len(dfs), dtype=bool)\n",
    "new_mask[np.where(mask)[0][mask_inds]] = True\n",
    "mask = new_mask\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17997"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_indices = np.cumsum(mask) - 1  # maps old indices to new\n",
    "removed_terms = set()\n",
    "for term, old_index in list(vocabulary.items()):\n",
    "    if mask[old_index]:\n",
    "        vocabulary[term] = new_indices[old_index]\n",
    "    else:\n",
    "        del vocabulary[term]\n",
    "        removed_terms.add(term)\n",
    "len(removed_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     2,     26,     51, ..., 203376, 203378, 203440], dtype=int64)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept_indices = np.where(mask)[0]\n",
    "kept_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(kept_indices) == 0:\n",
    "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
    "                     \" min_df or a higher max_df.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, kept_indices]\n",
    "stop_words_ = removed_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
