{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    _token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    @property\n",
    "    def token_pattern(self):\n",
    "        return self._token_pattern\n",
    "\n",
    "    @token_pattern.setter\n",
    "    def token_pattern(self, s):\n",
    "        if isinstance(s, str):\n",
    "            self._token_pattern = re.compile(s)\n",
    "        elif isinstance(s, type(re.compile(\"\"))):\n",
    "            self._token_pattern = s\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [\n",
    "            self.wnl.lemmatize(t) for t in doc.split() \n",
    "            if (len(t) >= 2 and \n",
    "                re.match(\"[a-z].*\", t) and \n",
    "                re.match(self.token_pattern, t))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jinma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (data\\wikitext\\wikitext-103-v1\\1.0.0\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af)\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\n",
    "    path=\"./wlda/wikitext.py\", name=\"wikitext-103-v1\", cache_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(\n",
    "    input='content', analyzer='word', stop_words='english',\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    max_df=0.8, min_df=3, max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\jinma\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.8, max_features=20000, min_df=3, stop_words='english',\n",
       "                tokenizer=<__main__.LemmaTokenizer object at 0x000001D9933F5668>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x20000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 743 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([docs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2=CountVectorizer(\n",
    "    input='content', analyzer='word', stop_words='english',\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    max_df=0.8, min_df=3, max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4bbf19d027a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvectorizer2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1250\u001b[0m                 \u001b[1;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m                 \"string object received.\")\n\u001b[1;32m-> 1252\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "vectorizer2.transform([docs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('i')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca, indptr, indices = [], [], []\n",
    "values = array.array(str(\"i\")) # signed integer\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2535"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2=CountVectorizer(\n",
    "    input='content', analyzer='word', stop_words='english',\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    max_df=0.8, min_df=3, max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `validate_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.ngram_range # min_n, max_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `validate_vocabulary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. setting vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.max_df, vectorizer2.min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<method-wrapper '__len__' of collections.defaultdict object at 0x000001FB74AFB7C8>,\n",
       "            {})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = defaultdict()\n",
    "vocabulary.default_factory = vocabulary.__len__\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "analzer = \"word\"\n",
    "# http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n",
    "stop_words = ENGLISH_STOP_WORDS\n",
    "tokenize = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_accents = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def _preprocess(doc, accent_function, lower):\n",
    "    if lower:\n",
    "        doc = doc.lower()\n",
    "    if accent_function is not None:\n",
    "        doc = accent_function(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "preprocess = partial(_preprocess, accent_function=strip_accents, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistent = set()\n",
    "for w in stop_words:\n",
    "    tokens = list(tokenize(preprocess(w)))\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            inconsistent.add(token)\n",
    "_stop_words_id = id(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not inconsistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze(doc, analyzer=None, tokenizer=None, ngrams=None,\n",
    "             preprocessor=None, decoder=None, stop_words=None):\n",
    "    if decoder is not None:\n",
    "        doc = decoder(doc)\n",
    "    if analyzer is not None:\n",
    "        doc = analyzer(doc)\n",
    "    else:\n",
    "        if preprocessor is not None:\n",
    "            doc = preprocessor(doc)\n",
    "        if tokenizer is not None:\n",
    "            doc = tokenizer(doc)\n",
    "        if ngrams is not None:\n",
    "            if stop_words is not None:\n",
    "                doc = ngrams(doc, stop_words)\n",
    "            else:\n",
    "                doc = ngrams(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'you']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"i like you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _word_ngrams(tokens, stop_words=None, ngram_range=(1,1)):\n",
    "    if stop_words is not None:\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "    # handle token n-grams\n",
    "    min_n, max_n = ngram_range\n",
    "    if max_n != 1:\n",
    "        original_tokens = tokens\n",
    "        if min_n == 1:\n",
    "            tokens = list(original_tokens)\n",
    "            min_n += 1\n",
    "        else:\n",
    "            tokens = []\n",
    "        n_original_tokens = len(original_tokens)\n",
    "        \n",
    "        # bind method outside of loop to reduce overhead\n",
    "        tokens_append = tokens.append\n",
    "        space_join = \" \".join\n",
    "        \n",
    "        for n in range(min_n, min(max_n+1, n_original_tokens+1)):\n",
    "            for i in range(n_original_tokens-n+1):\n",
    "                tokens_append(space_join(original_tokens[i:i+n]))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'favorite', 'food', 'is', 'sea', 'food']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'favorite',\n",
       " 'food',\n",
       " 'sea',\n",
       " 'food',\n",
       " 'my favorite',\n",
       " 'favorite food',\n",
       " 'food sea',\n",
       " 'sea food',\n",
       " 'my favorite food',\n",
       " 'favorite food sea',\n",
       " 'food sea food',\n",
       " 'my favorite food sea',\n",
       " 'favorite food sea food',\n",
       " 'my favorite food sea food']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"my favorite food is sea food\")\n",
    "print(tokens)\n",
    "_word_ngrams(tokens, stop_words=[\"is\"], ngram_range=(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strict'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.decode_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(doc, input_type=\"content\"):\n",
    "    if input_type == \"filename\":\n",
    "        with open(doc, \"rb\") as fh:\n",
    "            doc = fh.read()\n",
    "    elif input_type == \"file\":\n",
    "        doc = doc.read()\n",
    "    \n",
    "    if isinstance(doc, bytes):\n",
    "        doc = doc.decode(\"utf-8\", \"strict\")\n",
    "    \n",
    "    if doc is np.nan:\n",
    "        raise ValueError\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LemmaTokenizer at 0x1fb9856a550>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = partial(_analyze, ngrams=_word_ngrams,\n",
    "                  tokenizer=tokenize, preprocessor=preprocess,\n",
    "                  decoder=decode, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2._validate_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.fixed_vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_int_array():\n",
    "    return array.array(str(\"i\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "_v = defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "_v.default_factory = _v.__len__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_v[\"i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "j_indices = []\n",
    "indptr = []\n",
    "values = _make_int_array()\n",
    "indptr.append(0)\n",
    "for doc in docs:\n",
    "    feature_counter = {}\n",
    "    for feature in analyze(doc):\n",
    "        try:\n",
    "            feature_idx = vocabulary[feature]\n",
    "            if feature_idx not in feature_counter:\n",
    "                feature_counter[feature_idx] = 1\n",
    "            else:\n",
    "                feature_counter[feature_idx] += 1\n",
    "        except KeyError:\n",
    "            print(\"ERROR\")\n",
    "            continue\n",
    "    j_indices.extend(feature_counter.keys())\n",
    "    values.extend(feature_counter.values())\n",
    "    indptr.append(len(j_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indptr[-1] > np.iinfo(np.int32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_dtype = np.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203446"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
    "indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "values = np.frombuffer(values, dtype=np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19320318,), (19320318,), (29445,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape, j_indices.shape, indptr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sp.csr_matrix((values, j_indices, indptr),\n",
    "              shape=(len(indptr)-1, len(vocabulary)),\n",
    "              dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "_j_indices = copy.deepcopy(X.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sort_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19320318,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.equal(_j_indices, X.indices).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203446"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<29444x203446 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19320318 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29444"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc = X.shape[0]\n",
    "n_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_df = vectorizer2.max_df\n",
    "min_df = vectorizer2.min_df\n",
    "max_df, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(min_df, numbers.Integral), isinstance(max_df, numbers.Integral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23555.2, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_doc_count = max_df * n_doc\n",
    "min_doc_count = min_df\n",
    "max_doc_count, min_doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 113659, 130858, 195014])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _sort_features\n",
    "sorted_features = sorted(vocabulary.items())\n",
    "map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)\n",
    "for new_val, (term, old_val) in enumerate(sorted_features):\n",
    "    vocabulary[term] = new_val\n",
    "    map_index[old_val] = new_val\n",
    "X.indices = map_index.take(X.indices, mode=\"clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([190852,  32715,  83216, ...,  25047, 128116,  10166])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _limit_features(self, X, vocabulary, high=None, low=None,\n",
    "                    limit=None):\n",
    "    \"\"\"Remove too rare or too common features.\n",
    "    Prune features that are non zero in more samples than high or less\n",
    "    documents than low, modifying the vocabulary, and restricting it to\n",
    "    at most the limit most frequent.\n",
    "    This does not prune samples with zero features.\n",
    "    \"\"\"\n",
    "    if high is None and low is None and limit is None:\n",
    "        return X, set()\n",
    "\n",
    "    # Calculate a mask based on document frequencies\n",
    "    dfs = _document_frequency(X)\n",
    "    mask = np.ones(len(dfs), dtype=bool)\n",
    "    if high is not None:\n",
    "        mask &= dfs <= high\n",
    "    if low is not None:\n",
    "        mask &= dfs >= low\n",
    "    if limit is not None and mask.sum() > limit:\n",
    "        tfs = np.asarray(X.sum(axis=0)).ravel()\n",
    "        mask_inds = (-tfs[mask]).argsort()[:limit]\n",
    "        new_mask = np.zeros(len(dfs), dtype=bool)\n",
    "        new_mask[np.where(mask)[0][mask_inds]] = True\n",
    "        mask = new_mask\n",
    "\n",
    "    new_indices = np.cumsum(mask) - 1  # maps old indices to new\n",
    "    removed_terms = set()\n",
    "    for term, old_index in list(vocabulary.items()):\n",
    "        if mask[old_index]:\n",
    "            vocabulary[term] = new_indices[old_index]\n",
    "        else:\n",
    "            del vocabulary[term]\n",
    "            removed_terms.add(term)\n",
    "    kept_indices = np.where(mask)[0]\n",
    "    if len(kept_indices) == 0:\n",
    "        raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
    "                         \" min_df or a higher max_df.\")\n",
    "    return X[:, kept_indices], removed_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,   4, 122, ...,   3,   2,   9], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate a mask based on document frequencies\n",
    "if sp.isspmatrix_csr(X):\n",
    "    dfs = np.bincount(X.indices, minlength=X.shape[1])\n",
    "else:\n",
    "    dfs = np.diff(X.indptr)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.ones(len(dfs), dtype=bool)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True, False,  True])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask &= dfs <= max_doc_count\n",
    "mask &= dfs >= min_doc_count\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum() > max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,   4, 290, ...,   3,   3,  14], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = np.asarray(X.sum(axis=0)).ravel()\n",
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203446,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([194715, 181522, 200865, ..., 163407,  66830,  24708], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-tfs).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([147178,  54761,  90653, ...,  60786,  62603, 119810], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_inds = (-tfs[mask]).argsort()[:max_features]\n",
    "mask_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mask = np.zeros(len(dfs), dtype=bool)\n",
    "new_mask[np.where(mask)[0][mask_inds]] = True\n",
    "mask = new_mask\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183446"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_indices = np.cumsum(mask) - 1  # maps old indices to new\n",
    "removed_terms = set()\n",
    "for term, old_index in list(vocabulary.items()):\n",
    "    if mask[old_index]:\n",
    "        vocabulary[term] = new_indices[old_index]\n",
    "    else:\n",
    "        del vocabulary[term]\n",
    "        removed_terms.add(term)\n",
    "len(removed_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     2,     26,     51, ..., 203376, 203378, 203440], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept_indices = np.where(mask)[0]\n",
    "kept_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(kept_indices) == 0:\n",
    "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
    "                     \" min_df or a higher max_df.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, kept_indices]\n",
    "stop_words_ = removed_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
