{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface and W-LDA\n",
    "- ArrowDataset으로 porting을 편하게!\n",
    "- load_dataset을 wikitext-103을 받아오며 뜯어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_dataset\n",
    "``load_dataset``은 아래 5단계를 수행한다.\n",
    "\n",
    "1. Dataset Preprocessing Script 다운로드\n",
    "2. Preprocessing Script에서 Dataset Builder Class 얻기\n",
    "3. DatasetBuilder를 객체화\n",
    "4. Data Download\n",
    "5. Dataset Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Dataset Preprocessing Script\n",
    "- 무조건 script를 만들 것이기 때문에 다운로드할 필요없음\n",
    "- 즉, 필요한 부분만 꺼내올 것\n",
    "- huggingface에서 내부적으로 처리하는 코드는 아래와 같음\n",
    "\n",
    "```python\n",
    "from datasets import prepare_module\n",
    "\n",
    "\n",
    "module_path, hash, resolved_file_path = prepare_module(\n",
    "    path,\n",
    "    script_version=script_version,\n",
    "    download_config=download_config,\n",
    "    download_mode=download_mode,\n",
    "    dataset=True,\n",
    "    return_resolved_file_path=True,\n",
    "    use_auth_token=use_auth_token,\n",
    ")\n",
    "```\n",
    "\n",
    "- 동작 코드 확인해보니, 아래 역할을 수행\n",
    "    1. dataset인지 metric인지\n",
    "    2. 강제 저장 위치 지정했는지, dynamic하게 저장할 것인지\n",
    "    3. local에서 script를 가져올 것인지 아니면 github에서 가져올 것인지 결정\n",
    "        - 내 코드에선 git에서 가져오는 것은 아직 불필요\n",
    "        - 다음에 한 번 따로 짜보자\n",
    "    4. info 파일이 있다면 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable, List, Union, Any, NewType, Iterable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../wikitext-103.py\"\n",
    "name = \"wikitext-103-v1\"\n",
    "data_dir = None\n",
    "data_files = None\n",
    "split = \"train\"\n",
    "cache_dir = \"cache\"\n",
    "features = None\n",
    "download_config = None\n",
    "download_mode = None\n",
    "ignore_verifications = False\n",
    "keep_in_memory = False\n",
    "save_infos = False\n",
    "script_version = None\n",
    "use_auth_token = None\n",
    "config_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DownloadConfig:\n",
    "    cache_dir: Optional[Union[str, Path]] = None\n",
    "    force_download: bool = False\n",
    "    resume_download: bool = False\n",
    "    local_files_only: bool = False\n",
    "    proxies: Optional[Dict] = None\n",
    "    user_agent: Optional[str] = None\n",
    "    extract_compressed_file: bool = False\n",
    "    force_extract: bool = False\n",
    "    use_etag: bool = True\n",
    "    num_proc: Optional[int] = None\n",
    "    max_retries: int = 1\n",
    "    use_auth_token: Optional[Union[str, bool]] = None\n",
    "\n",
    "    def copy(self) -> \"DownloadConfig\":\n",
    "        return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DownloadConfig(cache_dir=None, force_download=False, resume_download=False, local_files_only=False, proxies=None, user_agent=None, extract_compressed_file=True, force_extract=True, use_etag=True, num_proc=None, max_retries=1, use_auth_token=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_config = DownloadConfig()\n",
    "download_config.extract_compressed_file = True\n",
    "download_config.force_extract = True\n",
    "download_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wikitext-103.py', 'wikitext-103')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_type = \"dataset\"\n",
    "_name = list(filter(lambda x: x, path.replace(os.sep, \"/\").split(\"/\")))[-1]\n",
    "if not _name.endswith(\".py\"):\n",
    "    _name = _name + \".py\"\n",
    "    \n",
    "# Short name is name without the '.py' at the end (for the module)\n",
    "short_name = name[:-3]\n",
    "\n",
    "_name, short_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'csv': ('datasets.packaged_modules.csv.csv',\n",
       "   '965b6429be0fc05f975b608ce64e1fa941cc8fb4f30629b523d2390f3c0e1a93'),\n",
       "  'json': ('datasets.packaged_modules.json.json',\n",
       "   '5068e8663a7669137d288ea22cd76c5f4cac5f20db7ed8252b722e51c43c0760'),\n",
       "  'pandas': ('datasets.packaged_modules.pandas.pandas',\n",
       "   '3547ff13f14ebf30dc8f11ab416dcf7721c18de2eb7e054bf9c80b88e765a791'),\n",
       "  'text': ('datasets.packaged_modules.text.text',\n",
       "   '44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691')},\n",
       " False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first check if the module is packaged with the `datasets` package\n",
    "# What is this?\n",
    "from datasets.packaged_modules import _PACKAGED_DATASETS_MODULES\n",
    "\n",
    "_PACKAGED_DATASETS_MODULES, path in _PACKAGED_DATASETS_MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_modules_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULE_NAME_FOR_DYNAMIC_MODULES = \"datasets_modules\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import config\n",
    "from datasets.load import init_dynamic_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets_modules'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODULE_NAME_FOR_DYNAMIC_MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/jinma/.cache/huggingface/modules')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.HF_MODULES_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma/.cache\\w-lda\\modules\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/jinma/.cache/w-lda/modules')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_XDG_CACHE_HOME = \"~/.cache\"\n",
    "XDG_CACHE_HOME = os.getenv(\"XDG_CACHE_HOME\", DEFAULT_XDG_CACHE_HOME)\n",
    "DEFAULT_MY_CACHE_HOME = os.path.join(XDG_CACHE_HOME, \"w-lda\")\n",
    "MY_CACHE_HOME = os.path.expanduser(os.getenv(\"MY_HOME\", DEFAULT_MY_CACHE_HOME))\n",
    "DEFAULT_MODULES_CACHE = os.path.join(MY_CACHE_HOME, \"modules\")\n",
    "print(os.getenv(\"MODULES_CACHE\", DEFAULT_MODULES_CACHE))\n",
    "MODULES_CACHE = Path(os.getenv(\"MODULES_CACHE\", DEFAULT_MODULES_CACHE))\n",
    "MODULES_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otherwise the module is added to the dynamic modules\n",
    "dynamic_modules_path = (\n",
    "    dynamic_modules_path\n",
    "    if dynamic_modules_path is not None\n",
    "    else init_dynamic_modules(MODULE_NAME_FOR_DYNAMIC_MODULES, hf_modules_cache=config.HF_MODULES_CACHE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\modules\\\\datasets_modules'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_modules_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기 필요없넹."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dynamic_modules_path가 안주어지면 강제적으로 .cache/huggingface/modules로 설정됨\n",
    "- 난 여길 항상 root direct으로 주도록 setting하면 되겠지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_name_for_dynamic_modules = os.path.basename(dynamic_modules_path)\n",
    "datasets_modules_path = os.path.join(dynamic_modules_path, \"datasets\")\n",
    "datasets_modules_name = module_name_for_dynamic_modules + \".datasets\"\n",
    "metrics_modules_path = os.path.join(dynamic_modules_path, \"metrics\")\n",
    "metrics_modules_name = module_name_for_dynamic_modules + \".metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets_modules'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_name_for_dynamic_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\modules\\\\datasets_modules\\\\datasets'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_modules_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets_modules.datasets'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_modules_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\modules\\\\datasets_modules\\\\metrics'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_modules_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets_modules.metrics'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_modules_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\modules\\\\datasets_modules\\\\datasets\\\\wikitext-103'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_local_path = None\n",
    "\n",
    "main_folder_path = os.path.join(datasets_modules_path if True else metrics_modules_path, short_name)\n",
    "main_folder_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기의 if/elif/else절에서 local에 py파일이 있는지,\n",
    "- 없다면 가져오는 부분이 try/except! (github에서 가져옴)\n",
    "    - 이 부분은 불필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../wikitext-103.py\\\\wikitext-103-v1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_path = os.path.join(path, name)\n",
    "combined_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(combined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../wikitext-103.py'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path\n",
    "local_path = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils.file_utils import (\n",
    "    url_or_path_parent, url_or_path_join, cached_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.info import DATASET_INFOS_DICT_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the module in two steps\n",
    "# 1. get the processing file on the local filesystem if it's not ther (download to cache dir)\n",
    "# 2. copy from the local file system inside the modules cache to import it\n",
    "\n",
    "base_path = url_or_path_parent(file_path)  # remove the filename\n",
    "dataset_infos = url_or_path_join(base_path, DATASET_INFOS_DICT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('..', '../dataset_infos.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path, dataset_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset infos file if available\n",
    "try:\n",
    "    local_dataset_infos_path = cached_path(\n",
    "        dataset_infos,\n",
    "        download_config=download_config,\n",
    "    )\n",
    "except (FileNotFoundError, ConnectionError):\n",
    "    local_dataset_infos_path = None\n",
    "    \n",
    "\n",
    "local_dataset_infos_path is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load import get_imports # 굉장히 유용한 함수!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../wikitext-103.py',\n",
       " [('library', '__future__', '__future__', None),\n",
       "  ('library', 'os', 'os', None),\n",
       "  ('library', 'datasets', 'datasets', None)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download external imports if needed\n",
    "imports = get_imports(local_path)\n",
    "local_path, imports # import_type, import_name, import_path, sub_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikitext-103'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_imports = []\n",
    "library_imports = []\n",
    "\n",
    "for _, import_name, import_path, _ in imports:\n",
    "    library_imports.append((import_name, import_path)) # Import from a library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'datasets' from 'C:\\\\Users\\\\jinma\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\basic\\\\lib\\\\site-packages\\\\datasets\\\\__init__.py'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check library imports\n",
    "import importlib\n",
    "importlib.import_module(library_imports[0][0])\n",
    "importlib.import_module(library_imports[1][0])\n",
    "importlib.import_module(library_imports[2][0])\n",
    "# Error X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load import files_to_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6efa45b4d3a90f1e7df0212a0eeb0adfd7e96468534172fb2ec8d338fe5f6d58'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a directory with a unique name in our dataset or metric folder\n",
    "# path is: ./datasets|metrics/dataset|metric_name/hash_from_code/script.py\n",
    "# we use a hash to be able to have multiple versions of a dataset/metric processing file together\n",
    "hash_ = files_to_hash([local_path] + [loc[1] for loc in local_imports])\n",
    "hash_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../wikitext-103.py']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[local_path] + [loc[1] for loc in local_imports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\modules\\\\datasets_modules\\\\datasets\\\\wikitext-103\\\\6efa45b4d3a90f1e7df0212a0eeb0adfd7e96468534172fb2ec8d338fe5f6d58'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기서 조합되는구만\n",
    "hash_folder_path = os.path.join(main_folder_path, hash_)\n",
    "hash_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\modules\\\\datasets_modules\\\\datasets\\\\wikitext-103\\\\6efa45b4d3a90f1e7df0212a0eeb0adfd7e96468534172fb2ec8d338fe5f6d58\\\\wikitext-103-v1',\n",
       " 'C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\modules\\\\datasets_modules\\\\datasets\\\\wikitext-103\\\\6efa45b4d3a90f1e7df0212a0eeb0adfd7e96468534172fb2ec8d338fe5f6d58\\\\dataset_infos.json')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_file_path = os.path.join(hash_folder_path, name)\n",
    "dataset_infos_path = os.path.join(hash_folder_path, DATASET_INFOS_DICT_FILE_NAME)\n",
    "\n",
    "local_file_path, dataset_infos_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../wikitext-103.py.lock'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prevent parallel disk operations\n",
    "lock_path = local_path + \".lock\"\n",
    "lock_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils.filelock import FileLock\n",
    "\n",
    "\n",
    "filelock = FileLock(lock_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.utils.filelock._Acquire_ReturnProxy at 0x19ec99b6ac8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelock.acquire() # ENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wikitext-103\n"
     ]
    }
   ],
   "source": [
    "# Create main dataset/metrics folder if needed\n",
    "from datasets.utils.download_manager import GenerateMode\n",
    "\n",
    "# 삭제 조건이 여기에 이렇게 있네? 참고\n",
    "print(main_folder_path)\n",
    "if (download_mode == GenerateMode.FORCE_REDOWNLOAD and \n",
    "    os.path.exists(main_folder_path)):\n",
    "    shutil.rmtree(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils.logging import get_logger\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not os.path.exists(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 없으면 만들기\n",
    "if not os.path.exists(main_folder_path):\n",
    "    logger.info(f\"Creating main folder for {module_type} {file_path} at {main_folder_path}\")\n",
    "    os.makedirs(main_folder_path, exist_ok=True)\n",
    "else:\n",
    "    logger.info(f\"Found main folder for {module_type} {file_path} at {main_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wikitext-103\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "# add an __init__ file to the main dataset folder if needed\n",
    "init_file_path = os.path.join(main_folder_path, \"__init__.py\")\n",
    "print(init_file_path)\n",
    "if not os.path.exists(init_file_path):\n",
    "    with open(init_file_path, \"w\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create hash dataset folder if needed\n",
    "print(not os.path.exists(hash_folder_path))\n",
    "if not os.path.exists(hash_folder_path):\n",
    "    logger.info(f\"Creating specific version folder for {module_type} {file_path} at {hash_folder_path}\")\n",
    "    os.makedirs(hash_folder_path)\n",
    "else:\n",
    "    logger.info(f\"Found specific version folder for {module_type} {file_path} at {hash_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wikitext\\041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "# add an __init__ file to the hash dataset folder if needed\n",
    "init_file_path = os.path.join(hash_folder_path, \"__init__.py\")\n",
    "print(init_file_path)\n",
    "if not os.path.exists(init_file_path):\n",
    "    with open(init_file_path, \"w\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Script 복사\n",
    "- 난 굳이 할 필요 없을 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Copy dataset.py file in hash folder if needed\n",
    "print(not os.path.exists(local_file_path))\n",
    "if not os.path.exists(local_file_path):\n",
    "    logger.info(\"Copying script file from %s to %s\", file_path, local_file_path)\n",
    "    shutil.copyfile(local_path, local_file_path)\n",
    "else:\n",
    "    logger.info(\"Found script file from %s to %s\", file_path, local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset info 복사\n",
    "- 난 굳이...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Copy dataset infos file if needed\n",
    "print(not os.path.exists(dataset_infos_path))\n",
    "print(local_dataset_infos_path is not None)\n",
    "print(\n",
    "    local_dataset_infos_path is not None and \n",
    "    not filecmp.cmp(local_dataset_infos_path, dataset_infos_path)\n",
    ")\n",
    "if not os.path.exists(dataset_infos_path):\n",
    "    if local_dataset_infos_path is not None:\n",
    "        logger.info(\"Copying dataset infos file from %s to %s\", dataset_infos, dataset_infos_path)\n",
    "        shutil.copyfile(local_dataset_infos_path, dataset_infos_path)\n",
    "    else:\n",
    "        logger.info(\"Couldn't find dataset infos file at %s\", dataset_infos)\n",
    "else:\n",
    "    if local_dataset_infos_path is not None and not filecmp.cmp(local_dataset_infos_path, dataset_infos_path):\n",
    "        logger.info(\"Updating dataset infos file from %s to %s\", dataset_infos, dataset_infos_path)\n",
    "        shutil.copyfile(local_dataset_infos_path, dataset_infos_path)\n",
    "    else:\n",
    "        logger.info(\"Found dataset infos file from %s to %s\", dataset_infos, dataset_infos_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wikitext\\041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638\\wikitext.json\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Record metadata associating original dataset path with local unique folder\n",
    "meta_path = local_file_path.split(\".py\")[0] + \".json\"\n",
    "print(meta_path)\n",
    "print(not os.path.exists(meta_path))\n",
    "if not os.path.exists(meta_path):\n",
    "    logger.info(f\"Creating metadata file for {module_type} {file_path} at {meta_path}\")\n",
    "    meta = {\"original file path\": file_path, \"local file path\": local_file_path}\n",
    "    # the filename is *.py in our case, so better rename to filenam.json instead of filename.py.json\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "        json.dump(meta, meta_file)\n",
    "else:\n",
    "    logger.info(f\"Found metadata file for {module_type} {file_path} at {meta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all the additional imports\n",
    "local_imports # internal, external 정보\n",
    "for import_name, import_path in local_imports:\n",
    "    if os.path.isfile(import_path):\n",
    "        full_path_local_import = os.path.join(hash_folder_path, import_name + \".py\")\n",
    "        if not os.path.exists(full_path_local_import):\n",
    "            logger.info(\"Copying local import file from %s at %s\", import_path, full_path_local_import)\n",
    "            shutil.copyfile(import_path, full_path_local_import)\n",
    "        else:\n",
    "            logger.info(\"Found local import file from %s at %s\", import_path, full_path_local_import)\n",
    "    elif os.path.isdir(import_path):\n",
    "        full_path_local_import = os.path.join(hash_folder_path, import_name)\n",
    "        if not os.path.exists(full_path_local_import):\n",
    "            logger.info(\"Copying local import directory from %s at %s\", import_path, full_path_local_import)\n",
    "            shutil.copytree(import_path, full_path_local_import)\n",
    "        else:\n",
    "            logger.info(\"Found local import directory from %s at %s\", import_path, full_path_local_import)\n",
    "    else:\n",
    "        raise OSError(f\"Error with local import at {import_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelock.release() # EXIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets_modules.datasets.wikitext.041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638.wikitext'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path = \".\".join(\n",
    "    [datasets_modules_name if True else metrics_modules_name, short_name, hash_, short_name]\n",
    ")\n",
    "module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the new module to be noticed by the import system\n",
    "importlib.invalidate_caches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_resolved_file_path = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets_modules.datasets.wikitext.041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638.wikitext'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikitext.py'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path # 맨 처음에 인자로 받은!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Processing Script에서 Dataset Builder Class 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load import import_main_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets_modules.datasets.wikitext.041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638.wikitext.Wikitext"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_cls = import_main_class(module_path, dataset=True)\n",
    "builder_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) DatasetBuilder를 객체화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.builder import DatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_instance: DatasetBuilder = builder_cls(\n",
    "    cache_dir=cache_dir,\n",
    "    name=name,\n",
    "    data_dir=data_dir,\n",
    "    data_files=data_files,\n",
    "    hash=hash_,\n",
    "    features=features,\n",
    "    **config_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets_modules.datasets.wikitext.041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638.wikitext.Wikitext at 0x2a5cb784518>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-103-v1 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to cache\\wikitext\\wikitext-103-v1\\1.0.0\\041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3295113070b5464499182b0139430131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=190229076.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to cache\\wikitext\\wikitext-103-v1\\1.0.0\\041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Download and prepare data\n",
    "builder_instance.download_and_prepare(\n",
    "    download_config=download_config,\n",
    "    download_mode=download_mode,\n",
    "    ignore_verifications=ignore_verifications,\n",
    "    try_from_hf_gcs=True,\n",
    "    base_path=base_path,\n",
    "    use_auth_token=use_auth_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Dataset Build!\n",
    "- 여기선 여기에 집중할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets_modules.datasets.wikitext.041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638.wikitext.Wikitext at 0x2a5cb784518>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not os.path.exists(builder_instance._cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': SplitInfo(name='test', num_bytes=1295579, num_examples=4358, dataset_name='wikitext'),\n",
       " 'train': SplitInfo(name='train', num_bytes=545142639, num_examples=1801350, dataset_name='wikitext'),\n",
       " 'validation': SplitInfo(name='validation', num_bytes=1154755, num_examples=3760, dataset_name='wikitext')}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "function = partial(\n",
    "    builder_instance._build_single_dataset,\n",
    "    run_post_process=True,\n",
    "    ignore_verifications=False,\n",
    "    in_memory=False\n",
    ")\n",
    "data_struct = {s: s for s in builder_instance.info.splits}\n",
    "types = None\n",
    "num_proc = None\n",
    "map_numpy = False\n",
    "map_tuple = True\n",
    "map_list = False\n",
    "dict_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple,)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = [tuple]\n",
    "types = tuple(types)\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not isinstance(data_struct, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train', 'validation']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterable = list(data_struct.values()) if isinstance(data_struct, dict) else data_struct\n",
    "iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " [(functools.partial(<bound method DatasetBuilder._build_single_dataset of <datasets_modules.datasets.wikitext.041b2070e5d6463bbec9a8ae743190512269d9ab8aa53d33ed10a56c8a804638.wikitext.Wikitext object at 0x000002A5CB784518>>, run_post_process=True, ignore_verifications=False, in_memory=False),\n",
       "   ['test', 'train', 'validation'],\n",
       "   (tuple,),\n",
       "   0,\n",
       "   False)])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_kwds = [] # We organize the splits ourselves (contiguous splits)\n",
    "for index in range(num_proc):\n",
    "    div = len(iterable) // num_proc\n",
    "    mod = len(iterable) % num_proc\n",
    "    start = div * index + min(index, mod)\n",
    "    end = start + div + (1 if index < mod else 0)\n",
    "    split_kwds.append((function, iterable[start:end], types, index, False))\n",
    "assert len(iterable) == sum(len(i[1]) for i in split_kwds), (\n",
    "    f\"Error dividing inputs iterable among processes. \"\n",
    "    f\"Total number of objects {len(iterable)}, \"\n",
    "    f\"length: {sum(len(i[1]) for i in split_kwds)}\"\n",
    ")\n",
    "div, mod, start, end, split_kwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "function, data_struct, types, rank, disable_tqdm = split_kwds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not isinstance(data_struct, dict) and not isinstance(data_struct, types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WikitextConfig(name='wikitext-103-v1', version=1.0.0, data_dir=None, data_files=None, description='raw level dataset. The raw tokens before the addition of <unk> tokens. They should only be used for character level work or for creating newly derived datasets.')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
