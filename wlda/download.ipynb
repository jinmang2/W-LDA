{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_dataset\n",
    "1. Dataset Preprocessing Script 다운로드\n",
    "2. Preprocessing Script에서 Dataset Builder Class 얻기\n",
    "3. DatasetBuilder 객체화\n",
    "4. Data Download\n",
    "5. Dataset Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수정할 부분\n",
    "1. 무조건 w-lda repo 내부에 data 폴더로 파일이 생기게 만들 것\n",
    "2. ./cache/huggingface/modules엔 cache 파일이 생기지 않게 수정할 것\n",
    "3. extracted 파일이 원본 파일. 이를 정확히 매핑하기\n",
    "4. 내부 파일에서 cache를 체크해서 데이터셋 로드하도록 코드 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable, List, Union, Any, NewType, Iterable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"wikitext.py\"\n",
    "name = \"wikitext-103-v1\"\n",
    "data_dir = None\n",
    "data_files = None\n",
    "split = \"train\"\n",
    "cache_dir = \"data\"\n",
    "features = None\n",
    "download_config = None\n",
    "download_mode = None\n",
    "ignore_verifications = False\n",
    "keep_in_memory = False\n",
    "save_infos = False\n",
    "script_version = None\n",
    "use_auth_token = None\n",
    "config_kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preprocessing Script 다운\n",
    "```python\n",
    "module_path = prepare_module(\n",
    "    path : str,\n",
    "    name : str,\n",
    "    cache_dir : str,\n",
    "    download_mode : Optional[GenerateMode],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikitext'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_type = \"dataset\"\n",
    "script_name = list(filter(lambda x: x, path.replace(os.sep, \"/\").split(\"/\")))[-1]\n",
    "if not script_name.endswith(\".py\"):\n",
    "    raise AttributeError(\"\")\n",
    "short_name = script_name[:-3]\n",
    "short_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__main__'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\W-LDA\\\\new\\\\data\\\\datasets_modules'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_modules_path = os.path.join(os.path.abspath(cache_dir), \"datasets_modules\")\n",
    "dynamic_modules_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('datasets_modules',\n",
       " 'C:\\\\workspace\\\\W-LDA\\\\new\\\\data\\\\datasets_modules\\\\datasets',\n",
       " 'datasets_modules.datasets')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_name_for_dynamic_modules = os.path.basename(dynamic_modules_path)\n",
    "datasets_modules_path = os.path.join(dynamic_modules_path, \"datasets\")\n",
    "datasets_modules_name = module_name_for_dynamic_modules + \".datasets\"\n",
    "\n",
    "module_name_for_dynamic_modules, datasets_modules_path, datasets_modules_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\W-LDA\\\\new\\\\data\\\\datasets_modules\\\\datasets\\\\wikitext'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_folder_path = os.path.join(datasets_modules_path, short_name)\n",
    "main_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikitext.py'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = path\n",
    "local_path = path\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils.file_utils import (\n",
    "    url_or_path_parent, url_or_path_join, cached_path)\n",
    "from datasets.info import DATASET_INFOS_DICT_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', 'dataset_infos.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = url_or_path_parent(file_path)  # remove the filename\n",
    "dataset_infos = url_or_path_join(base_path, DATASET_INFOS_DICT_FILE_NAME)\n",
    "\n",
    "base_path, dataset_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load import get_imports # 굉장히 유용한 함수!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wikitext.py',\n",
       " [('library', '__future__', '__future__', None),\n",
       "  ('library', 'os', 'os', None),\n",
       "  ('library', 'datasets', 'datasets', None)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download external imports if needed\n",
    "imports = get_imports(local_path)\n",
    "local_path, imports # import_type, import_name, import_path, sub_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- only support library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__future__', '__future__'), ('os', 'os'), ('datasets', 'datasets')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "library_imports = []\n",
    "for import_type, import_name, import_path, sub_directory in imports:\n",
    "    if import_name == short_name:\n",
    "        raise ValueError(\n",
    "            f\"Error in {module_type} script at {file_path}, importing relative {import_name} module \"\n",
    "            f\"but {import_name} is the name of the {module_type} script. \"\n",
    "            f\"Please change relative import {import_name} to another name and add a '# From: URL_OR_PATH' \"\n",
    "            f\"comment pointing to the original realtive import file path.\"\n",
    "        )\n",
    "    if import_type == \"library\":\n",
    "        library_imports.append((import_name, import_path))  # Import from a library\n",
    "    else:\n",
    "        raise ValueError(\"Wrong import_type\")\n",
    "library_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check library imports\n",
    "needs_to_be_installed = []\n",
    "for library_import_name, library_import_path in library_imports:\n",
    "    try:\n",
    "        lib = importlib.import_module(library_import_name)  # noqa F841\n",
    "    except ImportError:\n",
    "        needs_to_be_installed.append((library_import_name, library_import_path))\n",
    "if needs_to_be_installed:\n",
    "    raise ImportError(\n",
    "        f\"To be able to use this {module_type}, you need to install the following dependencies\"\n",
    "        f\"{[lib_name for lib_name, lib_path in needs_to_be_installed]} using 'pip install \"\n",
    "        f\"{' '.join([lib_path for lib_name, lib_path in needs_to_be_installed])}' for instance'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hash 안씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikitext.py']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[local_path] + []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load import files_to_hash\n",
    "\n",
    "\n",
    "local_imports = []\n",
    "hash_ = files_to_hash([local_path] + [loc[1] for loc in local_imports])\n",
    "# hash_folder_path = os.path.join(main_folder_path, hash_)\n",
    "hash_folder_path = main_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\workspace\\\\W-LDA\\\\new\\\\data\\\\datasets_modules\\\\datasets\\\\wikitext\\\\wikitext-103-v1',\n",
       " 'C:\\\\workspace\\\\W-LDA\\\\new\\\\data\\\\datasets_modules\\\\datasets\\\\wikitext\\\\dataset_infos.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_file_path = os.path.join(hash_folder_path, name)\n",
    "dataset_infos_path = os.path.join(hash_folder_path, DATASET_INFOS_DICT_FILE_NAME)\n",
    "\n",
    "local_file_path, dataset_infos_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wikitext.py.lock',\n",
       " <datasets.utils.filelock.WindowsFileLock at 0x1d13d1ec748>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.utils.filelock import FileLock\n",
    "\n",
    "\n",
    "# Prevent parallel disk operations\n",
    "lock_path = local_path + \".lock\"\n",
    "filelock = FileLock(lock_path)\n",
    "\n",
    "lock_path, filelock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "\n",
    "class GenerateMode(enum.Enum):\n",
    "    \"\"\"`Enum` for how to treat pre-existing downloads and data.\n",
    "    The default mode is `REUSE_DATASET_IF_EXISTS`, which will reuse both\n",
    "    raw downloads and the prepared dataset if they exist.\n",
    "    The generations modes:\n",
    "    +------------------------------------+-----------+---------+\n",
    "    |                                    | Downloads | Dataset |\n",
    "    +====================================+===========+=========+\n",
    "    | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   |\n",
    "    +------------------------------------+-----------+---------+\n",
    "    | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   |\n",
    "    +------------------------------------+-----------+---------+\n",
    "    | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   |\n",
    "    +------------------------------------+-----------+---------+\n",
    "    \"\"\"\n",
    "\n",
    "    REUSE_DATASET_IF_EXISTS = \"reuse_dataset_if_exists\"\n",
    "    REUSE_CACHE_IF_EXISTS = \"reuse_cache_if_exists\"\n",
    "    FORCE_REDOWNLOAD = \"force_redownload\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_mode = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (download_mode == GenerateMode.FORCE_REDOWNLOAD and\n",
    "    os.path.exists(main_folder_path)\n",
    "):\n",
    "    print(\"FORCE REDOWNLOAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(main_folder_path):\n",
    "    os.makedirs(main_folder_path, exist_ok=True)\n",
    "else:\n",
    "    print(\"Do not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\workspace\\W-LDA\\new\\data\\datasets_modules\\datasets\\wikitext\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "# add an __init__ file to the main dataset folder if needed\n",
    "init_file_path = os.path.join(main_folder_path, \"__init__.py\")\n",
    "print(init_file_path)\n",
    "if not os.path.exists(init_file_path):\n",
    "    with open(init_file_path, \"w\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wikitext.py',\n",
       " 'C:\\\\workspace\\\\W-LDA\\\\new\\\\data\\\\datasets_modules\\\\datasets\\\\wikitext\\\\wikitext-103-v1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path, local_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy\n"
     ]
    }
   ],
   "source": [
    "# Copy dataset.py file in hash folder if needed\n",
    "if not os.path.exists(local_file_path):\n",
    "    print(\"Copy\")\n",
    "    shutil.copyfile(local_path, local_file_path)\n",
    "else:\n",
    "    print(\"Do not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset info 아직 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\workspace\\W-LDA\\new\\data\\datasets_modules\\datasets\\wikitext\\wikitext-103-v1.json\n",
      "Meta file\n"
     ]
    }
   ],
   "source": [
    "# Record metadata associating original dataset path with local unique folder\n",
    "meta_path = local_file_path.split(\".py\")[0] + \".json\"\n",
    "print(meta_path)\n",
    "if not os.path.exists(meta_path):\n",
    "    print(\"Meta file\")\n",
    "    meta = {\"original file path\": file_path, \"local file path\": local_file_path}\n",
    "    # the filename is *.py in our case, so better rename to filenam.json instead of filename.py.json\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "        json.dump(meta, meta_file)\n",
    "else:\n",
    "    print(\"Do not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original file path': 'wikitext.py',\n",
       " 'local file path': 'C:\\\\workspace\\\\W-LDA\\\\new\\\\data\\\\datasets_modules\\\\datasets\\\\wikitext\\\\wikitext-103-v1'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelock.release() # EXIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets_modules.datasets.wikitext'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path = \".\".join(\n",
    "    [datasets_modules_name, short_name]\n",
    ")\n",
    "module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikitext.py'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
