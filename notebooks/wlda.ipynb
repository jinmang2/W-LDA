{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "\n",
    "import sys\n",
    "\n",
    "import mxnet as mx\n",
    "mx.random.seed(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import socket\n",
    "from functools import reduce\n",
    "from scipy.special import logit\n",
    "from tqdm import tqdm\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_helper(gpu):\n",
    "    if gpu >= 0 and gpu_exists(gpu):\n",
    "        model_ctx = mx.gpu(gpu)\n",
    "    else:\n",
    "        model_ctx = mx.cpu()\n",
    "    return model_ctx\n",
    "\n",
    "def gpu_exists(gpu):\n",
    "    try:\n",
    "        mx.nd.zeros((1,), ctx=mx.gpu(gpu))\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def calc_topic_uniqueness(top_words_idx_all_topics):\n",
    "    \"\"\"\n",
    "    This function calculates topic uniqueness scores for a given list of topics.\n",
    "    For each topics, the uniqueness is calculated as: (\\sum_{i=1}^n 1/cnt(i)) / n,\n",
    "    where n is the number of top words in the topic and cnt(i) is the counter for the number of times the word\n",
    "    appears in the top words of all the topics.\n",
    "    \"\"\"\n",
    "    n_topics = len(top_words_idx_all_topics)\n",
    "    \n",
    "    # build word_cnt_dict that is number of times the word appears in top words\n",
    "    word_cnt_dict = collections.Counter()\n",
    "    for i in range(n_topics):\n",
    "        word_cnt_dict.update(top_words_idx_all_topics[i])\n",
    "    \n",
    "    uniqueness_dict = dict()\n",
    "    for i in range(n_topics):\n",
    "        cnt_inv_sum = 0.0\n",
    "        for ind in top_words_idx_all_topics[i]:\n",
    "            cnt_inv_sum += 1.0 / word_cnt_dict[ind]\n",
    "        uniqueness_dict[i] = cnt_inv_sum / len(top_words_idx_all_topics)\n",
    "        \n",
    "    return uniqueness_dict\n",
    "\n",
    "def request_pmi(topic_dict=None, filename='', port=1234):\n",
    "    try:\n",
    "        # Create a socket object\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        \n",
    "        # get local machine name\n",
    "        host = socket.gethostname()\n",
    "        # host = socket.gethostbyname('localhost')\n",
    "        \n",
    "        # connection to hostname on the port.\n",
    "        s.connect((host, port))\n",
    "        \n",
    "        if filename != '':\n",
    "            s.sendall(pickle.dumps(filename), )\n",
    "        else:\n",
    "            s.send(pickle.dumps(topic_dict), )\n",
    "            \n",
    "        data = []\n",
    "        while True:\n",
    "            packet = s.recv(4096)\n",
    "            # time.sleep(0.1)\n",
    "            # print('looking at packet # {0}'.format(len(data)))\n",
    "            # print(packet)\n",
    "            # print(type(packet))\n",
    "            wait = len(packet)\n",
    "            if not packet:\n",
    "                break\n",
    "            data.append(packet)\n",
    "            # print('received packet # {0}'.format(len(data)))\n",
    "            # time.sleep(1.0)\n",
    "        res_dict = pickle.loads(b\"\".join(data))\n",
    "        \n",
    "        s.close()\n",
    "        pmi_dict = res_dict['pmi_dict']\n",
    "        npmi_dict = res_dict['npmi_dict']\n",
    "    except:\n",
    "        # print('Failed to run NPMI calc, NPMI and PMI set to 0.0')\n",
    "        pmi_dict = dict()\n",
    "        npmi_dict = dict()\n",
    "        for k in topic_dict:\n",
    "            pmi_dict[k] = 0\n",
    "            npmi_dict[k] = 0\n",
    "            \n",
    "    return pmi_dict, npmi_dict\n",
    "\n",
    "def print_topic_with_scores(topic_json, **kwargs):\n",
    "    topic_keys = sorted(list(topic_json.keys()))\n",
    "    \n",
    "    sortby = kwargs.pop('sortby', None)\n",
    "    if sortby is None:\n",
    "        sortby = kwargs.pop('sort_by', None)\n",
    "    \n",
    "    if sortby in kwargs.keys():\n",
    "        topic_keys = sorted(kwargs[sortby], key=kwargs[sortby].get)[::-1]\n",
    "        \n",
    "    entries = []\n",
    "    dict_names = sorted(list(kwargs.keys()))\n",
    "    header_str = 'Avg scores: '\n",
    "    for dn in dict_names:\n",
    "        assert isinstance(kwargs[dn], dict)\n",
    "        # mean_dict 왜 갑자기 튀어나옴?\n",
    "        header_str += '{}: {:.2f}'.format(dn, mean_dict(kwargs[dn]))\n",
    "    for k in topic_keys:\n",
    "        score_str = []\n",
    "        for dn in dict_names:\n",
    "            score_str.append('{:.2f}'.format(kwargs[dn][k]))\n",
    "        score_str = ', '.join(score_str)\n",
    "        entries.append('T{} [{}]'.format(k, score_str) + ', '.join(topic_json[k]))\n",
    "        \n",
    "    msg = header_str + '\\n' + '\\n'.join(entries)\n",
    "    print(msg)\n",
    "    return msg\n",
    "\n",
    "def print_topics(topic_json, npmi_dict, topic_uniqs, data, print_topic_names=False):\n",
    "    for k, v in topic_json.items():\n",
    "        prefix_msg = '[ '\n",
    "        if hasattr(data, 'maps') and print_topic_names:\n",
    "            prefix_msg += data.maps['dim2topic'][k]\n",
    "        else:\n",
    "            prefix_msg += str(k)\n",
    "        if hasattr(data, 'selected_topics') and print_topic_names:\n",
    "            if data.maps['dim2topic'][k] in data.selected_topics:\n",
    "                prefix_msg += '*'\n",
    "        prefix_msg += ' - {:.5g} - {:.5g}]:'.format(topic_uniqs[k], npmi_dict[k])\n",
    "        print(prefix_msg, v)\n",
    "        \n",
    "# 아래는 사용안하는 함수\n",
    "def reverse_dict(d):\n",
    "    return {v:k for k,v in d.items()}\n",
    "\n",
    "def to_numpy(x):\n",
    "    x_npy = []\n",
    "    for x in X:\n",
    "        if isinstance(x, list):\n",
    "            x_npy += [to_numpy(x)]\n",
    "        else:\n",
    "            x_npy += [x.asnumpy()]\n",
    "    return x_npy\n",
    "\n",
    "def stack_numpy(X, xnew):\n",
    "    for i in range(len(X)):\n",
    "        if isinstance(xnew[i], list):\n",
    "            X[i] = stack_numpy(x[i], xnew[i])\n",
    "        else:\n",
    "            X[i] = np.vstack([X[i], xnew[i]])\n",
    "    return X\n",
    "\n",
    "def get_topic_words_decoder_weights(D, data, ctx, k=10, decoder_weights=False):\n",
    "    if decoder_weights:\n",
    "        params = D.collect_params()\n",
    "        params = params['decoder0_dense0_weight'].data().transpose()\n",
    "    else:\n",
    "        y = D.y_as_topics()\n",
    "        params = D(y.copyto(ctx))\n",
    "    top_word_ids = mx.nd.argsort(params, axis=1, is_ascend=False)[:, :k].asnumpy()\n",
    "    if hasattr(data, 'id_to_word'):\n",
    "        top_word_strings = [[data.is_to_word]]\n",
    "    else:\n",
    "        top_word_strings = [[data.maps['dim2vocab'][int(w)] for w in topic] for topic in top_word_ids]\n",
    "        \n",
    "    return top_word_strings\n",
    "\n",
    "def get_topic_words(D, data, ctx, k=10):\n",
    "    y, z = D.yz_as_topics()\n",
    "    if z is not None:\n",
    "        params = D(y.copyto(ctx), x.copyto(ctx))\n",
    "    else:\n",
    "        params = D(y.copyto(ctx), None)\n",
    "    top_word_ids = mx.nd.argsort(params, axis=1, is_ascend=False)[:,:k].asnumpy()\n",
    "    if hasattr(data, 'id_to_word'):\n",
    "        top_word_strings = [[data.id_to_word[int(w)] for w in topic] for topic in top_word_ids]\n",
    "    else:\n",
    "        top_word_strings = [[data.maps['dim2vocab'][int(w)] for w in topic] for topic in top_word_ids]\n",
    "\n",
    "    return top_word_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Training WAE in MXNet')\n",
    "parser.add_argument('-dom','--domain', type=str, default='twenty_news', help='domain to run', required=False)\n",
    "parser.add_argument('-data','--data_path', type=str, default='', help='file path for dataset', required=False)\n",
    "parser.add_argument('-max_labels','--max_labels', type=int, default=100, help='max number of topics to specify as labels for a single training document', required=False)\n",
    "parser.add_argument('-max_labeled_samples','--max_labeled_samples', type=int, default=10, help='max number of labeled samples per topic', required=False)\n",
    "parser.add_argument('-label_seed','--label_seed', type=lambda x: int(x) if x != 'None' else None, default=None, help='random seed for subsampling the labeled dataset', required=False)\n",
    "parser.add_argument('-mod','--model', type=str, default='dirichlet', help='model to use', required=False)\n",
    "parser.add_argument('-desc','--description', type=str, default='', help='description for the experiment', required=False)\n",
    "parser.add_argument('-alg','--algorithm', type=str, default='standard', help='algorithm to use for training: standard', required=False)\n",
    "parser.add_argument('-bs','--batch_size', type=int, default=256, help='batch_size for training', required=False)\n",
    "parser.add_argument('-opt','--optim', type=str, default='Adam', help='encoder training algorithm', required=False)\n",
    "parser.add_argument('-lr','--learning_rate', type=float, default=1e-4, help='learning rate', required=False)\n",
    "parser.add_argument('-l2','--weight_decay', type=float, default=0., help='weight decay', required=False)\n",
    "parser.add_argument('-e_nh','--enc_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for encoder or list of hiddens for each layer', required=False)\n",
    "parser.add_argument('-e_nl','--enc_n_layer', type=int, default=1, help='# of hidden layers for encoder, set to -1 if passing list of n_hiddens', required=False)\n",
    "parser.add_argument('-e_nonlin','--enc_nonlinearity', type=str, default='sigmoid', help='type of nonlinearity for encoder', required=False)\n",
    "parser.add_argument('-e_weights','--enc_weights', type=str, default='', help='file path for encoder weights', required=False)\n",
    "parser.add_argument('-e_freeze','--enc_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the encoder weights', required=False)\n",
    "parser.add_argument('-lat_nonlin','--latent_nonlinearity', type=str, default='', help='type of to use prior to decoder', required=False)\n",
    "parser.add_argument('-d_nh','--dec_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for decoder or list of hiddens for each layer', required=False)\n",
    "parser.add_argument('-d_nl','--dec_n_layer', type=int, default=0, help='# of hidden layers for decoder', required=False)\n",
    "parser.add_argument('-d_nonlin','--dec_nonlinearity', type=str, default='', help='type of nonlinearity for decoder', required=False)\n",
    "parser.add_argument('-d_weights','--dec_weights', type=str, default='', help='file path for decoder weights', required=False)\n",
    "parser.add_argument('-d_freeze','--dec_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the decoder weights', required=False)\n",
    "parser.add_argument('-d_word_dist','--dec_word_dist', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to init decoder weights with training set word distributions', required=False)\n",
    "parser.add_argument('-dis_nh','--dis_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for encoder or list of hiddens for each layer', required=False)\n",
    "parser.add_argument('-dis_nl','--dis_n_layer', type=int, default=1, help='# of hidden layers for encoder, set to -1 if passing list of n_hiddens', required=False)\n",
    "parser.add_argument('-dis_nonlin','--dis_nonlinearity', type=str, default='sigmoid', help='type of nonlinearity for discriminator', required=False)\n",
    "parser.add_argument('-dis_y_weights','--dis_y_weights', type=str, default='', help='file path for discriminator_y weights', required=False)\n",
    "parser.add_argument('-dis_z_weights','--dis_z_weights', type=str, default='', help='file path for discriminator_z weights', required=False)\n",
    "parser.add_argument('-dis_freeze','--dis_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the encoder weights', required=False)\n",
    "parser.add_argument('-include_w','--include_weights', type=str, nargs='*', default=[], help='weights to train on (default is all weights) -- all others are kept fixed; Ex: E.z_encoder D.decoder', required=False)\n",
    "parser.add_argument('-eps','--epsilon', type=float, default=1e-8, help='epsilon param for Adam', required=False)\n",
    "parser.add_argument('-mx_it','--max_iter', type=int, default=50001, help='max # of training iterations', required=False)\n",
    "parser.add_argument('-train_stats_every','--train_stats_every', type=int, default=100, help='skip train_stats_every iterations between recording training stats', required=False)\n",
    "parser.add_argument('-eval_stats_every','--eval_stats_every', type=int, default=100, help='skip eval_stats_every iterations between recording evaluation stats', required=False)\n",
    "parser.add_argument('-ndim_y','--ndim_y', type=int, default=256, help='dimensionality of y - topic indicator', required=False)\n",
    "parser.add_argument('-ndim_x','--ndim_x', type=int, default=2, help='dimensionality of p(x) - data distribution', required=False)\n",
    "parser.add_argument('-saveto','--saveto', type=str, default='', help='path prefix for saving results', required=False)\n",
    "parser.add_argument('-gpu','--gpu', type=int, default=-2, help='if/which gpu to use (-1: all, -2: None)', required=False)\n",
    "parser.add_argument('-hybrid','--hybridize', type=lambda x: (str(x).lower() == 'true'), default=False, help='declaritive True (hybridize) or imperative False', required=False)\n",
    "parser.add_argument('-full_npmi','--full_npmi', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to compute NPMI for full trajectory', required=False)\n",
    "parser.add_argument('-eot','--eval_on_test', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to evaluate on the test set (True) or validation set (False)', required=False)\n",
    "parser.add_argument('-verb','--verbose', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to print progress to stdout', required=False)\n",
    "parser.add_argument('-dirich_alpha','--dirich_alpha', type=float, default=1e-1, help='param for Dirichlet prior', required=False)\n",
    "parser.add_argument('-adverse','--adverse', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to turn on adverserial training (MMD or GAN). set to False if only train auto-encoder', required=False)\n",
    "parser.add_argument('-update_enc','--update_enc', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to update encoder for unlabed_train_op()', required=False)\n",
    "parser.add_argument('-labeled_loss_lambda','--labeled_loss_lambda', type=float, default=1.0, help='param for Dirichlet noise for label', required=False)\n",
    "parser.add_argument('-train_mode','--train_mode', type=str, default='mmd', help=\"set to mmd or adv (for GAN)\", required=False)\n",
    "parser.add_argument('-kernel_alpha','--kernel_alpha', type=float, default=1.0, help='param for information diffusion kernel', required=False)\n",
    "parser.add_argument('-recon_alpha','--recon_alpha', type=float, default=-1.0, help='multiplier of the reconstruction loss when combined with mmd loss', required=False)\n",
    "parser.add_argument('-recon_alpha_adapt','--recon_alpha_adapt', type=float, default=-1.0, help='adaptively change recon_alpha so that [total loss = mmd + recon_alpha_adapt * recon loss], set to -1 if no adapt', required=False)\n",
    "parser.add_argument('-dropout_p','--dropout_p', type=float, default=-1.0, help='dropout probability in encoder', required=False)\n",
    "parser.add_argument('-l2_alpha','--l2_alpha', type=float, default=-1.0, help='alpha multipler for L2 regularization on latent vector', required=False)\n",
    "parser.add_argument('-latent_noise','--latent_noise', type=float, default=0.0, help='proportion of dirichlet noise added to the latent vector after softmax', required=False)\n",
    "parser.add_argument('-topic_decoder_weight','--topic_decoder_weight', type=lambda x: (str(x).lower() == 'true'), default=False, help='extract topic words based on decoder weights or decoder outputs', required=False)\n",
    "parser.add_argument('-retrain_enc_only','--retrain_enc_only', type=lambda x: (str(x).lower() == 'true'), default=False, help='only retrain the encoder for reconstruction loss', required=False)\n",
    "parser.add_argument('-l2_alpha_retrain','--l2_alpha_retrain', type=float, default=0.1, help='alpha multipler for L2 regularization on encoder output during retraining', required=False)\n",
    "args = vars(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, io\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.metrics import log_loss, v_measure_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    '''\n",
    "    Data Generator object. Main functionality is contained in ``minibatch'' method\n",
    "    and ``subsampled_labeled_data'' if training in a semi-supervised fashion.\n",
    "    Introducing new datasets requires implementing ``load'' and possibly overwriting\n",
    "    portions of ``__init__''.\n",
    "    '''\n",
    "    def __init__(self, batch_size=1, data_path='', ctx=mx.cpu(0)):\n",
    "        '''\n",
    "        Constructor for Data.\n",
    "        Args\n",
    "        ----\n",
    "        batch_size: int, default 1\n",
    "          An integer specifying the batch size - required for precompiling the graph.\n",
    "        data_path: string, default ''\n",
    "          This is primarily used by Mulan to specify which dataset to load from Mulan,\n",
    "          e.g., data_path='bibtex'.\n",
    "        ctx: mxnet device context, default mx.cpu(0)\n",
    "          Which device to store/run the data and model on.\n",
    "        Returns\n",
    "        -------\n",
    "        Data object\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        if data_path == '':\n",
    "            data, labels, maps = self.load()\n",
    "        else:\n",
    "            data, labels, maps = self.load(data_path)\n",
    "        self.ctx = ctx\n",
    "        # # normalize the data:\n",
    "        # def softmax(x):\n",
    "        #     \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        #     e_x = np.exp(x - np.max(x, axis=1).reshape((-1,1)))\n",
    "        #     return e_x / np.sum(e_x, axis=1).reshape((-1,1))\n",
    "        # for i in range(len(data)):\n",
    "        #     data[i] = softmax(data[i])\n",
    "\n",
    "        data_names = ['train','valid','test','train_with_labels','valid_with_labels','test_with_labels']\n",
    "        label_names = ['train_label', 'valid_label', 'test_label']\n",
    "\n",
    "        self.data = dict(zip(data_names, data))\n",
    "        self.labels = dict(zip(label_names, labels))\n",
    "\n",
    "        # repeat data to at least match batch_size\n",
    "        for k, v in self.data.items():\n",
    "            if v is not None and v.shape[0] < self.batch_size:\n",
    "                print('NOTE: Number of samples for {0} is smaller than batch_size ({1}<{2}). Duplicating samples to exceed batch_size.'.format(k,v.shape[0],self.batch_size))\n",
    "                if type(v) is np.ndarray:\n",
    "                    self.data[k] = np.tile(v, (self.batch_size // v.shape[0] + 1, 1))\n",
    "                else:\n",
    "                    self.data[k] = mx.nd.tile(v, (self.batch_size // v.shape[0] + 1, 1))\n",
    "\n",
    "        for k, v in self.labels.items():\n",
    "            if v is not None and v.shape[0] < self.batch_size:\n",
    "                print('NOTE: Number of samples for {0} is smaller than batch_size ({1}<{2}). Duplicating samples to exceed batch_size.'.format(k,v.shape[0],self.batch_size))\n",
    "                self.labels[k] = np.tile(v, (self.batch_size // v.shape[0] + 1, ))\n",
    "\n",
    "        map_names = ['vocab2dim','dim2vocab','topic2dim','dim2topic']\n",
    "        self.maps = dict(zip(map_names, maps))\n",
    "        dls = [self.dataloader(d, batch_size) for d in data]\n",
    "        dis = [iter(dl) if dl is not None else None for dl in dls]\n",
    "        self.dataloaders = dict(zip(data_names, dls))\n",
    "        self.dataiters = dict(zip(data_names, dis))\n",
    "        self.wasreset = dict(zip(data_names, np.ones(len(data_names), dtype='bool')))\n",
    "\n",
    "        self.data_dim = self.data['train'].shape[1]\n",
    "        if self.data['train_with_labels'] is not None:\n",
    "            self.label_dim = self.data['train_with_labels'].shape[1] - self.data['train'].shape[1]\n",
    "\n",
    "\n",
    "    def dataloader(self, data, batch_size, shuffle=True):\n",
    "        '''\n",
    "        Constructs a data loader for generating minibatches of data.\n",
    "        Args\n",
    "        ----\n",
    "        data: numpy array, no default\n",
    "          The data from which to load minibatches.\n",
    "        batch_size: integer, no default\n",
    "          The # of samples returned in each minibatch.\n",
    "        shuffle: boolean, default True\n",
    "          Whether or not to shuffle the data prior to returning the data loader.\n",
    "        Returns\n",
    "        -------\n",
    "        DataLoader: A gluon DataLoader iterator\n",
    "        '''\n",
    "        if data is None:\n",
    "            return None\n",
    "        else:\n",
    "            # inds = np.arange(data.shape[0])\n",
    "            # if shuffle:\n",
    "            #     np.random.shuffle(inds)\n",
    "            # ordered = data[inds]\n",
    "            # N, r = divmod(data.shape[0], batch_size)\n",
    "            # if r > 0:\n",
    "            #     ordered = np.vstack([ordered, ordered[:r]])\n",
    "            if type(data) is np.ndarray:\n",
    "                return gluon.data.DataLoader(data, batch_size, last_batch='discard', shuffle=shuffle)\n",
    "            else:\n",
    "                return io.NDArrayIter(data={'data': data}, batch_size=batch_size, shuffle=shuffle, last_batch_handle='discard')\n",
    "\n",
    "    def force_reset_data(self, key, shuffle=True):\n",
    "        '''\n",
    "        Resets minibatch index to zero to restart an epoch.\n",
    "        Args\n",
    "        ----\n",
    "        key: string, no default\n",
    "          Required to select appropriate data in ``data'' object,\n",
    "          e.g., 'train', 'test', 'train_with_labels', 'test_with_labels'.\n",
    "        shuffle: boolean, default True\n",
    "          Whether or not to shuffle the data prior to returning the data loader.\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing.\n",
    "        '''\n",
    "        if self.data[key] is not None:\n",
    "            if type(self.data[key]) is np.ndarray:\n",
    "                self.dataloaders[key] = self.dataloader(self.data[key], self.batch_size, shuffle)\n",
    "                self.dataiters[key] = iter(self.dataloaders[key])\n",
    "            else:\n",
    "                self.dataiters[key].hard_reset()\n",
    "            self.wasreset[key] = True\n",
    "\n",
    "    def minibatch(self, key, pad_width=0):\n",
    "        '''\n",
    "        Returns a minibatch of data (stored on device self.ctx).\n",
    "        Args\n",
    "        ----\n",
    "        key: string, no default\n",
    "          Required to select appropriate data in ``data'' object,\n",
    "          e.g., 'train', 'test', 'train_with_labels', 'test_with_labels'.\n",
    "        pad_width: integer, default 0\n",
    "          The amount to zero-pad the labels to match the dimensionality of z.\n",
    "        Returns\n",
    "        -------\n",
    "        minibatch: NDArray on device self.ctx\n",
    "          An NDArray of size batch_size x # of features.\n",
    "        '''\n",
    "        if self.dataiters[key] is None:\n",
    "            return None\n",
    "        else:\n",
    "            if type(self.data[key]) is np.ndarray:\n",
    "                try:\n",
    "                    mb = self.dataiters[key].__next__().reshape((self.batch_size, -1))\n",
    "                    if pad_width > 0:\n",
    "                        mb = mx.nd.concat(mb, mx.nd.zeros((self.batch_size, pad_width)))\n",
    "                    return mb.copyto(self.ctx)\n",
    "                except:\n",
    "                    self.force_reset_data(key)\n",
    "                    mb = self.dataiters[key].__next__().reshape((self.batch_size, -1))\n",
    "                    if pad_width > 0:\n",
    "                        mb = mx.nd.concat(mb, mx.nd.zeros((self.batch_size, pad_width)))\n",
    "                    return mb.copyto(self.ctx)\n",
    "            else:\n",
    "                try:\n",
    "                    mb = self.dataiters[key].__next__().data[0].as_in_context(self.ctx)\n",
    "                    return mb\n",
    "                except:\n",
    "                    self.dataiters[key].hard_reset()\n",
    "                    mb = self.dataiters[key].__next__().data[0].as_in_context(self.ctx)\n",
    "                    return mb\n",
    "\n",
    "    def get_documents(self, key, split_on=None):\n",
    "        '''\n",
    "        Retrieves a minibatch of documents via ``data'' object parameter.\n",
    "        Args\n",
    "        ----\n",
    "        key: string, no default\n",
    "          Required to select appropriate data in ``data'' object,\n",
    "          e.g., 'train', 'test', 'train_with_labels', 'test_with_labels'.\n",
    "        split_on: integer, default None\n",
    "          Useful if self.data[key] contains both data and labels in one\n",
    "          matrix and want to split them, e.g., split_on = data_dim.\n",
    "        Returns\n",
    "        -------\n",
    "        minibatch: NDArray if split_on is None, else [NDarray, NDArray]\n",
    "        '''\n",
    "        if 'labels' in key:\n",
    "            batch = self.minibatch(key, pad_width=self.label_pad_width)\n",
    "        else:\n",
    "            batch = self.minibatch(key)\n",
    "        if split_on is not None:\n",
    "            batch, labels = batch[:,:split_on], batch[:,split_on:]\n",
    "            return batch, labels\n",
    "        else:\n",
    "            return batch\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_series(y, ylabel, file, args, iteration, total_samples, labels=None):\n",
    "        '''\n",
    "        Plots and saves a figure of y vs iterations and epochs to file.\n",
    "        Args\n",
    "        ----\n",
    "        y: a list (of lists) or numpy array, no default\n",
    "          A list (of possibly another list) of numbers to plot.\n",
    "        ylabel: string, no default\n",
    "          The label for the y-axis.\n",
    "        file: string, no default\n",
    "          A path with filename to save the figure to.\n",
    "        args: dictionary, no default\n",
    "          A dictionary of model, training, and evaluation specifications.\n",
    "        iteration: integer, no default\n",
    "          The current iteration in training.\n",
    "        total_samples: integer, no default\n",
    "          The total number of samples in the dataset - used along with batch_size\n",
    "          to convert iterations to epochs.\n",
    "        labels: list of strings, default None\n",
    "          If y is a list of lists, the labels contains names for each element\n",
    "          in the nested list. This is used to create an appropriate legend\n",
    "          for the plot.\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing.\n",
    "        '''\n",
    "        if len(y) > 0:\n",
    "            fig = plt.figure()\n",
    "            ax = plt.subplot(111)\n",
    "            x = np.linspace(0, iteration, num=len(y)) * args['batch_size'] / total_samples\n",
    "            y = np.array(y)\n",
    "            if len(y.shape) > 1:\n",
    "                for i in range(y.shape[1]):\n",
    "                    if labels is None:\n",
    "                        plt.plot(x,y[:,i])\n",
    "                    else:\n",
    "                        plt.plot(x,y[:,i], label=labels[i])\n",
    "            else:\n",
    "                plt.plot(x,y)\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            plt.grid(True)\n",
    "\n",
    "            ax2 = ax.twiny()\n",
    "\n",
    "            # https://pythonmatplotlibtips.blogspot.com/2018/01/add-second-x-axis-below-first-x-axis-python-matplotlib-pyplot.html\n",
    "            # Decide the ticklabel position in the new x-axis,\n",
    "            # then convert them to the position in the old x-axis\n",
    "            # xticks list seems to be padded with extra lower and upper ticks --> subtract 2 from length\n",
    "            newlabel = np.around(np.linspace(0, iteration, num=len(ax.get_xticks())-2)).astype('int') # labels of the xticklabels: the position in the new x-axis\n",
    "            # ax2.set_xticks(ax.get_xticks())\n",
    "            ax2.set_xticks(newlabel * args['batch_size'] / total_samples)\n",
    "            ax2.set_xticklabels(newlabel//1000)\n",
    "\n",
    "            ax2.xaxis.set_ticks_position('bottom') # set the position of the second x-axis to bottom\n",
    "            ax2.xaxis.set_label_position('bottom') # set the position of the second x-axis to bottom\n",
    "            ax2.spines['bottom'].set_position(('outward', 36))\n",
    "            ax2.set_xlabel('Thousand Iterations')\n",
    "            ax2.set_xlim(ax.get_xlim())\n",
    "\n",
    "            if labels is not None:\n",
    "                lgd = ax.legend(loc='center left', bbox_to_anchor=(1.05, 1))\n",
    "                fig.savefig(args['saveto']+file, additional_artists=[lgd], bbox_inches='tight')\n",
    "            else:\n",
    "                fig.tight_layout()\n",
    "                fig.savefig(args['saveto']+file)\n",
    "            plt.close()\n",
    "\n",
    "    def load(self, path=''):\n",
    "        '''\n",
    "        Loads data and maps from path.\n",
    "        Args\n",
    "        ----\n",
    "        path: string, default ''\n",
    "          A path to the data file.\n",
    "        Returns\n",
    "        -------\n",
    "        data: list of numpy arrays\n",
    "          A list of the different subsets of data, e.g.,\n",
    "          `train', `test', 'train_with_labels', 'test_with_labels'.\n",
    "        maps: list of dictionaries\n",
    "          A list of dictionaries for mapping between dimensions and strings,\n",
    "          e.g., 'vocab2dim', 'dim2vocab', 'topic2dim', 'dim2topic'.\n",
    "        '''\n",
    "        data = [np.empty((1,1)) for data in ['train','valid','test','train_with_labels','valid_with_labels','test_with_labels']]\n",
    "        maps = [{'a':0}, {0:'a'}, {'Letters':0}, {0:'Letters'}]\n",
    "        self.data_path = path + '***.npz'\n",
    "        return data, maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wikitext103(Data):\n",
    "    def __init__(self, batch_size, data_path='', ctx=None, saveto='', **kwargs):\n",
    "        self.saveto = saveto\n",
    "        super(Wikitext103, self).__init__(batch_size, data_path, ctx)\n",
    "\n",
    "    def load(self, path='./data/wikitext-103', features='BoW', match_avitm=True):\n",
    "        if path[:2] == '~/':\n",
    "            path = os.path.join(os.path.expanduser(path[:2]), path[2:])\n",
    "\n",
    "        ### Specify the file locations\n",
    "        train_path = path + '/wikitext-103_tra.csr.npz'\n",
    "        test_path = path + '/wikitext-103_test.csr.npz'\n",
    "        vocab_path = path + '/vocab.txt'\n",
    "\n",
    "        ### Load train\n",
    "        train_csr = sparse.load_npz(train_path)\n",
    "        train = np.array(train_csr.todense()).astype('float32')\n",
    "\n",
    "        ### Load test\n",
    "        test_csr = sparse.load_npz(test_path)\n",
    "        test = np.array(test_csr.todense()).astype('float32')\n",
    "\n",
    "        ### load vocab\n",
    "        ENCODING = \"ISO-8859-1\"\n",
    "        # ENCODING = \"utf-8\"\n",
    "        with open(vocab_path, encoding=ENCODING) as f:\n",
    "             vocab_list = [line.strip('\\n') for line in f]\n",
    "\n",
    "        # construct maps\n",
    "        vocab2dim = dict(zip(vocab_list, range(len(vocab_list))))\n",
    "        dim2vocab = reverse_dict(vocab2dim)\n",
    "\n",
    "        return [train, None, test, None, None, None], [None, None, None], [vocab2dim, dim2vocab, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit, expit\n",
    "\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENet(gluon.HybridBlock):\n",
    "    '''\n",
    "    A gluon HybridBlock Encoder (skeleton) class.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor for Encoder.\n",
    "        Args\n",
    "        ----\n",
    "        None\n",
    "        Returns\n",
    "        -------\n",
    "        Encoder object\n",
    "        '''\n",
    "        super(ENet, self).__init__()\n",
    "            \n",
    "    def hybrid_forward(self, x):\n",
    "        '''\n",
    "        Encodes x.\n",
    "        Args\n",
    "        ----\n",
    "        x: mx.NDArray or sym, No default\n",
    "          Input to encoder.\n",
    "        Returns (should)\n",
    "        -------\n",
    "        params: list of NDArray or sym\n",
    "          parameters for the encoding distribution\n",
    "        samples: NDArray or sym\n",
    "          samples drawn from the encoding distribution\n",
    "        '''\n",
    "        raise NotImplementedError('Need to write your own Encoder that inherits from ENet. Put this file in models/.')\n",
    "\n",
    "    def init_weights(self, weights=None):\n",
    "        '''\n",
    "        Initializes the encoder weights. Default is Xavier initialization.\n",
    "        Args\n",
    "        ----\n",
    "        weights: list of numpy arrays, No default\n",
    "          Weights to load into the model. Not required. Preference is to\n",
    "          load weights from file.\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing.\n",
    "        '''\n",
    "        loaded = False\n",
    "        source = 'keyword argument'\n",
    "        if self.weights_file != '' and weights is None:\n",
    "            try:\n",
    "                self.load_params(self.weights_file, self.model_ctx)\n",
    "                source = 'mxnet weights file: '+self.weights_file\n",
    "                print('NOTE: Loaded encoder weights from '+source+'.')\n",
    "                if self.freeze:\n",
    "                    self.freeze_params()\n",
    "                    print('NOTE: Froze encoder weights from '+source+'.')\n",
    "                weights = None\n",
    "                loaded = True\n",
    "            except:\n",
    "                weights = pickle.load(open(self.weights_file,'rb'))\n",
    "                source = 'pickle file: '+self.weights_file\n",
    "        if weights is not None:\n",
    "            assert self.n_layers == 0\n",
    "            for p,w in zip(self.collect_params().values(), weights):\n",
    "                if w is not None:\n",
    "                    p.initialize(mx.init.Constant(mx.nd.array(w.squeeze())), ctx=self.model_ctx)\n",
    "                    if self.freeze:\n",
    "                        p.lr_mult = 0.\n",
    "            print('NOTE: Loaded encoder weights from '+source+'.')\n",
    "            if self.freeze:\n",
    "                print('NOTE: Froze encoder weights from '+source+'.')\n",
    "            loaded = True\n",
    "        if not loaded:\n",
    "            self.collect_params().initialize(mx.init.Xavier(), ctx=self.model_ctx)\n",
    "            print('NOTE: Randomly initialized encoder weights.')\n",
    "            # self.collect_params().initialize(mx.init.Zero(), ctx=self.model_ctx)\n",
    "            # print('NOTE: initialized encoder weights to ZERO.')\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for p in self.collect_params().values():\n",
    "            p.lr_mult = 0.\n",
    "\n",
    "\n",
    "class DNet(gluon.HybridBlock):\n",
    "    '''\n",
    "    A gluon HybridBlock Decoder (skeleton) class.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor for Decoder.\n",
    "        Args\n",
    "        ----\n",
    "        None\n",
    "        Returns\n",
    "        -------\n",
    "        Decoder object\n",
    "        '''\n",
    "        super(DNet, self).__init__()\n",
    "\n",
    "    def hybrid_forward(self, y, z):\n",
    "        '''\n",
    "        Decodes x.\n",
    "        Args\n",
    "        ----\n",
    "        x: mx.NDArray or sym, no default\n",
    "          Input to decoder.\n",
    "        Returns (should)\n",
    "        -------\n",
    "        params: list of NDArray or sym\n",
    "          parameters for the encoding distribution\n",
    "        samples: NDArray or sym\n",
    "          samples drawn from the encoding distribution. None if sampling is not implemented.\n",
    "        '''\n",
    "        raise NotImplementedError('Need to write your own Decoder that inherits from ENet. Put this file in models/.')\n",
    "\n",
    "    def init_weights(self, weights=None):\n",
    "        '''\n",
    "        Initializes the decoder weights. Default is Xavier initialization.\n",
    "        Args\n",
    "        ----\n",
    "        weights: list of numpy arrays, No default\n",
    "          Weights to load into the model. Not required. Preference is to\n",
    "          load weights from file.\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing.\n",
    "        '''\n",
    "        loaded = False\n",
    "        source = 'keyword argument'\n",
    "        if self.weights_file != '' and weights is None:\n",
    "            try:\n",
    "                self.load_params(self.weights_file, self.model_ctx)\n",
    "                source = 'mxnet weights file: '+self.weights_file\n",
    "                print('NOTE: Loaded decoder weights from '+source+'.')\n",
    "                if self.freeze:\n",
    "                    self.freeze_params()\n",
    "                    print('NOTE: Froze decoder weights from '+source+'.')\n",
    "                weights = None\n",
    "                loaded = True\n",
    "            except:\n",
    "                weights = pickle.load(open(self.weights_file,'rb'))\n",
    "                source = 'pickle file: '+self.weights_file\n",
    "        if weights is not None:\n",
    "            assert self.n_layers == 0\n",
    "            for p,w in zip(self.collect_params().values(), weights):\n",
    "                if w is not None:\n",
    "                    p.initialize(mx.init.Constant(mx.nd.array(w.squeeze())), ctx=self.model_ctx)\n",
    "                    if self.freeze:\n",
    "                        p.lr_mult = 0.\n",
    "            print('NOTE: Loaded decoder weights from '+source+'.')\n",
    "            if self.freeze:\n",
    "                print('NOTE: Froze decoder weights from '+source+'.')\n",
    "            loaded = True\n",
    "        if not loaded:\n",
    "            self.collect_params().initialize(mx.init.Xavier(), ctx=self.model_ctx)\n",
    "            print('NOTE: Randomly initialized decoder weights.')\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for p in self.collect_params().values():\n",
    "            p.lr_mult = 0.\n",
    "\n",
    "\n",
    "class Compute(object):\n",
    "    '''\n",
    "    Skeleton class to manage training, testing, and retrieving outputs.\n",
    "    See ``compute_op.py'' for ``flesh''.\n",
    "    '''\n",
    "    def __init__(self,  data, Enc, Dec,  Dis_y, args):\n",
    "        '''\n",
    "        Constructor for Compute.\n",
    "        Returns\n",
    "        -------\n",
    "        Compute object\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.Enc = Enc\n",
    "        self.Dec = Dec\n",
    "        self.Dis_y = Dis_y\n",
    "        self.args = args\n",
    "        self.model_ctx = Enc.model_ctx\n",
    "        self.ndim_y = args['ndim_y']\n",
    "\n",
    "        weights_enc = Enc.collect_params()\n",
    "        weights_dec = Dec.collect_params()\n",
    "        weights_dis_y = Dis_y.collect_params()\n",
    "\n",
    "        if self.args['optim'] == 'Adam':\n",
    "            # args_dict = {'learning_rate': self.args['learning_rate'], 'beta1': self.args['betas'][0], 'beta2': self.args['betas'][1], 'epsilon': self.args['epsilon']}\n",
    "            # optimizer_enc = gluon.Trainer(weights_enc, 'adam', args_dict)\n",
    "            # optimizer_dec = gluon.Trainer(weights_dec, 'adam', args_dict)\n",
    "            # optimizer_dis_y = gluon.Trainer(weights_dis_y, 'adam', args_dict)\n",
    "            optimizer_enc = gluon.Trainer(weights_enc, 'adam', {'learning_rate': self.args['learning_rate'], 'beta1': 0.99})\n",
    "            optimizer_dec = gluon.Trainer(weights_dec, 'adam', {'learning_rate': self.args['learning_rate'], 'beta1': 0.99})\n",
    "            optimizer_dis_y = gluon.Trainer(weights_dis_y, 'adam', {'learning_rate': self.args['learning_rate']})\n",
    "        if self.args['optim'] == 'Adadelta':\n",
    "            # note: learning rate has no effect on Adadelta --> https://mxnet.incubator.apache.org/_modules/mxnet/optimizer.html#AdaDelta\n",
    "            args_dict = {'rescale_grad': 1}  #, 'clip_gradient': 0.1}\n",
    "            optimizer_enc = gluon.Trainer(weights_enc, 'adadelta', args_dict)\n",
    "            optimizer_dec = gluon.Trainer(weights_dec, 'adadelta', args_dict)\n",
    "            optimizer_dis_y = gluon.Trainer(weights_dis_y, 'adadelta', args_dict)\n",
    "        if self.args['optim'] == 'RMSprop':\n",
    "            args_dict = {'learning_rate': self.args['learning_rate'], 'epsilon': 1e-10, 'alpha': 0.9}\n",
    "            optimizer_enc = gluon.Trainer(weights_enc, 'rmsprop', args_dict)\n",
    "            optimizer_dec = gluon.Trainer(weights_dec, 'rmsprop', args_dict)\n",
    "            optimizer_dis_y = gluon.Trainer(weights_dis_y, 'rmsprop', args_dict)\n",
    "        if self.args['optim'] == 'SGD':\n",
    "            args_dict = {'learning_rate': self.args['learning_rate'], 'wd': self.args['weight_decay'], 'rescale_grad': 1., 'momentum': 0.0, 'lazy_update': False}\n",
    "            optimizer_enc = gluon.Trainer(weights_enc, 'sgd', args_dict)\n",
    "            optimizer_dec = gluon.Trainer(weights_dec, 'sgd', args_dict)\n",
    "            optimizer_dis_y = gluon.Trainer(weights_dis_y, 'sgd', args_dict)\n",
    "\n",
    "        self.optimizer_enc = optimizer_enc\n",
    "        self.optimizer_dec = optimizer_dec\n",
    "        self.optimizer_dis_y = optimizer_dis_y\n",
    "        self.weights_enc = weights_enc\n",
    "        self.weights_dec = weights_dec\n",
    "        self.weights_dis_y = weights_dis_y\n",
    "\n",
    "    def train_op(self):\n",
    "        '''\n",
    "        Trains the model using one minibatch of data.\n",
    "        '''\n",
    "        return None, None, None, None\n",
    "\n",
    "    def test_op(self, num_samples=None, num_epochs=None, reset=True, dataset='test'):\n",
    "        '''\n",
    "        Evaluates the model using num_samples.\n",
    "        Args\n",
    "        ----\n",
    "        num_samples: integer, default None\n",
    "          The number of samples to evaluate on. This is converted to\n",
    "          evaluating on (num_samples // batch_size) minibatches.\n",
    "        num_epochs: integer, default None\n",
    "          The number of epochs to evaluate on. This used if num_samples\n",
    "          is not specified. If neither is specified, defaults to 1 epoch.\n",
    "        reset: bool, default True\n",
    "          Whether to reset the test data index to 0 before iterating\n",
    "          through and evaluating on minibatches.\n",
    "        dataset: string, default 'test':\n",
    "          Which dataset to evaluate on: 'valid' or 'test'.\n",
    "        '''\n",
    "        if num_samples is None:\n",
    "            num_samples = self.data.data[dataset].shape[0]\n",
    "\n",
    "        if reset:\n",
    "            # Reset Data to Index Zero\n",
    "            self.data.force_reset_data(dataset)\n",
    "            self.data.force_reset_data(dataset+'_with_labels')\n",
    "\n",
    "        return None, None, None, None\n",
    "\n",
    "    def get_outputs(self, num_samples=None, num_epochs=None, reset=True, dataset='test'):\n",
    "        '''\n",
    "        Retrieves raw outputs from model for num_samples.\n",
    "        Args\n",
    "        ----\n",
    "        num_samples: integer, default None\n",
    "          The number of samples to evaluate on. This is converted to\n",
    "          evaluating on (num_samples // batch_size) minibatches.\n",
    "        num_epochs: integer, default None\n",
    "          The number of epochs to evaluate on. This used if num_samples\n",
    "          is not specified. If neither is specified, defaults to 1 epoch.\n",
    "        reset: bool, default True\n",
    "          Whether to reset the test data index to 0 before iterating\n",
    "          through and evaluating on minibatches.\n",
    "        dataset: string, default 'test':\n",
    "          Which dataset to evaluate on: 'valid' or 'test'.\n",
    "        '''\n",
    "        if num_samples is None:\n",
    "            num_samples = self.data.data[dataset].shape[0]\n",
    "\n",
    "        if reset:\n",
    "            # Reset Data to Index Zero\n",
    "            self.data.force_reset_data(dataset)\n",
    "            self.data.force_reset_data(dataset+'_with_labels')\n",
    "\n",
    "        return None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(ENet):\n",
    "    '''\n",
    "    A gluon HybridBlock Encoder class\n",
    "    '''\n",
    "    def __init__(self, model_ctx, batch_size, input_dim, n_hidden=64, ndim_y=16, ndim_z=10, n_layers=0, nonlin=None,\n",
    "                 weights_file='', freeze=False, latent_nonlin='sigmoid', **kwargs):\n",
    "        '''\n",
    "        Constructor for encoder.\n",
    "        Args\n",
    "        ----\n",
    "        model_ctx: mxnet device context, No default\n",
    "          Which device to store/run the data and model on.\n",
    "        batch_size: integer, No default\n",
    "          The minibatch size.\n",
    "        input_dim: integer, No default\n",
    "          The data dimensionality that is input to the encoder.\n",
    "        n_hidden: integer or list, default 64\n",
    "          If integer, specifies the number of hidden units in\n",
    "          every hidden layer.\n",
    "          If list, each element specifies the number of hidden\n",
    "          units in each hidden layer.\n",
    "        output_dim: integer, default 10\n",
    "          The dimensionality of the latent space, z.\n",
    "        n_layers: integer, default 0\n",
    "          The number of hidden layers.\n",
    "        nonlin: string, default None\n",
    "          The nonlinearity to use in every hidden layer.\n",
    "        weights_file: string, default ''\n",
    "          The path to the file (mxnet params file or pickle file)\n",
    "          containing weights for each layer of the encoder.\n",
    "        freeze: boolean, default False\n",
    "          Whether to freeze the encoder weights (MIGHT BE BROKEN).\n",
    "        latent_nonlin: string, default 'sigmoid'\n",
    "          Which space to use for the latent variable:\n",
    "            if 'sigmoid': z in (0,1)\n",
    "            else: z in (-inf,inf)\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        encoder object.\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        if n_layers >= 0:\n",
    "            if isinstance(n_hidden, list):\n",
    "                n_hidden = n_hidden[0]\n",
    "                print('NOTE: Encoder ignoring list of hiddens because n_layer >= 0. Just using first element.')\n",
    "            n_hidden = n_layers*[n_hidden]\n",
    "        else:\n",
    "            n_layers = len(n_hidden)\n",
    "            print('NOTE: Encoder reading n_hidden as list.')\n",
    "\n",
    "        if nonlin == '':\n",
    "            nonlin = None\n",
    "        \n",
    "        in_units = input_dim\n",
    "        with self.name_scope():\n",
    "            self.main = nn.HybridSequential(prefix='encoder')\n",
    "            for i in range(n_layers):\n",
    "                self.main.add(nn.Dense(n_hidden[i], in_units=in_units, activation=nonlin))\n",
    "                in_units = n_hidden[i]\n",
    "            self.main.add(nn.Dense(ndim_y, in_units=in_units, activation=None))\n",
    "\n",
    "        self.model_ctx = model_ctx\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.ndim_y = ndim_y\n",
    "        self.ndim_z = ndim_z\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.nonlin = nonlin\n",
    "        self.latent_nonlin = latent_nonlin\n",
    "        self.weights_file = weights_file\n",
    "        self.freeze = freeze\n",
    "        self.dist_params = [None]\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        '''\n",
    "        Passes the input through the encoder.\n",
    "        Args\n",
    "        ----\n",
    "        F: mxnet.nd or mxnet.sym, No default\n",
    "          This will be passed implicitly when calling hybrid forward.\n",
    "        x: NDarray or mxnet symbol, No default\n",
    "          The input to the encoder.\n",
    "        Returns\n",
    "        -------\n",
    "        dist_params: list\n",
    "          A list of the posterior parameters as NDarrays, each being of size batch_size x z_dim.\n",
    "        samples: NDarray\n",
    "          The posterior samples as a batch_size x z_dim NDarray.\n",
    "        '''\n",
    "\n",
    "        y = self.main(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Decoder(DNet):\n",
    "    '''\n",
    "    A gluon HybridBlock Decoder class with Multinomial likelihood, p(x|z).\n",
    "    '''\n",
    "    def __init__(self, model_ctx, batch_size, output_dim, ndim_y=16,  n_hidden=64, n_layers=0, nonlin='',\n",
    "                 weights_file='', freeze=False, latent_nonlin='', **kwargs):\n",
    "        '''\n",
    "        Constructor for Multinomial decoder.\n",
    "        Args\n",
    "        ----\n",
    "        model_ctx: mxnet device context, No default\n",
    "          Which device to store/run the data and model on.\n",
    "        batch_size: integer, No default\n",
    "          The minibatch size.\n",
    "        n_hidden: integer or list, default 64\n",
    "          If integer, specifies the number of hidden units in\n",
    "          every hidden layer.\n",
    "          If list, each element specifies the number of hidden\n",
    "          units in each hidden layer.\n",
    "        output_dim: integer, No default\n",
    "          The dimensionality of the latent space, z.\n",
    "        n_layers: integer, default 0\n",
    "          The number of hidden layers.\n",
    "        nonlin: string, default 'sigmoid'\n",
    "          The nonlinearity to use in every hidden layer.\n",
    "        weights_file: string, default ''\n",
    "          The path to the file (mxnet params file or pickle file)\n",
    "          containing weights for each layer of the encoder.\n",
    "        freeze: boolean, default False\n",
    "          Whether to freeze the encoder weights (MIGHT BE BROKEN).\n",
    "        latent_nonlin: string, default 'sigmoid'\n",
    "          Which space to use for the latent variable:\n",
    "            if 'sigmoid': z in (0,1)\n",
    "            else: z in (-inf,inf)\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        Multinomial decoder object.\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        if n_layers >= 0:\n",
    "            if isinstance(n_hidden, list):\n",
    "                n_hidden = n_hidden[0]\n",
    "                print('NOTE: Decoder ignoring list of hiddens because n_layer >= 0. Just using first element.')\n",
    "            n_hidden = n_layers*[n_hidden]\n",
    "        else:\n",
    "            n_layers = len(n_hidden)\n",
    "            print('NOTE: Decoder reading n_hidden as list.')\n",
    "\n",
    "        if nonlin == '':\n",
    "            nonlin = None\n",
    "\n",
    "        in_units = n_hidden[0]\n",
    "        with self.name_scope():\n",
    "            self.main = nn.HybridSequential(prefix='decoder')\n",
    "            self.main.add(nn.Dense(n_hidden[0], in_units=ndim_y, activation=None))\n",
    "\n",
    "        self.model_ctx = model_ctx\n",
    "        self.batch_size = batch_size\n",
    "        self.ndim_y = ndim_y\n",
    "        self.n_hidden = n_hidden\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.nonlin = nonlin\n",
    "        self.latent_nonlin = latent_nonlin\n",
    "        self.weights_file = weights_file\n",
    "        self.freeze = freeze\n",
    "\n",
    "    def hybrid_forward(self, F, y):\n",
    "        '''\n",
    "        Passes the input through the decoder.\n",
    "        Args\n",
    "        ----\n",
    "        F: mxnet.nd or mxnet.sym, No default\n",
    "          This will be passed implicitly when calling hybrid forward.\n",
    "        x: NDarray or mxnet symbol, No default\n",
    "          The input to the decoder.\n",
    "        Returns\n",
    "        -------\n",
    "        dist_params: list\n",
    "          A list of the multinomial parameters as NDarrays, each being of size batch_size x z_dim.\n",
    "        samples: NDarray\n",
    "          The multinomial samples as a batch_size x z_dim NDarray (NOT IMPLEMENTED).\n",
    "        '''\n",
    "        out = self.main(y)\n",
    "        return out\n",
    "\n",
    "    def y_as_topics(self, eps=1e-10):\n",
    "        y = np.eye(self.ndim_y)\n",
    "        return mx.nd.array(y)\n",
    "\n",
    "class Discriminator_y(ENet):\n",
    "    '''\n",
    "    A gluon HybridBlock Discriminator Class for y\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model_ctx, batch_size, output_dim=2, ndim_y=16, n_hidden=64, n_layers=0, nonlin='sigmoid',\n",
    "                 weights_file='', freeze=False, latent_nonlin='sigmoid', apply_softmax=False, **kwargs):\n",
    "        '''\n",
    "        Constructor for Discriminator Class for y.\n",
    "        Args\n",
    "        ----\n",
    "        model_ctx: mxnet device context, No default\n",
    "          Which device to store/run the data and model on.\n",
    "        batch_size: integer, No default\n",
    "          The minibatch size.\n",
    "        n_hidden: integer or list, default 64\n",
    "          If integer, specifies the number of hidden units in\n",
    "          every hidden layer.\n",
    "          If list, each element specifies the number of hidden\n",
    "          units in each hidden layer.\n",
    "        output_dim: integer, No default\n",
    "          The dimensionality of the latent space, z.\n",
    "        n_layers: integer, default 0\n",
    "          The number of hidden layers.\n",
    "        nonlin: string, default 'sigmoid'\n",
    "          The nonlinearity to use in every hidden layer.\n",
    "        weights_file: string, default ''\n",
    "          The path to the file (mxnet params file or pickle file)\n",
    "          containing weights for each layer of the encoder.\n",
    "        freeze: boolean, default False\n",
    "          Whether to freeze the encoder weights (MIGHT BE BROKEN).\n",
    "        latent_nonlin: string, default 'sigmoid'\n",
    "          Which space to use for the latent variable:\n",
    "            if 'sigmoid': z in (0,1)\n",
    "            else: z in (-inf,inf)\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        Multinomial Discriminator object.\n",
    "        '''\n",
    "        super(Discriminator_y, self).__init__()\n",
    "\n",
    "        if n_layers >= 0:\n",
    "            if isinstance(n_hidden, list):\n",
    "                n_hidden = n_hidden[0]\n",
    "                print('NOTE: Decoder ignoring list of hiddens because n_layer >= 0. Just using first element.')\n",
    "            n_hidden = n_layers * [n_hidden]\n",
    "        else:\n",
    "            n_layers = len(n_hidden)\n",
    "            print('NOTE: Decoder reading n_hidden as list.')\n",
    "\n",
    "        if latent_nonlin != 'sigmoid':\n",
    "            print('NOTE: Latent z will be fed to decoder in logit-space (-inf,inf).')\n",
    "        else:\n",
    "            print('NOTE: Latent z will be fed to decoder in probability-space (0,1).')\n",
    "\n",
    "        if nonlin == '':\n",
    "            nonlin = None\n",
    "\n",
    "        in_units = ndim_y\n",
    "        with self.name_scope():\n",
    "            self.main = nn.HybridSequential(prefix='discriminator_y')\n",
    "            for i in range(n_layers):\n",
    "                self.main.add(nn.Dense(n_hidden[i], in_units=in_units, activation=nonlin))\n",
    "                in_units = n_hidden[i]\n",
    "            self.main.add(nn.Dense(output_dim, in_units=in_units, activation=None))\n",
    "\n",
    "        self.model_ctx = model_ctx\n",
    "        self.batch_size = batch_size\n",
    "        self.ndim_y = ndim_y\n",
    "        self.n_hidden = n_hidden\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.nonlin = nonlin\n",
    "        self.latent_nonlin = latent_nonlin\n",
    "        self.weights_file = weights_file\n",
    "        self.freeze = freeze\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def hybrid_forward(self, F, y):\n",
    "        '''\n",
    "        Passes the input through the decoder.\n",
    "        Args\n",
    "        ----\n",
    "        F: mxnet.nd or mxnet.sym, No default\n",
    "          This will be passed implicitly when calling hybrid forward.\n",
    "        x: NDarray or mxnet symbol, No default\n",
    "          The input to the decoder.\n",
    "        '''\n",
    "        logit = self.main(y)\n",
    "        if self.apply_softmax:\n",
    "            return F.softmax(logit)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd_loss(x, y, ctx_model, t=0.1, kernel='diffusion'):\n",
    "    '''\n",
    "    computes the mmd loss with information diffusion kernel\n",
    "    :param x: batch_size x latent dimension\n",
    "    :param y:\n",
    "    :param t:\n",
    "    :return:\n",
    "    '''\n",
    "    eps = 1e-6\n",
    "    n,d = x.shape\n",
    "    if kernel == 'tv':\n",
    "        sum_xx = nd.zeros(1, ctx=ctx_model)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                sum_xx = sum_xx + nd.norm(x[i] - x[j], ord=1)\n",
    "        sum_xx = sum_xx / (n * (n-1))\n",
    "\n",
    "        sum_yy = nd.zeros(1, ctx=ctx_model)\n",
    "        for i in range(y.shape[0]):\n",
    "            for j in range(i+1, y.shape[0]):\n",
    "                sum_yy = sum_yy + nd.norm(y[i] - y[j], ord=1)\n",
    "        sum_yy = sum_yy / (y.shape[0] * (y.shape[0]-1))\n",
    "\n",
    "        sum_xy = nd.zeros(1, ctx=ctx_model)\n",
    "        for i in range(n):\n",
    "            for j in range(y.shape[0]):\n",
    "                sum_xy = sum_xy + nd.norm(x[i] - y[j], ord=1)\n",
    "        sum_yy = sum_yy / (n * y.shape[0])\n",
    "    else:\n",
    "        qx = nd.sqrt(nd.clip(x, eps, 1))\n",
    "        qy = nd.sqrt(nd.clip(y, eps, 1))\n",
    "        xx = nd.dot(qx, qx, transpose_b=True)\n",
    "        yy = nd.dot(qy, qy, transpose_b=True)\n",
    "        xy = nd.dot(qx, qy, transpose_b=True)\n",
    "\n",
    "        def diffusion_kernel(a, tmpt, dim):\n",
    "            # return (4 * np.pi * tmpt)**(-dim / 2) * nd.exp(- nd.square(nd.arccos(a)) / tmpt)\n",
    "            return nd.exp(- nd.square(nd.arccos(a)) / tmpt)\n",
    "\n",
    "        off_diag = 1 - nd.eye(n, ctx=ctx_model)\n",
    "        k_xx = diffusion_kernel(nd.clip(xx, 0, 1-eps), t, d-1)\n",
    "        k_yy = diffusion_kernel(nd.clip(yy, 0, 1-eps), t, d-1)\n",
    "        k_xy = diffusion_kernel(nd.clip(xy, 0, 1-eps), t, d-1)\n",
    "        sum_xx = (k_xx * off_diag).sum() / (n * (n-1))\n",
    "        sum_yy = (k_yy * off_diag).sum() / (n * (n-1))\n",
    "        sum_xy = 2 * k_xy.sum() / (n * n)\n",
    "    return sum_xx + sum_yy - sum_xy\n",
    "\n",
    "\n",
    "class Unsupervised(Compute):\n",
    "    '''\n",
    "    Class to manage training, testing, and\n",
    "    retrieving outputs.\n",
    "    '''\n",
    "    def __init__(self, data, Enc, Dec,  Dis_y, args):\n",
    "        '''\n",
    "        Constructor.\n",
    "        Args\n",
    "        ----\n",
    "        Returns\n",
    "        -------\n",
    "        Compute object\n",
    "        '''\n",
    "        super(Unsupervised, self).__init__(data, Enc, Dec, Dis_y, args)\n",
    "\n",
    "    def unlabeled_train_op_mmd_combine(self, update_enc=True):\n",
    "        '''\n",
    "        Trains the MMD model\n",
    "        '''\n",
    "        batch_size = self.args['batch_size']\n",
    "        model_ctx = self.model_ctx\n",
    "        eps = 1e-10\n",
    "\n",
    "        # Retrieve data\n",
    "        docs = self.data.get_documents(key='train')\n",
    "\n",
    "        y_true = np.random.dirichlet(np.ones(self.ndim_y) * self.args['dirich_alpha'], size=batch_size)\n",
    "        y_true = nd.array(y_true, ctx=model_ctx)\n",
    "\n",
    "        with autograd.record():\n",
    "            ### reconstruction phase ###\n",
    "            y_onehot_u = self.Enc(docs)\n",
    "            y_onehot_u_softmax = nd.softmax(y_onehot_u)\n",
    "            if self.args['latent_noise'] > 0:\n",
    "                y_noise = np.random.dirichlet(np.ones(self.ndim_y) * self.args['dirich_alpha'], size=batch_size)\n",
    "                y_noise = nd.array(y_noise, ctx=model_ctx)\n",
    "                y_onehot_u_softmax = (1 - self.args['latent_noise']) * y_onehot_u_softmax + self.args['latent_noise'] * y_noise\n",
    "            x_reconstruction_u = self.Dec(y_onehot_u_softmax)\n",
    "\n",
    "            logits = nd.log_softmax(x_reconstruction_u)\n",
    "            loss_reconstruction = nd.mean(nd.sum(- docs * logits, axis=1))\n",
    "            loss_total = loss_reconstruction * self.args['recon_alpha']\n",
    "\n",
    "            ### mmd phase ###\n",
    "            if self.args['adverse']:\n",
    "                y_fake = self.Enc(docs)\n",
    "                y_fake = nd.softmax(y_fake)\n",
    "                loss_mmd = mmd_loss(y_true, y_fake, ctx_model=model_ctx, t=self.args['kernel_alpha'])\n",
    "                loss_total = loss_total + loss_mmd\n",
    "\n",
    "            if self.args['l2_alpha'] > 0:\n",
    "                loss_total = loss_total + self.args['l2_alpha'] * nd.mean(nd.sum(nd.square(y_onehot_u), axis=1))\n",
    "\n",
    "            loss_total.backward()\n",
    "\n",
    "        self.optimizer_enc.step(1)\n",
    "        self.optimizer_dec.step(1)  # self.m.args['batch_size']\n",
    "\n",
    "        latent_max = nd.zeros(self.args['ndim_y'], ctx=model_ctx)\n",
    "        for max_ind in nd.argmax(y_onehot_u, axis=1):\n",
    "            latent_max[max_ind] += 1.0\n",
    "        latent_max /= batch_size\n",
    "        latent_entropy = nd.mean(nd.sum(- y_onehot_u_softmax * nd.log(y_onehot_u_softmax + eps), axis=1))\n",
    "        latent_v = nd.mean(y_onehot_u_softmax, axis=0)\n",
    "        dirich_entropy = nd.mean(nd.sum(- y_true * nd.log(y_true + eps), axis=1))\n",
    "\n",
    "        if self.args['adverse']:\n",
    "            loss_mmd_return = loss_mmd.asscalar()\n",
    "        else:\n",
    "            loss_mmd_return = 0.0\n",
    "        return nd.mean(loss_reconstruction).asscalar(), loss_mmd_return, latent_max.asnumpy(), latent_entropy.asscalar(), latent_v.asnumpy(), dirich_entropy.asscalar()\n",
    "\n",
    "\n",
    "    def retrain_enc(self, l2_alpha=0.1):\n",
    "        docs = self.data.get_documents(key='train')\n",
    "        with autograd.record():\n",
    "            ### reconstruction phase ###\n",
    "            y_onehot_u = self.Enc(docs)\n",
    "            y_onehot_u_softmax = nd.softmax(y_onehot_u)\n",
    "            x_reconstruction_u = self.Dec(y_onehot_u_softmax)\n",
    "\n",
    "            logits = nd.log_softmax(x_reconstruction_u)\n",
    "            loss_reconstruction = nd.mean(nd.sum(- docs * logits, axis=1))\n",
    "            loss_reconstruction = loss_reconstruction + l2_alpha * nd.mean(nd.norm(y_onehot_u, ord=1, axis=1))\n",
    "            loss_reconstruction.backward()\n",
    "\n",
    "        self.optimizer_enc.step(1)\n",
    "        return loss_reconstruction.asscalar()\n",
    "\n",
    "\n",
    "    def unlabeled_train_op_adv_combine_add(self, update_enc=True):\n",
    "        '''\n",
    "        Trains the GAN model\n",
    "        '''\n",
    "        batch_size = self.args['batch_size']\n",
    "        model_ctx = self.model_ctx\n",
    "        eps = 1e-10\n",
    "        ##########################\n",
    "        ### unsupervised phase ###\n",
    "        ##########################\n",
    "        # Retrieve data\n",
    "        docs = self.data.get_documents(key='train')\n",
    "\n",
    "        class_true = nd.zeros(batch_size, dtype='int32', ctx=model_ctx)\n",
    "        class_fake = nd.ones(batch_size, dtype='int32', ctx=model_ctx)\n",
    "        loss_reconstruction = nd.zeros((1,), ctx=model_ctx)\n",
    "\n",
    "        ### adversarial phase ###\n",
    "        discriminator_z_confidence_true = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        discriminator_z_confidence_fake = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        discriminator_y_confidence_true = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        discriminator_y_confidence_fake = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        loss_discriminator = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        dirich_entropy = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "\n",
    "        ### generator phase ###\n",
    "        loss_generator = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "\n",
    "        ### reconstruction phase ###\n",
    "        with autograd.record():\n",
    "            y_u = self.Enc(docs)\n",
    "            y_onehot_u_softmax = nd.softmax(y_u)\n",
    "            x_reconstruction_u = self.Dec(y_onehot_u_softmax)\n",
    "\n",
    "            logits = nd.log_softmax(x_reconstruction_u)\n",
    "            loss_reconstruction = nd.sum(- docs * logits, axis=1)\n",
    "            loss_total = loss_reconstruction * self.args['recon_alpha']\n",
    "\n",
    "            if self.args['adverse']: #and np.random.rand()<0.8:\n",
    "                y_true = np.random.dirichlet(np.ones(self.ndim_y) * self.args['dirich_alpha'], size=batch_size)\n",
    "                y_true = nd.array(y_true, ctx=model_ctx)\n",
    "                dy_true = self.Dis_y(y_true)\n",
    "                dy_fake = self.Dis_y(y_onehot_u_softmax)\n",
    "                discriminator_y_confidence_true = nd.mean(nd.softmax(dy_true)[:, 0])\n",
    "                discriminator_y_confidence_fake = nd.mean(nd.softmax(dy_fake)[:, 1])\n",
    "                softmaxCEL = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "                loss_discriminator = softmaxCEL(dy_true, class_true) + \\\n",
    "                                       softmaxCEL(dy_fake, class_fake)\n",
    "                loss_generator = softmaxCEL(dy_fake, class_true)\n",
    "                loss_total = loss_total + loss_discriminator + loss_generator\n",
    "                dirich_entropy = nd.mean(nd.sum(- y_true * nd.log(y_true + eps), axis=1))\n",
    "\n",
    "        loss_total.backward()\n",
    "\n",
    "        self.optimizer_enc.step(batch_size)\n",
    "        self.optimizer_dec.step(batch_size)\n",
    "        self.optimizer_dis_y.step(batch_size)\n",
    "\n",
    "        latent_max = nd.zeros(self.args['ndim_y'], ctx=model_ctx)\n",
    "        for max_ind in nd.argmax(y_onehot_u_softmax, axis=1):\n",
    "            latent_max[max_ind] += 1.0\n",
    "        latent_max /= batch_size\n",
    "        latent_entropy = nd.mean(nd.sum(- y_onehot_u_softmax * nd.log(y_onehot_u_softmax + eps), axis=1))\n",
    "        latent_v = nd.mean(y_onehot_u_softmax, axis=0)\n",
    "\n",
    "        return nd.mean(loss_discriminator).asscalar(), nd.mean(loss_generator).asscalar(), nd.mean(loss_reconstruction).asscalar(), \\\n",
    "               nd.mean(discriminator_z_confidence_true).asscalar(), nd.mean(discriminator_z_confidence_fake).asscalar(), \\\n",
    "               nd.mean(discriminator_y_confidence_true).asscalar(), nd.mean(discriminator_y_confidence_fake).asscalar(), \\\n",
    "               latent_max.asnumpy(), latent_entropy.asscalar(), latent_v.asnumpy(), dirich_entropy.asscalar()\n",
    "\n",
    "\n",
    "    def test_synthetic_op(self):\n",
    "        batch_size = self.args['batch_size']\n",
    "        dataset = 'train'\n",
    "        num_samps = self.data.data[dataset].shape[0]\n",
    "        batches = int(np.ceil(num_samps / batch_size))\n",
    "        batch_iter = range(batches)\n",
    "        enc_out = nd.zeros(shape=(batches * batch_size, self.ndim_y))\n",
    "        for batch in batch_iter:\n",
    "            # 1. Retrieve data\n",
    "            if self.args['data_source'] == 'Ian':\n",
    "                docs = self.data.get_documents(key=dataset)\n",
    "            # 2. Compute loss\n",
    "            y_onehot_u = self.Enc(docs)\n",
    "            y_onehot_softmax = nd.softmax(y_onehot_u)\n",
    "            enc_out[batch*batch_size:(batch+1)*batch_size, :] = y_onehot_softmax\n",
    "\n",
    "        return enc_out\n",
    "\n",
    "    def test_op(self, num_samples=None, num_epochs=None, reset=True, dataset='test'):\n",
    "        '''\n",
    "        Evaluates the model using num_samples.\n",
    "        Args\n",
    "        ----\n",
    "        num_samples: integer, default None\n",
    "          The number of samples to evaluate on. This is converted to\n",
    "          evaluating on (num_samples // batch_size) minibatches.\n",
    "        num_epochs: integer, default None\n",
    "          The number of epochs to evaluate on. This used if num_samples\n",
    "          is not specified. If neither is specified, defaults to 1 epoch.\n",
    "        reset: bool, default True\n",
    "          Whether to reset the test data index to 0 before iterating\n",
    "          through and evaluating on minibatches.\n",
    "        dataset: string, default 'test':\n",
    "          Which dataset to evaluate on: 'valid' or 'test'.\n",
    "        Returns\n",
    "        -------\n",
    "        Loss_u: float\n",
    "          The loss on the unlabeled data.\n",
    "        Loss_l: float\n",
    "          The loss on the labeled data.\n",
    "        Eval_u: list of floats\n",
    "          A list of evaluation metrics on the unlabeled data.\n",
    "        Eval_l: list of floats\n",
    "          A list of evaluation metrics on the labeled data.\n",
    "        '''\n",
    "        batch_size = self.args['batch_size']\n",
    "        model_ctx = self.model_ctx\n",
    "\n",
    "        if num_samples is None and num_epochs is None:\n",
    "            # assume full dataset evaluation\n",
    "            num_epochs = 1\n",
    "\n",
    "        if reset:\n",
    "            # Reset Data to Index Zero\n",
    "            if self.data.data[dataset] is not None:\n",
    "                self.data.force_reset_data(dataset)\n",
    "            if self.data.data[dataset + '_with_labels'] is not None:\n",
    "                self.data.force_reset_data(dataset+'_with_labels')\n",
    "\n",
    "        # Unlabeled Data\n",
    "        u_loss = 'NA'\n",
    "        u_eval = []\n",
    "        if self.data.data[dataset] is not None:\n",
    "            u_loss = 0\n",
    "            if num_samples is None:\n",
    "                num_samps = self.data.data[dataset].shape[0] * num_epochs\n",
    "            else:\n",
    "                num_samps = num_samples\n",
    "            batches = int(np.ceil(num_samps / self.args['batch_size']))\n",
    "            batch_iter = range(batches)\n",
    "            if batches > 1: batch_iter = tqdm(batch_iter, desc='unlabeled')\n",
    "            for batch in batch_iter:\n",
    "                # 1. Retrieve data\n",
    "                docs = self.data.get_documents(key=dataset)\n",
    "\n",
    "                # 2. Compute loss\n",
    "                y_u = self.Enc(docs)\n",
    "                y_onehot_u_softmax = nd.softmax(y_u)\n",
    "                x_reconstruction_u = self.Dec(y_onehot_u_softmax)\n",
    "\n",
    "                logits = nd.log_softmax(x_reconstruction_u)\n",
    "                loss_recon_unlabel = nd.sum(- docs * logits, axis=1)\n",
    "\n",
    "                # 3. Convert to numpy\n",
    "                u_loss += nd.mean(loss_recon_unlabel).asscalar()\n",
    "            u_loss /= batches\n",
    "\n",
    "        # Labeled Data\n",
    "        l_loss = 0.0\n",
    "        l_acc = 0.0\n",
    "        if self.data.data[dataset+'_with_labels'] is not None:\n",
    "            l_loss = 0\n",
    "            if num_samples is None:\n",
    "                num_samps = self.data.data[dataset+'_with_labels'].shape[0] * num_epochs\n",
    "            else:\n",
    "                num_samps = num_samples\n",
    "            batches = int(np.ceil(num_samps / self.args['batch_size']))\n",
    "            batch_iter = range(batches)\n",
    "            if batches > 1: batch_iter = tqdm(batch_iter, desc='labeled')\n",
    "            softmaxCEL = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "            for batch in batch_iter:\n",
    "                # 1. Retrieve data\n",
    "                labeled_docs, labels = self.data.get_documents(key=dataset+'_with_labels', split_on=self.data.data_dim)\n",
    "                # 2. Compute loss\n",
    "                y_u = self.Enc(docs)\n",
    "                y_onehot_u_softmax = nd.softmax(y_u)\n",
    "                class_pred = nd.argmax(y_onehot_u_softmax, axis=1)\n",
    "                l_a = labels[list(range(labels.shape[0])), class_pred]\n",
    "                l_acc += nd.mean(l_a).asscalar()\n",
    "                labels = labels / nd.sum(labels, axis=1, keepdims=True)\n",
    "                l_l = softmaxCEL(y_onehot_u_softmax, labels)\n",
    "\n",
    "                # 3. Convert to numpy\n",
    "                l_loss += nd.mean(l_l).asscalar()\n",
    "            l_loss /= batches\n",
    "            l_acc /= batches\n",
    "\n",
    "        return u_loss, l_loss, l_acc\n",
    "\n",
    "\n",
    "    def save_latent(self, saveto):\n",
    "        before_softmax = True\n",
    "        try:\n",
    "            if type(self.data.data['train']) is np.ndarray:\n",
    "                dataset_train = gluon.data.dataset.ArrayDataset(self.data.data['train'])\n",
    "                train_data = gluon.data.DataLoader(dataset_train, self.args['batch_size'], shuffle=False, last_batch='discard')\n",
    "\n",
    "                dataset_val = gluon.data.dataset.ArrayDataset(self.data.data['valid'])\n",
    "                val_data = gluon.data.DataLoader(dataset_val, self.args['batch_size'], shuffle=False, last_batch='discard')\n",
    "\n",
    "                dataset_test = gluon.data.dataset.ArrayDataset(self.data.data['test'])\n",
    "                test_data = gluon.data.DataLoader(dataset_test, self.args['batch_size'], shuffle=False, last_batch='discard')\n",
    "            else:\n",
    "                train_data = io.NDArrayIter(data={'data': self.data.data['train']}, batch_size=self.args['batch_size'],\n",
    "                                            shuffle=False, last_batch_handle='discard')\n",
    "                val_data = io.NDArrayIter(data={'data': self.data.data['valid']}, batch_size=self.args['batch_size'],\n",
    "                                            shuffle=False, last_batch_handle='discard')\n",
    "                test_data = io.NDArrayIter(data={'data': self.data.data['test']}, batch_size=self.args['batch_size'],\n",
    "                                            shuffle=False, last_batch_handle='discard')\n",
    "        except:\n",
    "            print(\"Loading error during save_latent. Probably caused by not having validation or test set!\")\n",
    "            return\n",
    "\n",
    "        train_output = np.zeros((self.data.data['train'].shape[0], self.ndim_y))\n",
    "        # train_label_output = np.zeros(self.data.data['train'].shape[0])\n",
    "        # for i, (data, label) in enumerate(train_data):\n",
    "        for i, data in enumerate(train_data):\n",
    "            if type(data) is io.DataBatch:\n",
    "                data = data.data[0].as_in_context(self.model_ctx)\n",
    "            else:\n",
    "                data = data.as_in_context(self.model_ctx)\n",
    "            if before_softmax:\n",
    "                output = self.Enc(data)\n",
    "            else:\n",
    "                output = nd.softmax(self.Enc(data))\n",
    "            train_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = output.asnumpy()\n",
    "            # train_label_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = label.asnumpy()\n",
    "        train_output = np.delete(train_output, np.s_[(i+1)*self.args['batch_size']:], 0)\n",
    "        # train_label_output = np.delete(train_label_output, np.s_[(i+1)*self.args['batch_size']:])\n",
    "        np.save(os.path.join(saveto, self.args['domain']+'train_latent.npy'), train_output)\n",
    "        # np.save(os.path.join(saveto, self.args['domain']+'train_latent_label.npy'), train_label_output)\n",
    "\n",
    "        val_output = np.zeros((self.data.data['valid'].shape[0], self.ndim_y))\n",
    "        # train_label_output = np.zeros(self.data.data['train'].shape[0])\n",
    "        # for i, (data, label) in enumerate(train_data):\n",
    "        for i, data in enumerate(val_data):\n",
    "            if type(data) is io.DataBatch:\n",
    "                data = data.data[0].as_in_context(self.model_ctx)\n",
    "            else:\n",
    "                data = data.as_in_context(self.model_ctx)\n",
    "            if before_softmax:\n",
    "                output = self.Enc(data)\n",
    "            else:\n",
    "                output = nd.softmax(self.Enc(data))\n",
    "            val_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = output.asnumpy()\n",
    "            # train_label_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = label.asnumpy()\n",
    "        val_output = np.delete(val_output, np.s_[(i+1)*self.args['batch_size']:], 0)\n",
    "        # train_label_output = np.delete(train_label_output, np.s_[(i+1)*self.args['batch_size']:])\n",
    "        np.save(os.path.join(saveto, self.args['domain']+'val_latent.npy'), val_output)\n",
    "        # np.save(os.path.join(saveto, self.args['domain']+'train_latent_label.npy'), train_label_output)\n",
    "\n",
    "        test_output = np.zeros((self.data.data['test'].shape[0], self.ndim_y))\n",
    "        # test_label_output = np.zeros(self.data.data['test'].shape[0])\n",
    "        # for i, (data, label) in enumerate(test_data):\n",
    "        for i, data in enumerate(test_data):\n",
    "            if type(data) is io.DataBatch:\n",
    "                data = data.data[0].as_in_context(self.model_ctx)\n",
    "            else:\n",
    "                data = data.as_in_context(self.model_ctx)\n",
    "            if before_softmax:\n",
    "                output = self.Enc(data)\n",
    "            else:\n",
    "                output = nd.softmax(self.Enc(data))\n",
    "            test_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = output.asnumpy()\n",
    "            # test_label_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = label.asnumpy()\n",
    "        test_output = np.delete(test_output, np.s_[(i+1)*self.args['batch_size']:], 0)\n",
    "        # test_label_output = np.delete(test_label_output, np.s_[(i+1)*self.args['batch_size']:])\n",
    "        np.save(os.path.join(saveto, self.args['domain']+'test_latent.npy'), test_output)\n",
    "        # np.save(os.path.join(saveto, self.args['domain']+'test_latent_label.npy'), test_label_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['latent_noise'] >= 0.0 and args['latent_noise'] <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'un_label_coeffs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f677000e2148>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'domain'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'algorithm'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'un_label_coeffs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'un_label_coeffs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'-unsup'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'un_label_coeffs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'un_label_coeffs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'un_label_coeffs'"
     ]
    }
   ],
   "source": [
    "if args['description'] == '':\n",
    "    args['description'] = args['domain'] + '-' + args['algorithm'] + '-' + args['model']\n",
    "    if args['un_label_coeffs'][0] > 0 and args['un_label_coeffs'][1] == 0:\n",
    "        args['description'] += '-unsup'\n",
    "    elif args['un_label_coeffs'][0] > 0 and args['un_label_coeffs'][1] > 0:\n",
    "        args['description'] += '-semisup'\n",
    "    else:\n",
    "        args['description'] += '-sup'\n",
    "elif args['description'].isdigit():\n",
    "    args['description'] = args['domain'] + '-' + args['algorithm'] + '-' + args['model'] + '-' + args['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ctx = gpu_helper(0)\n",
    "model_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Number of samples for test is smaller than batch_size (60<256). Duplicating samples to exceed batch_size.\n"
     ]
    }
   ],
   "source": [
    "data = Wikitext103(batch_size=256, data_path='./data/wikitext-103', ctx=model_ctx, saveto='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dimension =  (28472, 20000)\n"
     ]
    }
   ],
   "source": [
    "print('train dimension = ', data.data['train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392.214"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if type(data.data['train']) is np.ndarray:\n",
    "    mean_length = np.mean(np.sum(data.data['train'], axis=1))\n",
    "else:\n",
    "    mean_length = mx.nd.mean(mx.nd.sum(data.data['train'], axis=1)).asscalar()\n",
    "mean_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = data.data['train'].shape[1]\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.data['train_with_labels'] is not None:\n",
    "    print('train_with_labels dimension = ', data.data['train_with_labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.252802419767204e-05"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args['recon_alpha'] < 0:\n",
    "    args['recon_alpha'] = 1.0 / (mean_length * np.log(vocab_size))\n",
    "args['recon_alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Encoder ignoring list of hiddens because n_layer >= 0. Just using first element.\n",
      "NOTE: Decoder ignoring list of hiddens because n_layer >= 0. Just using first element.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d144b35a0cbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m Dec = Decoder(model_ctx=model_ctx, batch_size=args['batch_size'], output_dim=args['ndim_x'], ndim_y=args['ndim_y'],\n\u001b[0;32m      5\u001b[0m               \u001b[0mn_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dec_n_hidden'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dec_n_layer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dec_nonlinearity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m               weights_file=args['dec_weights'], freeze=args['dec_freeze'], latent_nonlin=args['latent_nonlinearity'])\n\u001b[0m\u001b[0;32m      7\u001b[0m Dis_y = Discriminator_y(model_ctx=model_ctx, batch_size=args['batch_size'], ndim_y=args['ndim_y'],\n\u001b[0;32m      8\u001b[0m                         \u001b[0mn_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dis_n_hidden'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dis_n_layer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-8e4b58af2d93>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_ctx, batch_size, output_dim, ndim_y, n_hidden, n_layers, nonlin, weights_file, freeze, latent_nonlin, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mnonlin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0min_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHybridSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'decoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "Enc = Encoder(model_ctx=model_ctx, batch_size=args['batch_size'], input_dim=args['ndim_x'], ndim_y=args['ndim_y'],\n",
    "              n_hidden=args['enc_n_hidden'], n_layers=args['enc_n_layer'], nonlin=args['enc_nonlinearity'],\n",
    "              weights_file=args['enc_weights'], freeze=args['enc_freeze'], latent_nonlin=args['latent_nonlinearity'])\n",
    "Dec = Decoder(model_ctx=model_ctx, batch_size=args['batch_size'], output_dim=args['ndim_x'], ndim_y=args['ndim_y'],\n",
    "              n_hidden=args['dec_n_hidden'], n_layers=args['dec_n_layer'], nonlin=args['dec_nonlinearity'],\n",
    "              weights_file=args['dec_weights'], freeze=args['dec_freeze'], latent_nonlin=args['latent_nonlinearity'])\n",
    "Dis_y = Discriminator_y(model_ctx=model_ctx, batch_size=args['batch_size'], ndim_y=args['ndim_y'],\n",
    "                        n_hidden=args['dis_n_hidden'], n_layers=args['dis_n_layer'],\n",
    "                        nonlin=args['dis_nonlinearity'], weights_file=args['dis_y_weights'],\n",
    "                        freeze=args['dis_freeze'], latent_nonlin=args['latent_nonlinearity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['enc_n_hidden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['dec_n_hidden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
