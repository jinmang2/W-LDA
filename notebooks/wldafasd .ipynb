{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import mxnet as mx\n",
    "mx.random.seed(int(time.time()))\n",
    "\n",
    "from utils import gpu_helper, gpu_exists, calc_topic_uniqueness, get_topic_words_decoder_weights, request_pmi, print_topic_with_scores, print_topics\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Training WAE in MXNet')\n",
    "parser.add_argument('-dom','--domain', type=str, default='wikitext-103', help='domain to run', required=False)\n",
    "parser.add_argument('-data','--data_path', type=str, default='', help='file path for dataset', required=False)\n",
    "parser.add_argument('-max_labels','--max_labels', type=int, default=100, help='max number of topics to specify as labels for a single training document', required=False)\n",
    "parser.add_argument('-max_labeled_samples','--max_labeled_samples', type=int, default=10, help='max number of labeled samples per topic', required=False)\n",
    "parser.add_argument('-label_seed','--label_seed', type=lambda x: int(x) if x != 'None' else None, default=None, help='random seed for subsampling the labeled dataset', required=False)\n",
    "parser.add_argument('-mod','--model', type=str, default='dirichlet', help='model to use', required=False)\n",
    "parser.add_argument('-desc','--description', type=str, default='', help='description for the experiment', required=False)\n",
    "parser.add_argument('-alg','--algorithm', type=str, default='standard', help='algorithm to use for training: standard', required=False)\n",
    "parser.add_argument('-bs','--batch_size', type=int, default=256, help='batch_size for training', required=False)\n",
    "parser.add_argument('-opt','--optim', type=str, default='Adam', help='encoder training algorithm', required=False)\n",
    "parser.add_argument('-lr','--learning_rate', type=float, default=1e-4, help='learning rate', required=False)\n",
    "parser.add_argument('-l2','--weight_decay', type=float, default=0., help='weight decay', required=False)\n",
    "parser.add_argument('-e_nh','--enc_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for encoder or list of hiddens for each layer', required=False)\n",
    "parser.add_argument('-e_nl','--enc_n_layer', type=int, default=1, help='# of hidden layers for encoder, set to -1 if passing list of n_hiddens', required=False)\n",
    "parser.add_argument('-e_nonlin','--enc_nonlinearity', type=str, default='sigmoid', help='type of nonlinearity for encoder', required=False)\n",
    "parser.add_argument('-e_weights','--enc_weights', type=str, default='', help='file path for encoder weights', required=False)\n",
    "parser.add_argument('-e_freeze','--enc_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the encoder weights', required=False)\n",
    "parser.add_argument('-lat_nonlin','--latent_nonlinearity', type=str, default='', help='type of to use prior to decoder', required=False)\n",
    "parser.add_argument('-d_nh','--dec_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for decoder or list of hiddens for each layer', required=False)\n",
    "parser.add_argument('-d_nl','--dec_n_layer', type=int, default=0, help='# of hidden layers for decoder', required=False)\n",
    "parser.add_argument('-d_nonlin','--dec_nonlinearity', type=str, default='', help='type of nonlinearity for decoder', required=False)\n",
    "parser.add_argument('-d_weights','--dec_weights', type=str, default='', help='file path for decoder weights', required=False)\n",
    "parser.add_argument('-d_freeze','--dec_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the decoder weights', required=False)\n",
    "parser.add_argument('-d_word_dist','--dec_word_dist', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to init decoder weights with training set word distributions', required=False)\n",
    "parser.add_argument('-dis_nh','--dis_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for encoder or list of hiddens for each layer', required=False)\n",
    "parser.add_argument('-dis_nl','--dis_n_layer', type=int, default=1, help='# of hidden layers for encoder, set to -1 if passing list of n_hiddens', required=False)\n",
    "parser.add_argument('-dis_nonlin','--dis_nonlinearity', type=str, default='sigmoid', help='type of nonlinearity for discriminator', required=False)\n",
    "parser.add_argument('-dis_y_weights','--dis_y_weights', type=str, default='', help='file path for discriminator_y weights', required=False)\n",
    "parser.add_argument('-dis_z_weights','--dis_z_weights', type=str, default='', help='file path for discriminator_z weights', required=False)\n",
    "parser.add_argument('-dis_freeze','--dis_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the encoder weights', required=False)\n",
    "parser.add_argument('-include_w','--include_weights', type=str, nargs='*', default=[], help='weights to train on (default is all weights) -- all others are kept fixed; Ex: E.z_encoder D.decoder', required=False)\n",
    "parser.add_argument('-eps','--epsilon', type=float, default=1e-8, help='epsilon param for Adam', required=False)\n",
    "parser.add_argument('-mx_it','--max_iter', type=int, default=50001, help='max # of training iterations', required=False)\n",
    "parser.add_argument('-train_stats_every','--train_stats_every', type=int, default=100, help='skip train_stats_every iterations between recording training stats', required=False)\n",
    "parser.add_argument('-eval_stats_every','--eval_stats_every', type=int, default=100, help='skip eval_stats_every iterations between recording evaluation stats', required=False)\n",
    "parser.add_argument('-ndim_y','--ndim_y', type=int, default=256, help='dimensionality of y - topic indicator', required=False)\n",
    "parser.add_argument('-ndim_x','--ndim_x', type=int, default=2, help='dimensionality of p(x) - data distribution', required=False)\n",
    "parser.add_argument('-saveto','--saveto', type=str, default='', help='path prefix for saving results', required=False)\n",
    "parser.add_argument('-gpu','--gpu', type=int, default=-2, help='if/which gpu to use (-1: all, -2: None)', required=False)\n",
    "parser.add_argument('-hybrid','--hybridize', type=lambda x: (str(x).lower() == 'true'), default=False, help='declaritive True (hybridize) or imperative False', required=False)\n",
    "parser.add_argument('-full_npmi','--full_npmi', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to compute NPMI for full trajectory', required=False)\n",
    "parser.add_argument('-eot','--eval_on_test', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to evaluate on the test set (True) or validation set (False)', required=False)\n",
    "parser.add_argument('-verb','--verbose', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to print progress to stdout', required=False)\n",
    "parser.add_argument('-dirich_alpha','--dirich_alpha', type=float, default=1e-1, help='param for Dirichlet prior', required=False)\n",
    "parser.add_argument('-adverse','--adverse', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to turn on adverserial training (MMD or GAN). set to False if only train auto-encoder', required=False)\n",
    "parser.add_argument('-update_enc','--update_enc', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to update encoder for unlabed_train_op()', required=False)\n",
    "parser.add_argument('-labeled_loss_lambda','--labeled_loss_lambda', type=float, default=1.0, help='param for Dirichlet noise for label', required=False)\n",
    "parser.add_argument('-train_mode','--train_mode', type=str, default='mmd', help=\"set to mmd or adv (for GAN)\", required=False)\n",
    "parser.add_argument('-kernel_alpha','--kernel_alpha', type=float, default=1.0, help='param for information diffusion kernel', required=False)\n",
    "parser.add_argument('-recon_alpha','--recon_alpha', type=float, default=-1.0, help='multiplier of the reconstruction loss when combined with mmd loss', required=False)\n",
    "parser.add_argument('-recon_alpha_adapt','--recon_alpha_adapt', type=float, default=-1.0, help='adaptively change recon_alpha so that [total loss = mmd + recon_alpha_adapt * recon loss], set to -1 if no adapt', required=False)\n",
    "parser.add_argument('-dropout_p','--dropout_p', type=float, default=-1.0, help='dropout probability in encoder', required=False)\n",
    "parser.add_argument('-l2_alpha','--l2_alpha', type=float, default=-1.0, help='alpha multipler for L2 regularization on latent vector', required=False)\n",
    "parser.add_argument('-latent_noise','--latent_noise', type=float, default=0.0, help='proportion of dirichlet noise added to the latent vector after softmax', required=False)\n",
    "parser.add_argument('-topic_decoder_weight','--topic_decoder_weight', type=lambda x: (str(x).lower() == 'true'), default=False, help='extract topic words based on decoder weights or decoder outputs', required=False)\n",
    "parser.add_argument('-retrain_enc_only','--retrain_enc_only', type=lambda x: (str(x).lower() == 'true'), default=False, help='only retrain the encoder for reconstruction loss', required=False)\n",
    "parser.add_argument('-l2_alpha_retrain','--l2_alpha_retrain', type=float, default=0.1, help='alpha multipler for L2 regularization on encoder output during retraining', required=False)\n",
    "args = vars(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'domain': 'wikitext-103',\n",
       " 'data_path': '',\n",
       " 'max_labels': 100,\n",
       " 'max_labeled_samples': 10,\n",
       " 'label_seed': None,\n",
       " 'model': 'dirichlet',\n",
       " 'description': '',\n",
       " 'algorithm': 'standard',\n",
       " 'batch_size': 256,\n",
       " 'optim': 'Adam',\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 0.0,\n",
       " 'enc_n_hidden': [128],\n",
       " 'enc_n_layer': 1,\n",
       " 'enc_nonlinearity': 'sigmoid',\n",
       " 'enc_weights': '',\n",
       " 'enc_freeze': False,\n",
       " 'latent_nonlinearity': '',\n",
       " 'dec_n_hidden': [128],\n",
       " 'dec_n_layer': 0,\n",
       " 'dec_nonlinearity': '',\n",
       " 'dec_weights': '',\n",
       " 'dec_freeze': False,\n",
       " 'dec_word_dist': False,\n",
       " 'dis_n_hidden': [128],\n",
       " 'dis_n_layer': 1,\n",
       " 'dis_nonlinearity': 'sigmoid',\n",
       " 'dis_y_weights': '',\n",
       " 'dis_z_weights': '',\n",
       " 'dis_freeze': False,\n",
       " 'include_weights': [],\n",
       " 'epsilon': 1e-08,\n",
       " 'max_iter': 50001,\n",
       " 'train_stats_every': 100,\n",
       " 'eval_stats_every': 100,\n",
       " 'ndim_y': 256,\n",
       " 'ndim_x': 2,\n",
       " 'saveto': '',\n",
       " 'gpu': -2,\n",
       " 'hybridize': False,\n",
       " 'full_npmi': False,\n",
       " 'eval_on_test': False,\n",
       " 'verbose': True,\n",
       " 'dirich_alpha': 0.1,\n",
       " 'adverse': True,\n",
       " 'update_enc': True,\n",
       " 'labeled_loss_lambda': 1.0,\n",
       " 'train_mode': 'mmd',\n",
       " 'kernel_alpha': 1.0,\n",
       " 'recon_alpha': -1.0,\n",
       " 'recon_alpha_adapt': -1.0,\n",
       " 'dropout_p': -1.0,\n",
       " 'l2_alpha': -1.0,\n",
       " 'latent_noise': 0.0,\n",
       " 'topic_decoder_weight': False,\n",
       " 'retrain_enc_only': False,\n",
       " 'l2_alpha_retrain': 0.1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twenty_news_sklearn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['domain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./examples/domains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikitext103_wae import Wikitext103 as Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dirichlet import Encoder, Decoder, Discriminator_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compute_op import Unsupervised as Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['latent_noise'] >= 0.0 and args['latent_noise'] <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['description'] = args['domain'] + '-' \\\n",
    "                    + args['algorithm'] + '-' \\\n",
    "                    + args['model'] + '-sup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['saveto'] = 'examples/results/' + args['description'].replace('-', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveto = args['saveto'] + '/' + \\\n",
    "    datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S/{}').format('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(saveto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(saveto)\n",
    "os.makedirs(saveto + '/weights/encoder')\n",
    "os.makedirs(saveto + '/weights/decoder')\n",
    "os.makedirs(saveto + '/weights/discriminator_y')\n",
    "os.makedirs(saveto + '/weights/discriminator_z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Implementation\\\\w-lda\\\\compute_op.py'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.realpath('compute_op.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'examples/results/wikitext/103/standard/dirichlet/sup/2019-12-19-10-12-59/compute_op.py'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(saveto, 'compute_op.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'examples/results/wikitext/103/standard/dirichlet/sup/2019-12-19-10-12-59/utils.py'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(os.path.realpath('compute_op.py'), os.path.join(saveto, 'compute_op.py'))\n",
    "shutil.copy(os.path.realpath('core.py'), os.path.join(saveto, 'core.py'))\n",
    "shutil.copy(os.path.realpath('run.py'), os.path.join(saveto, 'run.py'))\n",
    "shutil.copy(os.path.realpath('utils.py'), os.path.join(saveto, 'utils.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'examples/results/wikitext/103/standard/dirichlet/sup/2019-12-19-10-12-59/dirichlet.py'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = args['model'] + '.py'\n",
    "shutil.copy(os.path.realpath('models/' + model_file), os.path.join(saveto, model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['saveto'] = saveto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(saveto + 'args.txt', 'w') as file:\n",
    "    for key, val in args.items():\n",
    "        if val != '':\n",
    "            if isinstance(val, list) or isinstance(val, tuple):\n",
    "                val = [str(v) for v in val]\n",
    "                file.write('--' + str(key) + ' ' + ' '.join(val) + '\\n')\n",
    "            else:\n",
    "                file.write('--' + str(key) + ' ' + str(val) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['gpu'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_exists(args['gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['description'] += ' (gpu' + str(args['gpu']) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikitext-103-standard-dirichlet-sup (gpu0)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(args, open(args['saveto'] + 'args.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(compute_op.Unsupervised,\n",
       " wikitext103_wae.Wikitext103,\n",
       " models.dirichlet.Encoder,\n",
       " models.dirichlet.Decoder,\n",
       " models.dirichlet.Discriminator_y,\n",
       " {'domain': 'wikitext-103',\n",
       "  'data_path': '',\n",
       "  'max_labels': 100,\n",
       "  'max_labeled_samples': 10,\n",
       "  'label_seed': None,\n",
       "  'model': 'dirichlet',\n",
       "  'description': 'wikitext-103-standard-dirichlet-sup (gpu0)',\n",
       "  'algorithm': 'standard',\n",
       "  'batch_size': 256,\n",
       "  'optim': 'Adam',\n",
       "  'learning_rate': 0.0001,\n",
       "  'weight_decay': 0.0,\n",
       "  'enc_n_hidden': [128],\n",
       "  'enc_n_layer': 1,\n",
       "  'enc_nonlinearity': 'sigmoid',\n",
       "  'enc_weights': '',\n",
       "  'enc_freeze': False,\n",
       "  'latent_nonlinearity': '',\n",
       "  'dec_n_hidden': [128],\n",
       "  'dec_n_layer': 0,\n",
       "  'dec_nonlinearity': '',\n",
       "  'dec_weights': '',\n",
       "  'dec_freeze': False,\n",
       "  'dec_word_dist': False,\n",
       "  'dis_n_hidden': [128],\n",
       "  'dis_n_layer': 1,\n",
       "  'dis_nonlinearity': 'sigmoid',\n",
       "  'dis_y_weights': '',\n",
       "  'dis_z_weights': '',\n",
       "  'dis_freeze': False,\n",
       "  'include_weights': [],\n",
       "  'epsilon': 1e-08,\n",
       "  'max_iter': 50001,\n",
       "  'train_stats_every': 100,\n",
       "  'eval_stats_every': 100,\n",
       "  'ndim_y': 256,\n",
       "  'ndim_x': 2,\n",
       "  'saveto': 'examples/results/wikitext/103/standard/dirichlet/sup/2019-12-19-10-12-59/',\n",
       "  'gpu': 0,\n",
       "  'hybridize': False,\n",
       "  'full_npmi': False,\n",
       "  'eval_on_test': False,\n",
       "  'verbose': True,\n",
       "  'dirich_alpha': 0.1,\n",
       "  'adverse': True,\n",
       "  'update_enc': True,\n",
       "  'labeled_loss_lambda': 1.0,\n",
       "  'train_mode': 'mmd',\n",
       "  'kernel_alpha': 1.0,\n",
       "  'recon_alpha': -1.0,\n",
       "  'recon_alpha_adapt': -1.0,\n",
       "  'dropout_p': -1.0,\n",
       "  'l2_alpha': -1.0,\n",
       "  'latent_noise': 0.0,\n",
       "  'topic_decoder_weight': False,\n",
       "  'retrain_enc_only': False,\n",
       "  'l2_alpha_retrain': 0.1})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Compute, Domain, Encoder, Decoder, Discriminator_y, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving to: examples/results/wikitext/103/standard/dirichlet/sup/2019-12-19-10-12-59/\n"
     ]
    }
   ],
   "source": [
    "print('\\nSaving to: ' + args['saveto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ctx = gpu_helper(args['gpu'])\n",
    "model_ctx = mx.gpu(args['gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/wikitext-103/wikitext-103_tra.csr.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-acd8bacff2db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m        \u001b[0mdata_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m        \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_ctx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m        saveto=args['saveto'])\n\u001b[0m",
      "\u001b[1;32mC:\\workspace\\Implementation\\w-lda\\examples\\domains\\wikitext103_wae.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_size, data_path, ctx, saveto, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaveto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaveto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWikitext103\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data/wikitext-103'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'BoW'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch_avitm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\workspace\\Implementation\\w-lda\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_size, data_path, ctx)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\workspace\\Implementation\\w-lda\\examples\\domains\\wikitext103_wae.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, features, match_avitm)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m### Load train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mtrain_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_csr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\scipy\\sparse\\_matrix_io.py\u001b[0m in \u001b[0;36mload_npz\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mPICKLE_KWARGS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mmatrix_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'format'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/wikitext-103/wikitext-103_tra.csr.npz'"
     ]
    }
   ],
   "source": [
    "Domain(batch_size=args['batch_size'],\n",
    "       data_path=args['data_path'],\n",
    "       ctx=model_ctx,\n",
    "       saveto=args['saveto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Implementation\\\\w-lda'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create directory for data\n",
    "dataset = 'wikitext-103'\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Implementation\\\\w-lda\\\\data'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join(current_dir, 'data')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory: C:\\workspace\\Implementation\\w-lda\\data\n"
     ]
    }
   ],
   "source": [
    "print('Creating directory:', data_dir)\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(current_dir, 'data', dataset)\n",
    "# os.path.exists(data_dir)\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\workspace\\Implementation\\w-lda\\data\\wikitext-103\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_dir)\n",
    "print('Current directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download data\n",
    "os.system('curl -O '\n",
    "  'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip')\n",
    "os.system('unzip wikitext-103-v1.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Implementation\\\\w-lda\\\\data\\\\wikitext-103\\\\wikitext-103'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = os.path.join(data_dir, dataset)\n",
    "input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'wiki.train.tokens'\n",
    "val_file = 'wiki.valid.tokens'\n",
    "test_file = 'wiki.test.tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse into documents\n",
    "def is_document_start(line):\n",
    "    if len(line) < 4:\n",
    "        return False\n",
    "    if line[0] is '=' and line[-1] is '=':\n",
    "        if line[2] is not '=':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_list_per_doc(input_dir, token_file):\n",
    "    lines_list = []\n",
    "    line_prev = ''\n",
    "    prev_line_start_doc = False\n",
    "    with open(os.path.join(input_dir, token_file), 'r', encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            line = l.strip()\n",
    "            if prev_line_start_doc and line:\n",
    "                # the previous line should not have been start of a document!\n",
    "                lines_list.pop()\n",
    "                lines_list[-1] = lines_list[-1] + ' ' + line_prev\n",
    "\n",
    "            if line:\n",
    "                if is_document_start(line) and not line_prev:\n",
    "                    lines_list.append(line)\n",
    "                    prev_line_start_doc = True\n",
    "                else:\n",
    "                    lines_list[-1] = lines_list[-1] + ' ' + line\n",
    "                    prev_line_start_doc = False\n",
    "            else:\n",
    "                prev_line_start_doc = False\n",
    "            line_prev = line\n",
    "    print(\"{} documents parsed!\".format(len(lines_list)))\n",
    "    return lines_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28472 documents parsed!\n",
      "60 documents parsed!\n",
      "60 documents parsed!\n"
     ]
    }
   ],
   "source": [
    "train_doc_list = token_list_per_doc(input_dir, train_file)\n",
    "val_doc_list = token_list_per_doc(input_dir, val_file)\n",
    "test_doc_list = token_list_per_doc(input_dir, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import Data\n",
    "from utils import reverse_dict\n",
    "import scipy.sparse as sparse\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jinma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r'(?u)\\b\\w\\w+\\b')\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in doc.split()\n",
    "                if len(t) >= 2 and \n",
    "                   re.match('[a-z].*', t) and\n",
    "                   re.match(token_pattern, t)]\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing and counting, this may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocab_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-1d73e1c7e1aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mvocab_lsit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vocab size:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done. Time elapsed: {:.2f}s'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_list' is not defined"
     ]
    }
   ],
   "source": [
    "print('Lemmatizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word',\n",
    "                             stop_words='english', tokenizer=LemmaTokenizer(),\n",
    "                             max_df=.8, min_df=3, max_features=20000)\n",
    "train_vectors = vectorizer.fit_transform(train_doc_list)\n",
    "val_vectors = vectorizer.transform(val_doc_list)\n",
    "test_vectors = vectorizer.transform(test_doc_list)\n",
    "\n",
    "# vocab_list = vectorizer.get_feature_names()\n",
    "# vocab_size = len(vocab_list)\n",
    "# print('vocab size:', vocab_size)\n",
    "# print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 20000\n",
      "Done. Time elapsed: 803.63s\n"
     ]
    }
   ],
   "source": [
    "vocab_list = vectorizer.get_feature_names()\n",
    "vocab_size = len(vocab_list)\n",
    "print('vocab size:', vocab_size)\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_dtype(vectors):\n",
    "    idx = np.arange(vectors.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    vectors = vectors[idx]\n",
    "    vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "    print(type(vectors), vectors.dtype)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n",
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n",
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "train_vectors = shuffle_and_dtype(train_vectors)\n",
    "val_vectors = shuffle_and_dtype(val_vectors)\n",
    "test_vectors = shuffle_and_dtype(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in vocab_list:\n",
    "        f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('wikitext-103_tra.csr.npz', train_vectors)\n",
    "sparse.save_npz('wikitext-103_val.csr.npz', val_vectors)\n",
    "sparse.save_npz('wikitext-103_test.csr.npz', test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/wikitext-103/wikitext-103_tra.csr.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-acd8bacff2db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m        \u001b[0mdata_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m        \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_ctx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m        saveto=args['saveto'])\n\u001b[0m",
      "\u001b[1;32m./examples/domains\\wikitext103_wae.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_size, data_path, ctx, saveto, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\workspace\\Implementation\\w-lda\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_size, data_path, ctx)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m./examples/domains\\wikitext103_wae.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, features, match_avitm)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\scipy\\sparse\\_matrix_io.py\u001b[0m in \u001b[0;36mload_npz\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mPICKLE_KWARGS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mmatrix_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'format'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/wikitext-103/wikitext-103_tra.csr.npz'"
     ]
    }
   ],
   "source": [
    "Domain(batch_size=args['batch_size'],\n",
    "       data_path=args['data_path'],\n",
    "       ctx=model_ctx,\n",
    "       saveto=args['saveto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.load()\n",
    "    def load(self):\n",
    "        print('First')\n",
    "        \n",
    "class Tester(Test):\n",
    "    def __init__(self, a, b):\n",
    "        super(Tester, self).__init__(a, b)\n",
    "    def load(self):\n",
    "        print('Not First')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not First\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Tester at 0x1b939797c18>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/wikitext-103'\n",
    "features = 'BoW'\n",
    "match_avitm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path + '/wikitext-103_tra.csr.npz'\n",
    "test_path = path + '/wikitext-103_test.csr.npz'\n",
    "vocab_path = path + '/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/wikitext-103/wikitext-103_tra.csr.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-7b69a92981f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\scipy\\sparse\\_matrix_io.py\u001b[0m in \u001b[0;36mload_npz\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mPICKLE_KWARGS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mmatrix_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'format'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/wikitext-103/wikitext-103_tra.csr.npz'"
     ]
    }
   ],
   "source": [
    "train_csr = sparse.load_npz(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\workspace\\Implementation\\w-lda\n"
     ]
    }
   ],
   "source": [
    "os.chdir(current_dir)\n",
    "print('Current directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Number of samples for test is smaller than batch_size (60<256). Duplicating samples to exceed batch_size.\n"
     ]
    }
   ],
   "source": [
    "data = Domain(\n",
    "    batch_size=args['batch_size'],\n",
    "    data_path=args['data_path'],\n",
    "    ctx=model_ctx,\n",
    "    saveto=args['saveto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dimension =  (28472, 20000)\n"
     ]
    }
   ],
   "source": [
    "print('train dimension = ', data.data['train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392.214"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_length = np.mean(np.sum(data.data['train'], axis=1))\n",
    "mean_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = data.data['train'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data['train_with_labels'] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['recon_alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['recon_alpha'] = 1.0 / (mean_length * np.log(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting recon_alpha to 7.252802419767204e-05\n"
     ]
    }
   ],
   "source": [
    "print('Setting recon_alpha to {}'.format(args['recon_alpha']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### core, dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss, v_measure_score\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, io\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENet(gluon.HybridBlock):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ENet, self).__init__()\n",
    "        \n",
    "    def hybrid_forward(self, x):\n",
    "        raise NotImplementedError(\n",
    "            'Need to write your own Encoder that inherits from ENet. '\n",
    "            'Put this file in models/.')\n",
    "    \n",
    "    def init_weights(self, weights=None):\n",
    "        loaded = False\n",
    "        source = 'keyword argument'\n",
    "        if self.weights_file != '' and weights is None:\n",
    "            try:\n",
    "                self.load_params(self.weights_file, self.model_ctx)\n",
    "                source = 'mxnet weights file: ' + self.weights_file\n",
    "                print('NOTE: Loaded encoder weights from ' + source + '.')\n",
    "                if self.freeze():\n",
    "                    self.freeze_params()\n",
    "                    print('NOTE: Froze encoder weights from ' + source + '.')\n",
    "                weights = None\n",
    "                loaded = None\n",
    "            except:\n",
    "                weights = pickle.load(open(self.weights_file, 'rb'))\n",
    "                source = 'pickle file: ' + self.weights_file\n",
    "        if weights is not None:\n",
    "            assert self.n_layers == 0\n",
    "            for p, w in zip(self.collect_params().values(), weights):\n",
    "                if w is not None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Encoder ignoring list of hiddens because n_layer >= 0. Just using first element.\n"
     ]
    }
   ],
   "source": [
    "Enc = Encoder(\n",
    "    model_ctx=model_ctx, \n",
    "    batch_size=args['batch_size'], \n",
    "    input_dim=args['ndim_x'], \n",
    "    ndim_y=args['ndim_y'],\n",
    "    n_hidden=args['enc_n_hidden'], \n",
    "    n_layers=args['enc_n_layer'], \n",
    "    nonlin=args['enc_nonlinearity'], \n",
    "    weights_file=args['enc_weights'], \n",
    "    freeze=args['enc_freeze'], \n",
    "    latent_nonlin=args['latent_nonlinearity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Decoder ignoring list of hiddens because n_layer >= 0. Just using first element.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-6fe0edf3e877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mweights_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dec_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfreeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dec_freeze'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     latent_nonlin=args['latent_nonlinearity'])\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\workspace\\Implementation\\w-lda\\models\\dirichlet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_ctx, batch_size, output_dim, ndim_y, n_hidden, n_layers, nonlin, weights_file, freeze, latent_nonlin, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mnonlin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0min_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHybridSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'decoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "Dec = Decoder(\n",
    "    model_ctx=model_ctx, \n",
    "    batch_size=args['batch_size'], \n",
    "    output_dim=args['ndim_x'], \n",
    "    ndim_y=args['ndim_y'],\n",
    "    n_hidden=[128], \n",
    "    n_layers=args['dec_n_layer'], \n",
    "    nonlin=args['dec_nonlinearity'],\n",
    "    weights_file=args['dec_weights'], \n",
    "    freeze=args['dec_freeze'], \n",
    "    latent_nonlin=args['latent_nonlinearity'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['dec_n_hidden'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Decoder ignoring list of hiddens because n_layer >= 0. Just using first element.\n",
      "NOTE: Latent z will be fed to decoder in logit-space (-inf,inf).\n"
     ]
    }
   ],
   "source": [
    "Dis_y = Discriminator_y(\n",
    "    model_ctx=model_ctx, \n",
    "    batch_size=args['batch_size'], \n",
    "    ndim_y=args['ndim_y'],\n",
    "    n_hidden=args['dis_n_hidden'], \n",
    "    n_layers=args['dis_n_layer'],\n",
    "    nonlin=args['dis_nonlinearity'], \n",
    "    weights_file=args['dis_y_weights'],\n",
    "    freeze=args['dis_freeze'], \n",
    "    latent_nonlin=args['latent_nonlinearity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
