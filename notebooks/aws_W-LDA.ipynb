{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import socket\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "from scipy.special import logit\n",
    "import matplotlib as mpl\n",
    "# mpl.use('Agg')\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import mxnet as mx\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from IPython import embed\n",
    "import collections\n",
    "\n",
    "def gpu_helper(gpu):\n",
    "    if gpu >= 0 and gpu_exists(gpu):\n",
    "        model_ctx = mx.gpu(gpu)\n",
    "    else:\n",
    "        model_ctx = mx.cpu()\n",
    "    return model_ctx\n",
    "\n",
    "def gpu_exists(gpu):\n",
    "    try:\n",
    "        mx.nd.zeros((1,), ctx=mx.gpu(gpu))\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def reverse_dict(d):\n",
    "    return {v:k for k,v in d.items()}\n",
    "\n",
    "def to_numpy(X):\n",
    "    x_npy = []\n",
    "    for x in X:\n",
    "        if isinstance(x,list):\n",
    "            x_npy += [to_numpy(x)]\n",
    "        else:\n",
    "            x_npy += [x.asnumpy()]\n",
    "    return x_npy\n",
    "\n",
    "def stack_numpy(X,xnew):\n",
    "    for i in range(len(X)):\n",
    "        if isinstance(xnew[i],list):\n",
    "            X[i] = stack_numpy(X[i], xnew[i])\n",
    "        else:\n",
    "            X[i] = np.vstack([X[i], xnew[i]])\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_topic_words_decoder_weights(D, data, ctx, k=10, decoder_weights=False):\n",
    "    if decoder_weights:\n",
    "        params = D.collect_params()\n",
    "        params = params['decoder0_dense0_weight'].data().transpose()\n",
    "    else:\n",
    "        y = D.y_as_topics()\n",
    "        params = D(y.copyto(ctx))\n",
    "    top_word_ids = mx.nd.argsort(params, axis=1, is_ascend=False)[:,:k].asnumpy()\n",
    "    if hasattr(data, 'id_to_word'):\n",
    "        top_word_strings = [[data.id_to_word[int(w)] for w in topic] for topic in top_word_ids]\n",
    "    else:\n",
    "        top_word_strings = [[data.maps['dim2vocab'][int(w)] for w in topic] for topic in top_word_ids]\n",
    "\n",
    "    return top_word_strings\n",
    "\n",
    "\n",
    "def get_topic_words(D, data, ctx, k=10):\n",
    "    y, z = D.yz_as_topics()\n",
    "    if z is not None:\n",
    "        params = D(y.copyto(ctx), z.copyto(ctx))\n",
    "    else:\n",
    "        params = D(y.copyto(ctx), None)\n",
    "    top_word_ids = mx.nd.argsort(params, axis=1, is_ascend=False)[:,:k].asnumpy()\n",
    "    if hasattr(data, 'id_to_word'):\n",
    "        top_word_strings = [[data.id_to_word[int(w)] for w in topic] for topic in top_word_ids]\n",
    "    else:\n",
    "        top_word_strings = [[data.maps['dim2vocab'][int(w)] for w in topic] for topic in top_word_ids]\n",
    "\n",
    "    return top_word_strings\n",
    "\n",
    "\n",
    "def calc_topic_uniqueness(top_words_idx_all_topics):\n",
    "    \"\"\"\n",
    "    This function calculates topic uniqueness scores for a given list of topics.\n",
    "    For each topic, the uniqueness is calculated as:  (\\sum_{i=1}^n 1/cnt(i)) / n,\n",
    "    where n is the number of top words in the topic and cnt(i) is the counter for the number of times the word\n",
    "    appears in the top words of all the topics.\n",
    "    :param top_words_idx_all_topics: a list, each element is a list of top word indices for a topic\n",
    "    :return: a dict, key is topic_id (starting from 0), value is topic_uniquness score\n",
    "    \"\"\"\n",
    "    n_topics = len(top_words_idx_all_topics)\n",
    "\n",
    "    # build word_cnt_dict: number of times the word appears in top words\n",
    "    word_cnt_dict = collections.Counter()\n",
    "    for i in range(n_topics):\n",
    "        word_cnt_dict.update(top_words_idx_all_topics[i])\n",
    "\n",
    "    uniqueness_dict = dict()\n",
    "    for i in range(n_topics):\n",
    "        cnt_inv_sum = 0.0\n",
    "        for ind in top_words_idx_all_topics[i]:\n",
    "            cnt_inv_sum += 1.0 / word_cnt_dict[ind]\n",
    "        uniqueness_dict[i] = cnt_inv_sum / len(top_words_idx_all_topics[i])\n",
    "\n",
    "    return uniqueness_dict\n",
    "\n",
    "def request_pmi(topic_dict=None, filename='', port=1234):\n",
    "    try:\n",
    "        # create a socket object\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "        # get local machine name\n",
    "        host = socket.gethostname()\n",
    "        # host = socket.gethostbyname('localhost')\n",
    "\n",
    "        # connection to hostname on the port.\n",
    "        s.connect((host, port))\n",
    "\n",
    "        if filename != '':\n",
    "            s.sendall(pickle.dumps(filename), )\n",
    "        else:\n",
    "            s.send(pickle.dumps(topic_dict), )\n",
    "\n",
    "        data = []\n",
    "        while True:\n",
    "            packet = s.recv(4096)\n",
    "            # time.sleep(1.0)\n",
    "            # print('looking at packet # {0}'.format(len(data)))\n",
    "            # print(packet)\n",
    "            # print(type(packet))\n",
    "            wait = len(packet)\n",
    "            if not packet:\n",
    "                # embed()\n",
    "                break\n",
    "            data.append(packet)\n",
    "            # print('received packet # {0}'.format(len(data)))\n",
    "            # time.sleep(1.0)\n",
    "        res_dict = pickle.loads(b\"\".join(data))\n",
    "\n",
    "        s.close()\n",
    "        pmi_dict = res_dict['pmi_dict']\n",
    "        npmi_dict = res_dict['npmi_dict']\n",
    "    except:\n",
    "        # print('Failed to run NPMI calc, NPMI and PMI set to 0.0')\n",
    "        pmi_dict = dict()\n",
    "        npmi_dict = dict()\n",
    "        for k in topic_dict:\n",
    "            pmi_dict[k] = 0\n",
    "            npmi_dict[k] = 0\n",
    "        # embed()\n",
    "\n",
    "    return pmi_dict, npmi_dict\n",
    "\n",
    "\n",
    "def print_topics(topic_json, npmi_dict, topic_uniqs, data, print_topic_names=False):\n",
    "    for k,v in topic_json.items():\n",
    "        prefix_msg = '[ '\n",
    "        if hasattr(data, 'maps') and print_topic_names:\n",
    "            prefix_msg += data.maps['dim2topic'][k]\n",
    "        else:\n",
    "            prefix_msg += str(k)\n",
    "        if hasattr(data, 'selected_topics') and print_topic_names:\n",
    "            if data.maps['dim2topic'][k] in data.selected_topics:\n",
    "                prefix_msg += '*'\n",
    "        prefix_msg += ' - '\n",
    "        prefix_msg += '{:.5g}'.format(topic_uniqs[k])\n",
    "        prefix_msg += ' - '\n",
    "        prefix_msg += '{:.5g}'.format(npmi_dict[k])\n",
    "        prefix_msg += ']: '\n",
    "        print(prefix_msg, v)\n",
    "\n",
    "def print_topic_with_scores(topic_json, **kwargs):\n",
    "    \"\"\"\n",
    "    :param topic_json:\n",
    "    :param kwargs: dict_name: content_dict; special argument sortby='xxx' will enable descending sort in printed result\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    topic_keys = sorted(list(topic_json.keys()))\n",
    "\n",
    "    sortby = kwargs.pop('sortby', None)\n",
    "    if sortby is None:\n",
    "        sortby = kwargs.pop('sort_by', None)\n",
    "\n",
    "    if sortby in kwargs.keys():\n",
    "        topic_keys = sorted(kwargs[sortby], key=kwargs[sortby].get)[::-1]\n",
    "\n",
    "    entries = []\n",
    "    dict_names = sorted(list(kwargs.keys()))\n",
    "    header_str = 'Avg scores: '\n",
    "    for dn in dict_names:\n",
    "        assert isinstance(kwargs[dn], dict)\n",
    "        header_str += '{}: {:.2f} '.format(dn, mean_dict(kwargs[dn]))\n",
    "    for k in topic_keys:\n",
    "        score_str = []\n",
    "        for dn in dict_names:\n",
    "            # score_str.append('{} {:.2f}'.format(dn, kwargs[dn][k]))\n",
    "            score_str.append('{:.2f}'.format(kwargs[dn][k]))\n",
    "        score_str = ', '.join(score_str)\n",
    "        entries.append('T{} [{}] '.format(k, score_str) + ', '.join(topic_json[k]))\n",
    "\n",
    "    msg = header_str + '\\n' + '\\n'.join(entries)\n",
    "    print(msg)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core.py\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import log_loss, v_measure_score\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, io\n",
    "\n",
    "\n",
    "# import misc as nm\n",
    "# import datasets as nuds\n",
    "import scipy.sparse as sparse\n",
    "import json\n",
    "# import wordvectors as nuwe\n",
    "\n",
    "\n",
    "class Data(object):\n",
    "    '''\n",
    "    Data Generator object. Main functionality is contained in ``minibatch'' method\n",
    "    and ``subsampled_labeled_data'' if training in a semi-supervised fashion.\n",
    "    Introducing new datasets requires implementing ``load'' and possibly overwriting\n",
    "    portions of ``__init__''.\n",
    "    '''\n",
    "    def __init__(self, batch_size=1, data_path='', ctx=mx.cpu(0)):\n",
    "        '''\n",
    "        Constructor for Data.\n",
    "        Args\n",
    "        ----\n",
    "        batch_size: int, default 1\n",
    "          An integer specifying the batch size - required for precompiling the graph.\n",
    "        data_path: string, default ''\n",
    "          This is primarily used by Mulan to specify which dataset to load from Mulan,\n",
    "          e.g., data_path='bibtex'.\n",
    "        ctx: mxnet device context, default mx.cpu(0)\n",
    "          Which device to store/run the data and model on.\n",
    "        Returns\n",
    "        -------\n",
    "        Data object\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        if data_path == '':\n",
    "            data, labels, maps = self.load()\n",
    "        else:\n",
    "            data, labels, maps = self.load(data_path)\n",
    "        self.ctx = ctx\n",
    "        # # normalize the data:\n",
    "        # def softmax(x):\n",
    "        #     \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        #     e_x = np.exp(x - np.max(x, axis=1).reshape((-1,1)))\n",
    "        #     return e_x / np.sum(e_x, axis=1).reshape((-1,1))\n",
    "        # for i in range(len(data)):\n",
    "        #     data[i] = softmax(data[i])\n",
    "\n",
    "        data_names = ['train','valid','test','train_with_labels','valid_with_labels','test_with_labels']\n",
    "        label_names = ['train_label', 'valid_label', 'test_label']\n",
    "\n",
    "        self.data = dict(zip(data_names, data))\n",
    "        self.labels = dict(zip(label_names, labels))\n",
    "\n",
    "        # repeat data to at least match batch_size\n",
    "        for k, v in self.data.items():\n",
    "            if v is not None and v.shape[0] < self.batch_size:\n",
    "                print('NOTE: Number of samples for {0} is smaller than batch_size ({1}<{2}). Duplicating samples to exceed batch_size.'.format(k,v.shape[0],self.batch_size))\n",
    "                if type(v) is np.ndarray:\n",
    "                    self.data[k] = np.tile(v, (self.batch_size // v.shape[0] + 1, 1))\n",
    "                else:\n",
    "                    self.data[k] = mx.nd.tile(v, (self.batch_size // v.shape[0] + 1, 1))\n",
    "\n",
    "        for k, v in self.labels.items():\n",
    "            if v is not None and v.shape[0] < self.batch_size:\n",
    "                print('NOTE: Number of samples for {0} is smaller than batch_size ({1}<{2}). Duplicating samples to exceed batch_size.'.format(k,v.shape[0],self.batch_size))\n",
    "                self.labels[k] = np.tile(v, (self.batch_size // v.shape[0] + 1, ))\n",
    "\n",
    "        map_names = ['vocab2dim','dim2vocab','topic2dim','dim2topic']\n",
    "        self.maps = dict(zip(map_names, maps))\n",
    "        dls = [self.dataloader(d, batch_size) for d in data]\n",
    "        dis = [iter(dl) if dl is not None else None for dl in dls]\n",
    "        self.dataloaders = dict(zip(data_names, dls))\n",
    "        self.dataiters = dict(zip(data_names, dis))\n",
    "        self.wasreset = dict(zip(data_names, np.ones(len(data_names), dtype='bool')))\n",
    "\n",
    "        self.data_dim = self.data['train'].shape[1]\n",
    "        if self.data['train_with_labels'] is not None:\n",
    "            self.label_dim = self.data['train_with_labels'].shape[1] - self.data['train'].shape[1]\n",
    "\n",
    "\n",
    "    def dataloader(self, data, batch_size, shuffle=True):\n",
    "        '''\n",
    "        Constructs a data loader for generating minibatches of data.\n",
    "        Args\n",
    "        ----\n",
    "        data: numpy array, no default\n",
    "          The data from which to load minibatches.\n",
    "        batch_size: integer, no default\n",
    "          The # of samples returned in each minibatch.\n",
    "        shuffle: boolean, default True\n",
    "          Whether or not to shuffle the data prior to returning the data loader.\n",
    "        Returns\n",
    "        -------\n",
    "        DataLoader: A gluon DataLoader iterator\n",
    "        '''\n",
    "        if data is None:\n",
    "            return None\n",
    "        else:\n",
    "            # inds = np.arange(data.shape[0])\n",
    "            # if shuffle:\n",
    "            #     np.random.shuffle(inds)\n",
    "            # ordered = data[inds]\n",
    "            # N, r = divmod(data.shape[0], batch_size)\n",
    "            # if r > 0:\n",
    "            #     ordered = np.vstack([ordered, ordered[:r]])\n",
    "            if type(data) is np.ndarray:\n",
    "                return gluon.data.DataLoader(data, batch_size, last_batch='discard', shuffle=shuffle)\n",
    "            else:\n",
    "                return io.NDArrayIter(data={'data': data}, batch_size=batch_size, shuffle=shuffle, last_batch_handle='discard')\n",
    "\n",
    "    def force_reset_data(self, key, shuffle=True):\n",
    "        '''\n",
    "        Resets minibatch index to zero to restart an epoch.\n",
    "        Args\n",
    "        ----\n",
    "        key: string, no default\n",
    "          Required to select appropriate data in ``data'' object,\n",
    "          e.g., 'train', 'test', 'train_with_labels', 'test_with_labels'.\n",
    "        shuffle: boolean, default True\n",
    "          Whether or not to shuffle the data prior to returning the data loader.\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing.\n",
    "        '''\n",
    "        if self.data[key] is not None:\n",
    "            if type(self.data[key]) is np.ndarray:\n",
    "                self.dataloaders[key] = self.dataloader(self.data[key], self.batch_size, shuffle)\n",
    "                self.dataiters[key] = iter(self.dataloaders[key])\n",
    "            else:\n",
    "                self.dataiters[key].hard_reset()\n",
    "            self.wasreset[key] = True\n",
    "\n",
    "    def minibatch(self, key, pad_width=0):\n",
    "        '''\n",
    "        Returns a minibatch of data (stored on device self.ctx).\n",
    "        Args\n",
    "        ----\n",
    "        key: string, no default\n",
    "          Required to select appropriate data in ``data'' object,\n",
    "          e.g., 'train', 'test', 'train_with_labels', 'test_with_labels'.\n",
    "        pad_width: integer, default 0\n",
    "          The amount to zero-pad the labels to match the dimensionality of z.\n",
    "        Returns\n",
    "        -------\n",
    "        minibatch: NDArray on device self.ctx\n",
    "          An NDArray of size batch_size x # of features.\n",
    "        '''\n",
    "        if self.dataiters[key] is None:\n",
    "            return None\n",
    "        else:\n",
    "            if type(self.data[key]) is np.ndarray:\n",
    "                try:\n",
    "                    mb = self.dataiters[key].__next__().reshape((self.batch_size, -1))\n",
    "                    if pad_width > 0:\n",
    "                        mb = mx.nd.concat(mb, mx.nd.zeros((self.batch_size, pad_width)))\n",
    "                    return mb.copyto(self.ctx)\n",
    "                except:\n",
    "                    self.force_reset_data(key)\n",
    "                    mb = self.dataiters[key].__next__().reshape((self.batch_size, -1))\n",
    "                    if pad_width > 0:\n",
    "                        mb = mx.nd.concat(mb, mx.nd.zeros((self.batch_size, pad_width)))\n",
    "                    return mb.copyto(self.ctx)\n",
    "            else:\n",
    "                try:\n",
    "                    mb = self.dataiters[key].__next__().data[0].as_in_context(self.ctx)\n",
    "                    return mb\n",
    "                except:\n",
    "                    self.dataiters[key].hard_reset()\n",
    "                    mb = self.dataiters[key].__next__().data[0].as_in_context(self.ctx)\n",
    "                    return mb\n",
    "\n",
    "    def get_documents(self, key, split_on=None):\n",
    "        '''\n",
    "        Retrieves a minibatch of documents via ``data'' object parameter.\n",
    "        Args\n",
    "        ----\n",
    "        key: string, no default\n",
    "          Required to select appropriate data in ``data'' object,\n",
    "          e.g., 'train', 'test', 'train_with_labels', 'test_with_labels'.\n",
    "        split_on: integer, default None\n",
    "          Useful if self.data[key] contains both data and labels in one\n",
    "          matrix and want to split them, e.g., split_on = data_dim.\n",
    "        Returns\n",
    "        -------\n",
    "        minibatch: NDArray if split_on is None, else [NDarray, NDArray]\n",
    "        '''\n",
    "        if 'labels' in key:\n",
    "            batch = self.minibatch(key, pad_width=self.label_pad_width)\n",
    "        else:\n",
    "            batch = self.minibatch(key)\n",
    "        if split_on is not None:\n",
    "            batch, labels = batch[:,:split_on], batch[:,split_on:]\n",
    "            return batch, labels\n",
    "        else:\n",
    "            return batch\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_series(y, ylabel, file, args, iteration, total_samples, labels=None):\n",
    "        '''\n",
    "        Plots and saves a figure of y vs iterations and epochs to file.\n",
    "        Args\n",
    "        ----\n",
    "        y: a list (of lists) or numpy array, no default\n",
    "          A list (of possibly another list) of numbers to plot.\n",
    "        ylabel: string, no default\n",
    "          The label for the y-axis.\n",
    "        file: string, no default\n",
    "          A path with filename to save the figure to.\n",
    "        args: dictionary, no default\n",
    "          A dictionary of model, training, and evaluation specifications.\n",
    "        iteration: integer, no default\n",
    "          The current iteration in training.\n",
    "        total_samples: integer, no default\n",
    "          The total number of samples in the dataset - used along with batch_size\n",
    "          to convert iterations to epochs.\n",
    "        labels: list of strings, default None\n",
    "          If y is a list of lists, the labels contains names for each element\n",
    "          in the nested list. This is used to create an appropriate legend\n",
    "          for the plot.\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing.\n",
    "        '''\n",
    "        if len(y) > 0:\n",
    "            fig = plt.figure()\n",
    "            ax = plt.subplot(111)\n",
    "            x = np.linspace(0, iteration, num=len(y)) * args['batch_size'] / total_samples\n",
    "            y = np.array(y)\n",
    "            if len(y.shape) > 1:\n",
    "                for i in range(y.shape[1]):\n",
    "                    if labels is None:\n",
    "                        plt.plot(x,y[:,i])\n",
    "                    else:\n",
    "                        plt.plot(x,y[:,i], label=labels[i])\n",
    "            else:\n",
    "                plt.plot(x,y)\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            plt.grid(True)\n",
    "\n",
    "            ax2 = ax.twiny()\n",
    "\n",
    "            # https://pythonmatplotlibtips.blogspot.com/2018/01/add-second-x-axis-below-first-x-axis-python-matplotlib-pyplot.html\n",
    "            # Decide the ticklabel position in the new x-axis,\n",
    "            # then convert them to the position in the old x-axis\n",
    "            # xticks list seems to be padded with extra lower and upper ticks --> subtract 2 from length\n",
    "            newlabel = np.around(np.linspace(0, iteration, num=len(ax.get_xticks())-2)).astype('int') # labels of the xticklabels: the position in the new x-axis\n",
    "            # ax2.set_xticks(ax.get_xticks())\n",
    "            ax2.set_xticks(newlabel * args['batch_size'] / total_samples)\n",
    "            ax2.set_xticklabels(newlabel//1000)\n",
    "\n",
    "            ax2.xaxis.set_ticks_position('bottom') # set the position of the second x-axis to bottom\n",
    "            ax2.xaxis.set_label_position('bottom') # set the position of the second x-axis to bottom\n",
    "            ax2.spines['bottom'].set_position(('outward', 36))\n",
    "            ax2.set_xlabel('Thousand Iterations')\n",
    "            ax2.set_xlim(ax.get_xlim())\n",
    "\n",
    "            if labels is not None:\n",
    "                lgd = ax.legend(loc='center left', bbox_to_anchor=(1.05, 1))\n",
    "                fig.savefig(args['saveto']+file, additional_artists=[lgd], bbox_inches='tight')\n",
    "            else:\n",
    "                fig.tight_layout()\n",
    "                fig.savefig(args['saveto']+file)\n",
    "            plt.close()\n",
    "\n",
    "    def load(self, path=''):\n",
    "        '''\n",
    "        Loads data and maps from path.\n",
    "        Args\n",
    "        ----\n",
    "        path: string, default ''\n",
    "          A path to the data file.\n",
    "        Returns\n",
    "        -------\n",
    "        data: list of numpy arrays\n",
    "          A list of the different subsets of data, e.g.,\n",
    "          `train', `test', 'train_with_labels', 'test_with_labels'.\n",
    "        maps: list of dictionaries\n",
    "          A list of dictionaries for mapping between dimensions and strings,\n",
    "          e.g., 'vocab2dim', 'dim2vocab', 'topic2dim', 'dim2topic'.\n",
    "        '''\n",
    "        data = [np.empty((1,1)) for data in ['train','valid','test','train_with_labels','valid_with_labels','test_with_labels']]\n",
    "        maps = [{'a':0}, {0:'a'}, {'Letters':0}, {0:'Letters'}]\n",
    "        self.data_path = path + '***.npz'\n",
    "        return data, maps\n",
    "\n",
    "\n",
    "class ENet(gluon.HybridBlock):\n",
    "    '''\n",
    "    A gluon HybridBlock Encoder (skeleton) class.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor for Encoder.\n",
    "        Args\n",
    "        ----\n",
    "        None\n",
    "        Returns\n",
    "        -------\n",
    "        Encoder object\n",
    "        '''\n",
    "        super(ENet, self).__init__()\n",
    "            \n",
    "    def hybrid_forward(self, x):\n",
    "        '''\n",
    "        Encodes x.\n",
    "        Args\n",
    "        ----\n",
    "        x: mx.NDArray or sym, No default\n",
    "          Input to encoder.\n",
    "        Returns (should)\n",
    "        -------\n",
    "        params: list of NDArray or sym\n",
    "          parameters for the encoding distribution\n",
    "        samples: NDArray or sym\n",
    "          samples drawn from the encoding distribution\n",
    "        '''\n",
    "        raise NotImplementedError('Need to write your own Encoder that inherits from ENet. Put this file in models/.')\n",
    "\n",
    "    def init_weights(self, weights=None):\n",
    "        '''\n",
    "        Initializes the encoder weights. Default is Xavier initialization.\n",
    "        Args\n",
    "        ----\n",
    "        weights: list of numpy arrays, No default\n",
    "          Weights to load into the model. Not required. Preference is to\n",
    "          load weights from file.\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing.\n",
    "        '''\n",
    "        loaded = False\n",
    "        source = 'keyword argument'\n",
    "        if self.weights_file != '' and weights is None:\n",
    "            try:\n",
    "                self.load_params(self.weights_file, self.model_ctx)\n",
    "                source = 'mxnet weights file: '+self.weights_file\n",
    "                print('NOTE: Loaded encoder weights from '+source+'.')\n",
    "                if self.freeze:\n",
    "                    self.freeze_params()\n",
    "                    print('NOTE: Froze encoder weights from '+source+'.')\n",
    "                weights = None\n",
    "                loaded = True\n",
    "            except:\n",
    "                weights = pickle.load(open(self.weights_file,'rb'))\n",
    "                source = 'pickle file: '+self.weights_file\n",
    "        if weights is not None:\n",
    "            assert self.n_layers == 0\n",
    "            for p,w in zip(self.collect_params().values(), weights):\n",
    "                if w is not None:\n",
    "                    p.initialize(mx.init.Constant(mx.nd.array(w.squeeze())), ctx=self.model_ctx)\n",
    "                    if self.freeze:\n",
    "                        p.lr_mult = 0.\n",
    "            print('NOTE: Loaded encoder weights from '+source+'.')\n",
    "            if self.freeze:\n",
    "                print('NOTE: Froze encoder weights from '+source+'.')\n",
    "            loaded = True\n",
    "        if not loaded:\n",
    "            self.collect_params().initialize(mx.init.Xavier(), ctx=self.model_ctx)\n",
    "            print('NOTE: Randomly initialized encoder weights.')\n",
    "            # self.collect_params().initialize(mx.init.Zero(), ctx=self.model_ctx)\n",
    "            # print('NOTE: initialized encoder weights to ZERO.')\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for p in self.collect_params().values():\n",
    "            p.lr_mult = 0.\n",
    "\n",
    "\n",
    "class DNet(gluon.HybridBlock):\n",
    "    '''\n",
    "    A gluon HybridBlock Decoder (skeleton) class.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor for Decoder.\n",
    "        Args\n",
    "        ----\n",
    "        None\n",
    "        Returns\n",
    "        -------\n",
    "        Decoder object\n",
    "        '''\n",
    "        super(DNet, self).__init__()\n",
    "\n",
    "    def hybrid_forward(self, y, z):\n",
    "        '''\n",
    "        Decodes x.\n",
    "        Args\n",
    "        ----\n",
    "        x: mx.NDArray or sym, no default\n",
    "          Input to decoder.\n",
    "        Returns (should)\n",
    "        -------\n",
    "        params: list of NDArray or sym\n",
    "          parameters for the encoding distribution\n",
    "        samples: NDArray or sym\n",
    "          samples drawn from the encoding distribution. None if sampling is not implemented.\n",
    "        '''\n",
    "        raise NotImplementedError('Need to write your own Decoder that inherits from ENet. Put this file in models/.')\n",
    "\n",
    "    def init_weights(self, weights=None):\n",
    "        '''\n",
    "        Initializes the decoder weights. Default is Xavier initialization.\n",
    "        Args\n",
    "        ----\n",
    "        weights: list of numpy arrays, No default\n",
    "          Weights to load into the model. Not required. Preference is to\n",
    "          load weights from file.\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing.\n",
    "        '''\n",
    "        loaded = False\n",
    "        source = 'keyword argument'\n",
    "        if self.weights_file != '' and weights is None:\n",
    "            try:\n",
    "                self.load_params(self.weights_file, self.model_ctx)\n",
    "                source = 'mxnet weights file: '+self.weights_file\n",
    "                print('NOTE: Loaded decoder weights from '+source+'.')\n",
    "                if self.freeze:\n",
    "                    self.freeze_params()\n",
    "                    print('NOTE: Froze decoder weights from '+source+'.')\n",
    "                weights = None\n",
    "                loaded = True\n",
    "            except:\n",
    "                weights = pickle.load(open(self.weights_file,'rb'))\n",
    "                source = 'pickle file: '+self.weights_file\n",
    "        if weights is not None:\n",
    "            assert self.n_layers == 0\n",
    "            for p,w in zip(self.collect_params().values(), weights):\n",
    "                if w is not None:\n",
    "                    p.initialize(mx.init.Constant(mx.nd.array(w.squeeze())), ctx=self.model_ctx)\n",
    "                    if self.freeze:\n",
    "                        p.lr_mult = 0.\n",
    "            print('NOTE: Loaded decoder weights from '+source+'.')\n",
    "            if self.freeze:\n",
    "                print('NOTE: Froze decoder weights from '+source+'.')\n",
    "            loaded = True\n",
    "        if not loaded:\n",
    "            self.collect_params().initialize(mx.init.Xavier(), ctx=self.model_ctx)\n",
    "            print('NOTE: Randomly initialized decoder weights.')\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for p in self.collect_params().values():\n",
    "            p.lr_mult = 0.\n",
    "\n",
    "\n",
    "class Compute(object):\n",
    "    '''\n",
    "    Skeleton class to manage training, testing, and retrieving outputs.\n",
    "    See ``compute_op.py'' for ``flesh''.\n",
    "    '''\n",
    "    def __init__(self,  data, Enc, Dec,  Dis_y, args):\n",
    "        '''\n",
    "        Constructor for Compute.\n",
    "        Returns\n",
    "        -------\n",
    "        Compute object\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.Enc = Enc\n",
    "        self.Dec = Dec\n",
    "        self.Dis_y = Dis_y\n",
    "        self.args = args\n",
    "        self.model_ctx = Enc.model_ctx\n",
    "        self.ndim_y = args['ndim_y']\n",
    "\n",
    "        weights_enc = Enc.collect_params()\n",
    "        weights_dec = Dec.collect_params()\n",
    "        weights_dis_y = Dis_y.collect_params()\n",
    "\n",
    "        if self.args['optim'] == 'Adam':\n",
    "            # args_dict = {'learning_rate': self.args['learning_rate'], 'beta1': self.args['betas'][0], 'beta2': self.args['betas'][1], 'epsilon': self.args['epsilon']}\n",
    "            # optimizer_enc = gluon.Trainer(weights_enc, 'adam', args_dict)\n",
    "            # optimizer_dec = gluon.Trainer(weights_dec, 'adam', args_dict)\n",
    "            # optimizer_dis_y = gluon.Trainer(weights_dis_y, 'adam', args_dict)\n",
    "            optimizer_enc = gluon.Trainer(weights_enc, 'adam', {'learning_rate': self.args['learning_rate'], 'beta1': 0.99})\n",
    "            optimizer_dec = gluon.Trainer(weights_dec, 'adam', {'learning_rate': self.args['learning_rate'], 'beta1': 0.99})\n",
    "            optimizer_dis_y = gluon.Trainer(weights_dis_y, 'adam', {'learning_rate': self.args['learning_rate']})\n",
    "        if self.args['optim'] == 'Adadelta':\n",
    "            # note: learning rate has no effect on Adadelta --> https://mxnet.incubator.apache.org/_modules/mxnet/optimizer.html#AdaDelta\n",
    "            args_dict = {'rescale_grad': 1}  #, 'clip_gradient': 0.1}\n",
    "            optimizer_enc = gluon.Trainer(weights_enc, 'adadelta', args_dict)\n",
    "            optimizer_dec = gluon.Trainer(weights_dec, 'adadelta', args_dict)\n",
    "            optimizer_dis_y = gluon.Trainer(weights_dis_y, 'adadelta', args_dict)\n",
    "        if self.args['optim'] == 'RMSprop':\n",
    "            args_dict = {'learning_rate': self.args['learning_rate'], 'epsilon': 1e-10, 'alpha': 0.9}\n",
    "            optimizer_enc = gluon.Trainer(weights_enc, 'rmsprop', args_dict)\n",
    "            optimizer_dec = gluon.Trainer(weights_dec, 'rmsprop', args_dict)\n",
    "            optimizer_dis_y = gluon.Trainer(weights_dis_y, 'rmsprop', args_dict)\n",
    "        if self.args['optim'] == 'SGD':\n",
    "            args_dict = {'learning_rate': self.args['learning_rate'], 'wd': self.args['weight_decay'], 'rescale_grad': 1., 'momentum': 0.0, 'lazy_update': False}\n",
    "            optimizer_enc = gluon.Trainer(weights_enc, 'sgd', args_dict)\n",
    "            optimizer_dec = gluon.Trainer(weights_dec, 'sgd', args_dict)\n",
    "            optimizer_dis_y = gluon.Trainer(weights_dis_y, 'sgd', args_dict)\n",
    "\n",
    "        self.optimizer_enc = optimizer_enc\n",
    "        self.optimizer_dec = optimizer_dec\n",
    "        self.optimizer_dis_y = optimizer_dis_y\n",
    "        self.weights_enc = weights_enc\n",
    "        self.weights_dec = weights_dec\n",
    "        self.weights_dis_y = weights_dis_y\n",
    "\n",
    "    def train_op(self):\n",
    "        '''\n",
    "        Trains the model using one minibatch of data.\n",
    "        '''\n",
    "        return None, None, None, None\n",
    "\n",
    "    def test_op(self, num_samples=None, num_epochs=None, reset=True, dataset='test'):\n",
    "        '''\n",
    "        Evaluates the model using num_samples.\n",
    "        Args\n",
    "        ----\n",
    "        num_samples: integer, default None\n",
    "          The number of samples to evaluate on. This is converted to\n",
    "          evaluating on (num_samples // batch_size) minibatches.\n",
    "        num_epochs: integer, default None\n",
    "          The number of epochs to evaluate on. This used if num_samples\n",
    "          is not specified. If neither is specified, defaults to 1 epoch.\n",
    "        reset: bool, default True\n",
    "          Whether to reset the test data index to 0 before iterating\n",
    "          through and evaluating on minibatches.\n",
    "        dataset: string, default 'test':\n",
    "          Which dataset to evaluate on: 'valid' or 'test'.\n",
    "        '''\n",
    "        if num_samples is None:\n",
    "            num_samples = self.data.data[dataset].shape[0]\n",
    "\n",
    "        if reset:\n",
    "            # Reset Data to Index Zero\n",
    "            self.data.force_reset_data(dataset)\n",
    "            self.data.force_reset_data(dataset+'_with_labels')\n",
    "\n",
    "        return None, None, None, None\n",
    "\n",
    "    def get_outputs(self, num_samples=None, num_epochs=None, reset=True, dataset='test'):\n",
    "        '''\n",
    "        Retrieves raw outputs from model for num_samples.\n",
    "        Args\n",
    "        ----\n",
    "        num_samples: integer, default None\n",
    "          The number of samples to evaluate on. This is converted to\n",
    "          evaluating on (num_samples // batch_size) minibatches.\n",
    "        num_epochs: integer, default None\n",
    "          The number of epochs to evaluate on. This used if num_samples\n",
    "          is not specified. If neither is specified, defaults to 1 epoch.\n",
    "        reset: bool, default True\n",
    "          Whether to reset the test data index to 0 before iterating\n",
    "          through and evaluating on minibatches.\n",
    "        dataset: string, default 'test':\n",
    "          Which dataset to evaluate on: 'valid' or 'test'.\n",
    "        '''\n",
    "        if num_samples is None:\n",
    "            num_samples = self.data.data[dataset].shape[0]\n",
    "\n",
    "        if reset:\n",
    "            # Reset Data to Index Zero\n",
    "            self.data.force_reset_data(dataset)\n",
    "            self.data.force_reset_data(dataset+'_with_labels')\n",
    "\n",
    "        return None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npmi_calc.py\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import socket\n",
    "import itertools\n",
    "\n",
    "py_version = 2\n",
    "if sys.version_info.major == 3:\n",
    "    import _pickle as pickle\n",
    "\n",
    "    py_version = 3\n",
    "else:\n",
    "    import cPickle as pickle\n",
    "# log_prefix = re.compile(r\"^\\[[^\\]]+\\]\")\n",
    "# topics_pattern = re.compile(r'[T|t]opics from epoch:([^\\s]+)\\s*\\(num_topics:([0-9]+)\\):')\n",
    "\n",
    "phrase_split_pattern = re.compile(r'-|_')\n",
    "\n",
    "\n",
    "def get_terminal_width():\n",
    "    try:\n",
    "        term_cols = os.get_terminal_size().columns\n",
    "    except:\n",
    "        term_cols = 80\n",
    "    return term_cols\n",
    "\n",
    "\n",
    "def print_center(string):\n",
    "    n_cols = get_terminal_width()\n",
    "    spacer = '  '\n",
    "    center_string = spacer + string + spacer\n",
    "    n_front = (n_cols - len(center_string)) // 2\n",
    "    n_back = n_cols - len(center_string) - n_front\n",
    "    new_string = ' ' * n_front + center_string + ' ' * n_back\n",
    "    print(new_string)\n",
    "\n",
    "\n",
    "def print_header(string, skipline=True, symbol='#', doubleline=False):\n",
    "    n_cols = get_terminal_width()\n",
    "    if skipline:\n",
    "        print()\n",
    "    print(symbol * n_cols)\n",
    "    if doubleline:\n",
    "        print(symbol * n_cols)\n",
    "    print_center(string)\n",
    "    print(symbol * n_cols)\n",
    "    if doubleline:\n",
    "        print(symbol * n_cols)\n",
    "    if skipline:\n",
    "        print()\n",
    "\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    pass_str = OKGREEN + 'Pass' + ENDC\n",
    "    fail_str = FAIL + 'Fail' + ENDC\n",
    "\n",
    "\n",
    "def print_warning(*args):\n",
    "    s = ' '.join(args)\n",
    "    print(bcolors.WARNING + '{}'.format(s) + bcolors.ENDC)\n",
    "\n",
    "\n",
    "def print_green(*args):\n",
    "    s = ' '.join(args)\n",
    "    print(bcolors.OKGREEN + '{}'.format(s) + bcolors.ENDC)\n",
    "\n",
    "\n",
    "def print_blue(*args):\n",
    "    s = ' '.join(args)\n",
    "    print(bcolors.OKBLUE + '{}'.format(s) + bcolors.ENDC)\n",
    "\n",
    "\n",
    "def print_error(*args):\n",
    "    s = ' '.join(args)\n",
    "    print(bcolors.FAIL + '{}'.format(s) + bcolors.ENDC)\n",
    "\n",
    "\n",
    "class RefCorpus:\n",
    "    def __init__(self):\n",
    "        home_dir = os.path.expanduser('~')\n",
    "        self.wiki_invind_file = os.path.join(home_dir, 'wikipedia.inv_index.pkl')\n",
    "        self.wiki_dict_file = os.path.join(home_dir, 'wikipedia.dict.pkl')\n",
    "\n",
    "    def load_corpus(self):\n",
    "        print_header('Loading reference corpus')\n",
    "\n",
    "        with open(self.wiki_dict_file, 'rb') as f:\n",
    "            if py_version == 3:\n",
    "                corpus_vocab = pickle.load(f, encoding='utf-8')\n",
    "            else:\n",
    "                corpus_vocab = pickle.load(f)\n",
    "\n",
    "        print(len(corpus_vocab))\n",
    "\n",
    "        with open(self.wiki_invind_file, 'rb') as f:\n",
    "            [inv_index, corpus_size] = pickle.load(f)\n",
    "\n",
    "        self.corpus_vocab = corpus_vocab  # number of words\n",
    "        self.inv_index = inv_index  # word_id: [doc_id1, doc_id2...]\n",
    "        self.corpus_size = corpus_size  # number of documents\n",
    "\n",
    "\n",
    "def get_docs_from_index(w, corpus_vocab, inv_index):\n",
    "    wdocs = set()\n",
    "    if re.search(phrase_split_pattern, w):\n",
    "        # this is to handle the phrases in NYT corpus, without which we will have 50% of the words considered OOV.\n",
    "        wdocs = intersecting_docs(w, corpus_vocab, inv_index)\n",
    "    elif w in corpus_vocab:\n",
    "        wdocs = inv_index[corpus_vocab[w]]\n",
    "    return wdocs\n",
    "\n",
    "\n",
    "def intersecting_docs(phrase, corpus_vocab, inverted_index):\n",
    "    words = re.split(phrase_split_pattern, phrase)\n",
    "    intersect_docs = set()\n",
    "    for word in words:\n",
    "        if not word in corpus_vocab:\n",
    "            # if any of the words in the phrase is not the corpus, the phrase also is not in the corpus\n",
    "            return set()\n",
    "        if not intersect_docs:\n",
    "            intersect_docs.update(inverted_index[corpus_vocab[word]])\n",
    "        else:\n",
    "            intersect_docs.intersection_update(inverted_index[corpus_vocab[word]])\n",
    "    return intersect_docs\n",
    "\n",
    "\n",
    "def get_pmi(docs_1, docs_2, corpus_size):\n",
    "    assert len(docs_1)\n",
    "    assert len(docs_2)\n",
    "    small, big = (docs_1, docs_2) if len(docs_1) < len(docs_2) else (docs_2, docs_1)\n",
    "    intersect = small.intersection(big)\n",
    "    pmi = 0.0\n",
    "    npmi = 0.0\n",
    "    if len(intersect):\n",
    "        pmi = math.log(corpus_size) + math.log(len(intersect)) - math.log(len(docs_1)) - math.log((len(docs_2)))\n",
    "        npmi = -1 * pmi / (math.log(len(intersect)) - math.log(corpus_size))\n",
    "\n",
    "    return pmi, npmi\n",
    "\n",
    "\n",
    "def get_idf(w, inv_index, corpus_vocab, corpus_size):\n",
    "    n_docs = len(get_docs_from_index(w, corpus_vocab, inv_index))\n",
    "    return math.log(corpus_size / (n_docs + 1.0))\n",
    "\n",
    "\n",
    "def test_pmi(inv_index, corpus_vocab, corpus_size):\n",
    "    word_pairs = [\n",
    "        [\"apple\", \"ipad\"],\n",
    "        [\"monkey\", \"business\"],\n",
    "        [\"white\", \"house\"],\n",
    "        [\"republican\", \"democrat\"],\n",
    "        [\"china\", \"usa\"],\n",
    "        [\"president\", \"bush\"],\n",
    "        [\"president\", \"george_bush\"],\n",
    "        [\"president\", \"george-bush\"]\n",
    "    ]\n",
    "    pmis = []\n",
    "    for pair in word_pairs:\n",
    "        w1docs = get_docs_from_index(pair[0], corpus_vocab, inv_index)\n",
    "        w2docs = get_docs_from_index(pair[1], corpus_vocab, inv_index)\n",
    "        assert len(w1docs)\n",
    "        assert len(w2docs)\n",
    "        pmi, _ = get_pmi(w1docs, w2docs, corpus_size)\n",
    "        assert pmi > 0.0\n",
    "        print(\"Testing PMI: w1: {}  w2: {}  pmi: {}\".format(pair[0], pair[1], pmi))\n",
    "        pmis.append(pmi)\n",
    "    assert pmis[0] > pmis[1]  # pmi(apple, ipad) > pmi(monkey, business)\n",
    "\n",
    "\n",
    "\n",
    "def get_topic_pmi(wlist, corpus_vocab, inv_index, corpus_size, max_words_per_topic):\n",
    "    num_pairs = 0\n",
    "    pmi = 0.0\n",
    "    npmi = 0.0\n",
    "    # compute topic coherence only for first 10 word in each topic.\n",
    "    wlist = wlist[:max_words_per_topic]\n",
    "    for (w1, w2) in itertools.combinations(wlist, 2):\n",
    "        w1docs = get_docs_from_index(w1, corpus_vocab, inv_index)\n",
    "        w2docs = get_docs_from_index(w2, corpus_vocab, inv_index)\n",
    "        if len(w1docs) and len(w2docs):\n",
    "            word_pair_pmi, word_pair_npmi = get_pmi(w1docs, w2docs, corpus_size)\n",
    "            pmi += word_pair_pmi\n",
    "            npmi += word_pair_npmi\n",
    "            num_pairs += 1\n",
    "    if num_pairs:\n",
    "        pmi /= num_pairs\n",
    "        npmi /= num_pairs\n",
    "    return pmi, npmi, num_pairs\n",
    "\n",
    "\n",
    "def calc_pmi_for_all_topics(topic_dict, corpus_vocab, inv_index, corpus_size):\n",
    "    pmi_dict = dict()\n",
    "    npmi_dict = dict()\n",
    "    for k in topic_dict.keys():\n",
    "        wlist = topic_dict[k]\n",
    "        use_N_words = len(wlist)  # use full list\n",
    "        pmi, npmi, _ = get_topic_pmi(wlist, corpus_vocab, inv_index, corpus_size, use_N_words)\n",
    "        # print(npmi, pmi)\n",
    "        # print(wlist)\n",
    "        pmi_dict[k] = pmi\n",
    "        npmi_dict[k] = npmi\n",
    "\n",
    "    return pmi_dict, npmi_dict\n",
    "\n",
    "\n",
    "def launch_socket(port=1234, ref_corpus=None):\n",
    "    print_header('Launching socket at port {}'.format(port))\n",
    "    # create a socket object\n",
    "    serversocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    serversocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "\n",
    "    # get local machine name\n",
    "    # host = socket.gethostname()\n",
    "    host = socket.gethostbyname('localhost')\n",
    "\n",
    "    # bind to the port\n",
    "    serversocket.bind((host, port))\n",
    "\n",
    "    # queue up to 5 requests\n",
    "    serversocket.listen(5)\n",
    "    print_header('Socket ready port {}'.format(port))\n",
    "\n",
    "    # set up socket first, then load corpus\n",
    "    if ref_corpus is None:\n",
    "        ref_corpus = RefCorpus()\n",
    "        ref_corpus.load_corpus()\n",
    "\n",
    "    test_pmi(ref_corpus.inv_index, ref_corpus.corpus_vocab, ref_corpus.corpus_size)\n",
    "    # test_idf(rc.inv_index, rc.corpus_vocab, rc.corpus_size)\n",
    "    print_header('Test done')\n",
    "\n",
    "    while True:\n",
    "        # establish a connection\n",
    "        clientsocket, addr = serversocket.accept()\n",
    "        start = time.time()\n",
    "\n",
    "        print(\"Got a connection from %s\" % str(addr))\n",
    "        # currentTime = time.ctime(time.time()) + \"\\r\\n\"\n",
    "        # clientsocket.send(currentTime.encode('ascii'))\n",
    "\n",
    "        # json_str = clientsocket.recv(10 * 1024 * 1024)\n",
    "        # topic_json = json.loads(json_str.decode('ascii'))\n",
    "\n",
    "        # https://stackoverflow.com/questions/24726495/pickle-eoferror-ran-out-of-input-when-recv-from-a-socket\n",
    "        # data = b\"\".join(iter(partial(clientsocket.recv, 1024 * 1024), b\"\"))\n",
    "        # topic_dict = pickle.loads(data)\n",
    "        # data = []\n",
    "        # while True:\n",
    "        #     packet = s.recv(1024 * 1024)\n",
    "        #     if not packet: break\n",
    "        #         data.append(packet)\n",
    "        # topic_dict = pickle.loads(b\"\".join(data))\n",
    "        try:\n",
    "            data = clientsocket.recv(1024 * 1024)\n",
    "            # data = []\n",
    "            # while True:\n",
    "            #     print('waiting for packet # {0}'.format(len(data)))\n",
    "            #     packet = clientsocket.recv(4096)\n",
    "            #     print(packet)\n",
    "            #     wait = len(packet)\n",
    "            #     if not packet:\n",
    "            #         break\n",
    "            #     data.append(packet)\n",
    "            #     print('received packet # {0}'.format(len(data)))\n",
    "            # print('got here')\n",
    "            # print(data)\n",
    "            # data = b\"\".join(data)\n",
    "            # data = clientsocket.recv(4096)\n",
    "            received = pickle.loads(data)\n",
    "            print(received)\n",
    "            if isinstance(received, str):\n",
    "                filename, i = received.split(':')\n",
    "                topic_dict = pickle.load(open(filename,'rb'))['Topic Words'][int(i)]\n",
    "            else:\n",
    "                topic_dict = received\n",
    "            # print('received data: {0} Mb'.format(len(data)))\n",
    "            print('received data')\n",
    "            print('Time elapsed: {:.2f}s'.format(time.time() - start))\n",
    "            print(topic_dict)\n",
    "            pmi_dict, npmi_dict = calc_pmi_for_all_topics(topic_dict,\n",
    "                                                          ref_corpus.corpus_vocab,\n",
    "                                                          ref_corpus.inv_index,\n",
    "                                                          ref_corpus.corpus_size)\n",
    "\n",
    "            print('completed calculation')\n",
    "            print('Time elapsed: {:.2f}s'.format(time.time() - start))\n",
    "            result_dict = {'pmi_dict': pmi_dict, 'npmi_dict': npmi_dict}\n",
    "\n",
    "            res = pickle.dumps(result_dict)\n",
    "            clientsocket.send(res, )\n",
    "            # clientsocket.shutdown(socket.SHUT_RDWR)\n",
    "            clientsocket.close()\n",
    "            del clientsocket\n",
    "            print('Connection closed')\n",
    "            print('Time elapsed: {:.2f}s'.format(time.time() - start))\n",
    "        except Exception as e:\n",
    "            print('Error occured when receiving packet')\n",
    "            print(e)\n",
    "            # clientsocket.shutdown(socket.SHUT_RDWR)\n",
    "            clientsocket.close()\n",
    "            del clientsocket\n",
    "\n",
    "\n",
    "def request_pmi(topic_dict, port=1234):\n",
    "    try:\n",
    "        # create a socket object\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "        # get local machine name\n",
    "        host = socket.gethostname()\n",
    "\n",
    "        # connection to hostname on the port.\n",
    "        s.connect((host, port))\n",
    "\n",
    "        # json_str = json.dumps(topic_json)\n",
    "        # s.send(json_str.encode('ascii'))\n",
    "\n",
    "        s.send(pickle.dumps(topic_dict), )\n",
    "\n",
    "        # Receive no more than 1024 * 1024 bytes\n",
    "        # res = s.recv(1024*1024*1024)\n",
    "        data = []\n",
    "        while True:\n",
    "            packet = s.recv(4096)\n",
    "            if not packet:\n",
    "                break\n",
    "            data.append(packet)\n",
    "        res_dict = pickle.loads(b\"\".join(data))\n",
    "\n",
    "        s.close()\n",
    "        pmi_dict = res_dict['pmi_dict']\n",
    "        npmi_dict = res_dict['npmi_dict']\n",
    "    except:\n",
    "        print_error('Failed to run NPMI calc, NPMI and PMI set to 0.0')\n",
    "        pmi_dict = dict()\n",
    "        npmi_dict = dict()\n",
    "        for k in topic_dict:\n",
    "            pmi_dict[k] = 0\n",
    "            npmi_dict[k] = 0\n",
    "\n",
    "    return pmi_dict, npmi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_op.py\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mxnet import nd, autograd, gluon, io\n",
    "\n",
    "# from core import Compute\n",
    "# from utils import to_numpy, stack_numpy\n",
    "# from diff_sample import normal\n",
    "import os\n",
    "\n",
    "\n",
    "def mmd_loss(x, y, ctx_model, t=0.1, kernel='diffusion'):\n",
    "    '''\n",
    "    computes the mmd loss with information diffusion kernel\n",
    "    :param x: batch_size x latent dimension\n",
    "    :param y:\n",
    "    :param t:\n",
    "    :return:\n",
    "    '''\n",
    "    eps = 1e-6\n",
    "    n,d = x.shape\n",
    "    if kernel == 'tv':\n",
    "        sum_xx = nd.zeros(1, ctx=ctx_model)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                sum_xx = sum_xx + nd.norm(x[i] - x[j], ord=1)\n",
    "        sum_xx = sum_xx / (n * (n-1))\n",
    "\n",
    "        sum_yy = nd.zeros(1, ctx=ctx_model)\n",
    "        for i in range(y.shape[0]):\n",
    "            for j in range(i+1, y.shape[0]):\n",
    "                sum_yy = sum_yy + nd.norm(y[i] - y[j], ord=1)\n",
    "        sum_yy = sum_yy / (y.shape[0] * (y.shape[0]-1))\n",
    "\n",
    "        sum_xy = nd.zeros(1, ctx=ctx_model)\n",
    "        for i in range(n):\n",
    "            for j in range(y.shape[0]):\n",
    "                sum_xy = sum_xy + nd.norm(x[i] - y[j], ord=1)\n",
    "        sum_yy = sum_yy / (n * y.shape[0])\n",
    "    else:\n",
    "        qx = nd.sqrt(nd.clip(x, eps, 1))\n",
    "        qy = nd.sqrt(nd.clip(y, eps, 1))\n",
    "        xx = nd.dot(qx, qx, transpose_b=True)\n",
    "        yy = nd.dot(qy, qy, transpose_b=True)\n",
    "        xy = nd.dot(qx, qy, transpose_b=True)\n",
    "\n",
    "        def diffusion_kernel(a, tmpt, dim):\n",
    "            # return (4 * np.pi * tmpt)**(-dim / 2) * nd.exp(- nd.square(nd.arccos(a)) / tmpt)\n",
    "            return nd.exp(- nd.square(nd.arccos(a)) / tmpt)\n",
    "\n",
    "        off_diag = 1 - nd.eye(n, ctx=ctx_model)\n",
    "        k_xx = diffusion_kernel(nd.clip(xx, 0, 1-eps), t, d-1)\n",
    "        k_yy = diffusion_kernel(nd.clip(yy, 0, 1-eps), t, d-1)\n",
    "        k_xy = diffusion_kernel(nd.clip(xy, 0, 1-eps), t, d-1)\n",
    "        sum_xx = (k_xx * off_diag).sum() / (n * (n-1))\n",
    "        sum_yy = (k_yy * off_diag).sum() / (n * (n-1))\n",
    "        sum_xy = 2 * k_xy.sum() / (n * n)\n",
    "    return sum_xx + sum_yy - sum_xy\n",
    "\n",
    "\n",
    "class Unsupervised(Compute):\n",
    "    '''\n",
    "    Class to manage training, testing, and\n",
    "    retrieving outputs.\n",
    "    '''\n",
    "    def __init__(self, data, Enc, Dec,  Dis_y, args):\n",
    "        '''\n",
    "        Constructor.\n",
    "        Args\n",
    "        ----\n",
    "        Returns\n",
    "        -------\n",
    "        Compute object\n",
    "        '''\n",
    "        super(Unsupervised, self).__init__(data, Enc, Dec, Dis_y, args)\n",
    "\n",
    "    def unlabeled_train_op_mmd_combine(self, update_enc=True):\n",
    "        '''\n",
    "        Trains the MMD model\n",
    "        '''\n",
    "        batch_size = self.args['batch_size']\n",
    "        model_ctx = self.model_ctx\n",
    "        eps = 1e-10\n",
    "\n",
    "        # Retrieve data\n",
    "        docs = self.data.get_documents(key='train')\n",
    "\n",
    "        y_true = np.random.dirichlet(np.ones(self.ndim_y) * self.args['dirich_alpha'], size=batch_size)\n",
    "        y_true = nd.array(y_true, ctx=model_ctx)\n",
    "\n",
    "        with autograd.record():\n",
    "            ### reconstruction phase ###\n",
    "            y_onehot_u = self.Enc(docs)\n",
    "            y_onehot_u_softmax = nd.softmax(y_onehot_u)\n",
    "            if self.args['latent_noise'] > 0:\n",
    "                y_noise = np.random.dirichlet(np.ones(self.ndim_y) * self.args['dirich_alpha'], size=batch_size)\n",
    "                y_noise = nd.array(y_noise, ctx=model_ctx)\n",
    "                y_onehot_u_softmax = (1 - self.args['latent_noise']) * y_onehot_u_softmax + self.args['latent_noise'] * y_noise\n",
    "            x_reconstruction_u = self.Dec(y_onehot_u_softmax)\n",
    "\n",
    "            logits = nd.log_softmax(x_reconstruction_u)\n",
    "            loss_reconstruction = nd.mean(nd.sum(- docs * logits, axis=1))\n",
    "            loss_total = loss_reconstruction * self.args['recon_alpha']\n",
    "\n",
    "            ### mmd phase ###\n",
    "            if self.args['adverse']:\n",
    "                y_fake = self.Enc(docs)\n",
    "                y_fake = nd.softmax(y_fake)\n",
    "                loss_mmd = mmd_loss(y_true, y_fake, ctx_model=model_ctx, t=self.args['kernel_alpha'])\n",
    "                loss_total = loss_total + loss_mmd\n",
    "\n",
    "            if self.args['l2_alpha'] > 0:\n",
    "                loss_total = loss_total + self.args['l2_alpha'] * nd.mean(nd.sum(nd.square(y_onehot_u), axis=1))\n",
    "\n",
    "            loss_total.backward()\n",
    "\n",
    "        self.optimizer_enc.step(1)\n",
    "        self.optimizer_dec.step(1)  # self.m.args['batch_size']\n",
    "\n",
    "        latent_max = nd.zeros(self.args['ndim_y'], ctx=model_ctx)\n",
    "        for max_ind in nd.argmax(y_onehot_u, axis=1):\n",
    "            latent_max[max_ind] += 1.0\n",
    "        latent_max /= batch_size\n",
    "        latent_entropy = nd.mean(nd.sum(- y_onehot_u_softmax * nd.log(y_onehot_u_softmax + eps), axis=1))\n",
    "        latent_v = nd.mean(y_onehot_u_softmax, axis=0)\n",
    "        dirich_entropy = nd.mean(nd.sum(- y_true * nd.log(y_true + eps), axis=1))\n",
    "\n",
    "        if self.args['adverse']:\n",
    "            loss_mmd_return = loss_mmd.asscalar()\n",
    "        else:\n",
    "            loss_mmd_return = 0.0\n",
    "        return nd.mean(loss_reconstruction).asscalar(), loss_mmd_return, latent_max.asnumpy(), latent_entropy.asscalar(), latent_v.asnumpy(), dirich_entropy.asscalar()\n",
    "\n",
    "\n",
    "    def retrain_enc(self, l2_alpha=0.1):\n",
    "        docs = self.data.get_documents(key='train')\n",
    "        with autograd.record():\n",
    "            ### reconstruction phase ###\n",
    "            y_onehot_u = self.Enc(docs)\n",
    "            y_onehot_u_softmax = nd.softmax(y_onehot_u)\n",
    "            x_reconstruction_u = self.Dec(y_onehot_u_softmax)\n",
    "\n",
    "            logits = nd.log_softmax(x_reconstruction_u)\n",
    "            loss_reconstruction = nd.mean(nd.sum(- docs * logits, axis=1))\n",
    "            loss_reconstruction = loss_reconstruction + l2_alpha * nd.mean(nd.norm(y_onehot_u, ord=1, axis=1))\n",
    "            loss_reconstruction.backward()\n",
    "\n",
    "        self.optimizer_enc.step(1)\n",
    "        return loss_reconstruction.asscalar()\n",
    "\n",
    "\n",
    "    def unlabeled_train_op_adv_combine_add(self, update_enc=True):\n",
    "        '''\n",
    "        Trains the GAN model\n",
    "        '''\n",
    "        batch_size = self.args['batch_size']\n",
    "        model_ctx = self.model_ctx\n",
    "        eps = 1e-10\n",
    "        ##########################\n",
    "        ### unsupervised phase ###\n",
    "        ##########################\n",
    "        # Retrieve data\n",
    "        docs = self.data.get_documents(key='train')\n",
    "\n",
    "        class_true = nd.zeros(batch_size, dtype='int32', ctx=model_ctx)\n",
    "        class_fake = nd.ones(batch_size, dtype='int32', ctx=model_ctx)\n",
    "        loss_reconstruction = nd.zeros((1,), ctx=model_ctx)\n",
    "\n",
    "        ### adversarial phase ###\n",
    "        discriminator_z_confidence_true = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        discriminator_z_confidence_fake = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        discriminator_y_confidence_true = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        discriminator_y_confidence_fake = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        loss_discriminator = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "        dirich_entropy = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "\n",
    "        ### generator phase ###\n",
    "        loss_generator = nd.zeros(shape=(1,), ctx=model_ctx)\n",
    "\n",
    "        ### reconstruction phase ###\n",
    "        with autograd.record():\n",
    "            y_u = self.Enc(docs)\n",
    "            y_onehot_u_softmax = nd.softmax(y_u)\n",
    "            x_reconstruction_u = self.Dec(y_onehot_u_softmax)\n",
    "\n",
    "            logits = nd.log_softmax(x_reconstruction_u)\n",
    "            loss_reconstruction = nd.sum(- docs * logits, axis=1)\n",
    "            loss_total = loss_reconstruction * self.args['recon_alpha']\n",
    "\n",
    "            if self.args['adverse']: #and np.random.rand()<0.8:\n",
    "                y_true = np.random.dirichlet(np.ones(self.ndim_y) * self.args['dirich_alpha'], size=batch_size)\n",
    "                y_true = nd.array(y_true, ctx=model_ctx)\n",
    "                dy_true = self.Dis_y(y_true)\n",
    "                dy_fake = self.Dis_y(y_onehot_u_softmax)\n",
    "                discriminator_y_confidence_true = nd.mean(nd.softmax(dy_true)[:, 0])\n",
    "                discriminator_y_confidence_fake = nd.mean(nd.softmax(dy_fake)[:, 1])\n",
    "                softmaxCEL = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "                loss_discriminator = softmaxCEL(dy_true, class_true) + \\\n",
    "                                       softmaxCEL(dy_fake, class_fake)\n",
    "                loss_generator = softmaxCEL(dy_fake, class_true)\n",
    "                loss_total = loss_total + loss_discriminator + loss_generator\n",
    "                dirich_entropy = nd.mean(nd.sum(- y_true * nd.log(y_true + eps), axis=1))\n",
    "\n",
    "        loss_total.backward()\n",
    "\n",
    "        self.optimizer_enc.step(batch_size)\n",
    "        self.optimizer_dec.step(batch_size)\n",
    "        self.optimizer_dis_y.step(batch_size)\n",
    "\n",
    "        latent_max = nd.zeros(self.args['ndim_y'], ctx=model_ctx)\n",
    "        for max_ind in nd.argmax(y_onehot_u_softmax, axis=1):\n",
    "            latent_max[max_ind] += 1.0\n",
    "        latent_max /= batch_size\n",
    "        latent_entropy = nd.mean(nd.sum(- y_onehot_u_softmax * nd.log(y_onehot_u_softmax + eps), axis=1))\n",
    "        latent_v = nd.mean(y_onehot_u_softmax, axis=0)\n",
    "\n",
    "        return nd.mean(loss_discriminator).asscalar(), nd.mean(loss_generator).asscalar(), nd.mean(loss_reconstruction).asscalar(), \\\n",
    "               nd.mean(discriminator_z_confidence_true).asscalar(), nd.mean(discriminator_z_confidence_fake).asscalar(), \\\n",
    "               nd.mean(discriminator_y_confidence_true).asscalar(), nd.mean(discriminator_y_confidence_fake).asscalar(), \\\n",
    "               latent_max.asnumpy(), latent_entropy.asscalar(), latent_v.asnumpy(), dirich_entropy.asscalar()\n",
    "\n",
    "\n",
    "    def test_synthetic_op(self):\n",
    "        batch_size = self.args['batch_size']\n",
    "        dataset = 'train'\n",
    "        num_samps = self.data.data[dataset].shape[0]\n",
    "        batches = int(np.ceil(num_samps / batch_size))\n",
    "        batch_iter = range(batches)\n",
    "        enc_out = nd.zeros(shape=(batches * batch_size, self.ndim_y))\n",
    "        for batch in batch_iter:\n",
    "            # 1. Retrieve data\n",
    "            if self.args['data_source'] == 'Ian':\n",
    "                docs = self.data.get_documents(key=dataset)\n",
    "            # 2. Compute loss\n",
    "            y_onehot_u = self.Enc(docs)\n",
    "            y_onehot_softmax = nd.softmax(y_onehot_u)\n",
    "            enc_out[batch*batch_size:(batch+1)*batch_size, :] = y_onehot_softmax\n",
    "\n",
    "        return enc_out\n",
    "\n",
    "    def test_op(self, num_samples=None, num_epochs=None, reset=True, dataset='test'):\n",
    "        '''\n",
    "        Evaluates the model using num_samples.\n",
    "        Args\n",
    "        ----\n",
    "        num_samples: integer, default None\n",
    "          The number of samples to evaluate on. This is converted to\n",
    "          evaluating on (num_samples // batch_size) minibatches.\n",
    "        num_epochs: integer, default None\n",
    "          The number of epochs to evaluate on. This used if num_samples\n",
    "          is not specified. If neither is specified, defaults to 1 epoch.\n",
    "        reset: bool, default True\n",
    "          Whether to reset the test data index to 0 before iterating\n",
    "          through and evaluating on minibatches.\n",
    "        dataset: string, default 'test':\n",
    "          Which dataset to evaluate on: 'valid' or 'test'.\n",
    "        Returns\n",
    "        -------\n",
    "        Loss_u: float\n",
    "          The loss on the unlabeled data.\n",
    "        Loss_l: float\n",
    "          The loss on the labeled data.\n",
    "        Eval_u: list of floats\n",
    "          A list of evaluation metrics on the unlabeled data.\n",
    "        Eval_l: list of floats\n",
    "          A list of evaluation metrics on the labeled data.\n",
    "        '''\n",
    "        batch_size = self.args['batch_size']\n",
    "        model_ctx = self.model_ctx\n",
    "\n",
    "        if num_samples is None and num_epochs is None:\n",
    "            # assume full dataset evaluation\n",
    "            num_epochs = 1\n",
    "\n",
    "        if reset:\n",
    "            # Reset Data to Index Zero\n",
    "            if self.data.data[dataset] is not None:\n",
    "                self.data.force_reset_data(dataset)\n",
    "            if self.data.data[dataset + '_with_labels'] is not None:\n",
    "                self.data.force_reset_data(dataset+'_with_labels')\n",
    "\n",
    "        # Unlabeled Data\n",
    "        u_loss = 'NA'\n",
    "        u_eval = []\n",
    "        if self.data.data[dataset] is not None:\n",
    "            u_loss = 0\n",
    "            if num_samples is None:\n",
    "                num_samps = self.data.data[dataset].shape[0] * num_epochs\n",
    "            else:\n",
    "                num_samps = num_samples\n",
    "            batches = int(np.ceil(num_samps / self.args['batch_size']))\n",
    "            batch_iter = range(batches)\n",
    "            if batches > 1: batch_iter = tqdm(batch_iter, desc='unlabeled')\n",
    "            for batch in batch_iter:\n",
    "                # 1. Retrieve data\n",
    "                docs = self.data.get_documents(key=dataset)\n",
    "\n",
    "                # 2. Compute loss\n",
    "                y_u = self.Enc(docs)\n",
    "                y_onehot_u_softmax = nd.softmax(y_u)\n",
    "                x_reconstruction_u = self.Dec(y_onehot_u_softmax)\n",
    "\n",
    "                logits = nd.log_softmax(x_reconstruction_u)\n",
    "                loss_recon_unlabel = nd.sum(- docs * logits, axis=1)\n",
    "\n",
    "                # 3. Convert to numpy\n",
    "                u_loss += nd.mean(loss_recon_unlabel).asscalar()\n",
    "            u_loss /= batches\n",
    "\n",
    "        # Labeled Data\n",
    "        l_loss = 0.0\n",
    "        l_acc = 0.0\n",
    "        if self.data.data[dataset+'_with_labels'] is not None:\n",
    "            l_loss = 0\n",
    "            if num_samples is None:\n",
    "                num_samps = self.data.data[dataset+'_with_labels'].shape[0] * num_epochs\n",
    "            else:\n",
    "                num_samps = num_samples\n",
    "            batches = int(np.ceil(num_samps / self.args['batch_size']))\n",
    "            batch_iter = range(batches)\n",
    "            if batches > 1: batch_iter = tqdm(batch_iter, desc='labeled')\n",
    "            softmaxCEL = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "            for batch in batch_iter:\n",
    "                # 1. Retrieve data\n",
    "                labeled_docs, labels = self.data.get_documents(key=dataset+'_with_labels', split_on=self.data.data_dim)\n",
    "                # 2. Compute loss\n",
    "                y_u = self.Enc(docs)\n",
    "                y_onehot_u_softmax = nd.softmax(y_u)\n",
    "                class_pred = nd.argmax(y_onehot_u_softmax, axis=1)\n",
    "                l_a = labels[list(range(labels.shape[0])), class_pred]\n",
    "                l_acc += nd.mean(l_a).asscalar()\n",
    "                labels = labels / nd.sum(labels, axis=1, keepdims=True)\n",
    "                l_l = softmaxCEL(y_onehot_u_softmax, labels)\n",
    "\n",
    "                # 3. Convert to numpy\n",
    "                l_loss += nd.mean(l_l).asscalar()\n",
    "            l_loss /= batches\n",
    "            l_acc /= batches\n",
    "\n",
    "        return u_loss, l_loss, l_acc\n",
    "\n",
    "\n",
    "    def save_latent(self, saveto):\n",
    "        before_softmax = True\n",
    "        try:\n",
    "            if type(self.data.data['train']) is np.ndarray:\n",
    "                dataset_train = gluon.data.dataset.ArrayDataset(self.data.data['train'])\n",
    "                train_data = gluon.data.DataLoader(dataset_train, self.args['batch_size'], shuffle=False, last_batch='discard')\n",
    "\n",
    "                dataset_val = gluon.data.dataset.ArrayDataset(self.data.data['valid'])\n",
    "                val_data = gluon.data.DataLoader(dataset_val, self.args['batch_size'], shuffle=False, last_batch='discard')\n",
    "\n",
    "                dataset_test = gluon.data.dataset.ArrayDataset(self.data.data['test'])\n",
    "                test_data = gluon.data.DataLoader(dataset_test, self.args['batch_size'], shuffle=False, last_batch='discard')\n",
    "            else:\n",
    "                train_data = io.NDArrayIter(data={'data': self.data.data['train']}, batch_size=self.args['batch_size'],\n",
    "                                            shuffle=False, last_batch_handle='discard')\n",
    "                val_data = io.NDArrayIter(data={'data': self.data.data['valid']}, batch_size=self.args['batch_size'],\n",
    "                                            shuffle=False, last_batch_handle='discard')\n",
    "                test_data = io.NDArrayIter(data={'data': self.data.data['test']}, batch_size=self.args['batch_size'],\n",
    "                                            shuffle=False, last_batch_handle='discard')\n",
    "        except:\n",
    "            print(\"Loading error during save_latent. Probably caused by not having validation or test set!\")\n",
    "            return\n",
    "\n",
    "        train_output = np.zeros((self.data.data['train'].shape[0], self.ndim_y))\n",
    "        # train_label_output = np.zeros(self.data.data['train'].shape[0])\n",
    "        # for i, (data, label) in enumerate(train_data):\n",
    "        for i, data in enumerate(train_data):\n",
    "            if type(data) is io.DataBatch:\n",
    "                data = data.data[0].as_in_context(self.model_ctx)\n",
    "            else:\n",
    "                data = data.as_in_context(self.model_ctx)\n",
    "            if before_softmax:\n",
    "                output = self.Enc(data)\n",
    "            else:\n",
    "                output = nd.softmax(self.Enc(data))\n",
    "            train_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = output.asnumpy()\n",
    "            # train_label_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = label.asnumpy()\n",
    "        train_output = np.delete(train_output, np.s_[(i+1)*self.args['batch_size']:], 0)\n",
    "        # train_label_output = np.delete(train_label_output, np.s_[(i+1)*self.args['batch_size']:])\n",
    "        np.save(os.path.join(saveto, self.args['domain']+'train_latent.npy'), train_output)\n",
    "        # np.save(os.path.join(saveto, self.args['domain']+'train_latent_label.npy'), train_label_output)\n",
    "\n",
    "        val_output = np.zeros((self.data.data['valid'].shape[0], self.ndim_y))\n",
    "        # train_label_output = np.zeros(self.data.data['train'].shape[0])\n",
    "        # for i, (data, label) in enumerate(train_data):\n",
    "        for i, data in enumerate(val_data):\n",
    "            if type(data) is io.DataBatch:\n",
    "                data = data.data[0].as_in_context(self.model_ctx)\n",
    "            else:\n",
    "                data = data.as_in_context(self.model_ctx)\n",
    "            if before_softmax:\n",
    "                output = self.Enc(data)\n",
    "            else:\n",
    "                output = nd.softmax(self.Enc(data))\n",
    "            val_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = output.asnumpy()\n",
    "            # train_label_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = label.asnumpy()\n",
    "        val_output = np.delete(val_output, np.s_[(i+1)*self.args['batch_size']:], 0)\n",
    "        # train_label_output = np.delete(train_label_output, np.s_[(i+1)*self.args['batch_size']:])\n",
    "        np.save(os.path.join(saveto, self.args['domain']+'val_latent.npy'), val_output)\n",
    "        # np.save(os.path.join(saveto, self.args['domain']+'train_latent_label.npy'), train_label_output)\n",
    "\n",
    "        test_output = np.zeros((self.data.data['test'].shape[0], self.ndim_y))\n",
    "        # test_label_output = np.zeros(self.data.data['test'].shape[0])\n",
    "        # for i, (data, label) in enumerate(test_data):\n",
    "        for i, data in enumerate(test_data):\n",
    "            if type(data) is io.DataBatch:\n",
    "                data = data.data[0].as_in_context(self.model_ctx)\n",
    "            else:\n",
    "                data = data.as_in_context(self.model_ctx)\n",
    "            if before_softmax:\n",
    "                output = self.Enc(data)\n",
    "            else:\n",
    "                output = nd.softmax(self.Enc(data))\n",
    "            test_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = output.asnumpy()\n",
    "            # test_label_output[i*self.args['batch_size']:(i+1)*self.args['batch_size']] = label.asnumpy()\n",
    "        test_output = np.delete(test_output, np.s_[(i+1)*self.args['batch_size']:], 0)\n",
    "        # test_label_output = np.delete(test_label_output, np.s_[(i+1)*self.args['batch_size']:])\n",
    "        np.save(os.path.join(saveto, self.args['domain']+'test_latent.npy'), test_output)\n",
    "        # np.save(os.path.join(saveto, self.args['domain']+'test_latent_label.npy'), test_label_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.py\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import mxnet as mx\n",
    "mx.random.seed(int(time.time()))\n",
    "\n",
    "# from utils import gpu_helper, gpu_exists, calc_topic_uniqueness, get_topic_words_decoder_weights, request_pmi, print_topic_with_scores, print_topics\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Training WAE in MXNet')\n",
    "    parser.add_argument('-dom','--domain', type=str, default='twenty_news', help='domain to run', required=False)\n",
    "    parser.add_argument('-data','--data_path', type=str, default='', help='file path for dataset', required=False)\n",
    "    parser.add_argument('-max_labels','--max_labels', type=int, default=100, help='max number of topics to specify as labels for a single training document', required=False)\n",
    "    parser.add_argument('-max_labeled_samples','--max_labeled_samples', type=int, default=10, help='max number of labeled samples per topic', required=False)\n",
    "    parser.add_argument('-label_seed','--label_seed', type=lambda x: int(x) if x != 'None' else None, default=None, help='random seed for subsampling the labeled dataset', required=False)\n",
    "    parser.add_argument('-mod','--model', type=str, default='dirichlet', help='model to use', required=False)\n",
    "    parser.add_argument('-desc','--description', type=str, default='', help='description for the experiment', required=False)\n",
    "    parser.add_argument('-alg','--algorithm', type=str, default='standard', help='algorithm to use for training: standard', required=False)\n",
    "    parser.add_argument('-bs','--batch_size', type=int, default=256, help='batch_size for training', required=False)\n",
    "    parser.add_argument('-opt','--optim', type=str, default='Adam', help='encoder training algorithm', required=False)\n",
    "    parser.add_argument('-lr','--learning_rate', type=float, default=1e-4, help='learning rate', required=False)\n",
    "    parser.add_argument('-l2','--weight_decay', type=float, default=0., help='weight decay', required=False)\n",
    "    parser.add_argument('-e_nh','--enc_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for encoder or list of hiddens for each layer', required=False)\n",
    "    parser.add_argument('-e_nl','--enc_n_layer', type=int, default=1, help='# of hidden layers for encoder, set to -1 if passing list of n_hiddens', required=False)\n",
    "    parser.add_argument('-e_nonlin','--enc_nonlinearity', type=str, default='sigmoid', help='type of nonlinearity for encoder', required=False)\n",
    "    parser.add_argument('-e_weights','--enc_weights', type=str, default='', help='file path for encoder weights', required=False)\n",
    "    parser.add_argument('-e_freeze','--enc_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the encoder weights', required=False)\n",
    "    parser.add_argument('-lat_nonlin','--latent_nonlinearity', type=str, default='', help='type of to use prior to decoder', required=False)\n",
    "    parser.add_argument('-d_nh','--dec_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for decoder or list of hiddens for each layer', required=False)\n",
    "    parser.add_argument('-d_nl','--dec_n_layer', type=int, default=0, help='# of hidden layers for decoder', required=False)\n",
    "    parser.add_argument('-d_nonlin','--dec_nonlinearity', type=str, default='', help='type of nonlinearity for decoder', required=False)\n",
    "    parser.add_argument('-d_weights','--dec_weights', type=str, default='', help='file path for decoder weights', required=False)\n",
    "    parser.add_argument('-d_freeze','--dec_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the decoder weights', required=False)\n",
    "    parser.add_argument('-d_word_dist','--dec_word_dist', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to init decoder weights with training set word distributions', required=False)\n",
    "    parser.add_argument('-dis_nh','--dis_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for encoder or list of hiddens for each layer', required=False)\n",
    "    parser.add_argument('-dis_nl','--dis_n_layer', type=int, default=1, help='# of hidden layers for encoder, set to -1 if passing list of n_hiddens', required=False)\n",
    "    parser.add_argument('-dis_nonlin','--dis_nonlinearity', type=str, default='sigmoid', help='type of nonlinearity for discriminator', required=False)\n",
    "    parser.add_argument('-dis_y_weights','--dis_y_weights', type=str, default='', help='file path for discriminator_y weights', required=False)\n",
    "    parser.add_argument('-dis_z_weights','--dis_z_weights', type=str, default='', help='file path for discriminator_z weights', required=False)\n",
    "    parser.add_argument('-dis_freeze','--dis_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the encoder weights', required=False)\n",
    "    parser.add_argument('-include_w','--include_weights', type=str, nargs='*', default=[], help='weights to train on (default is all weights) -- all others are kept fixed; Ex: E.z_encoder D.decoder', required=False)\n",
    "    parser.add_argument('-eps','--epsilon', type=float, default=1e-8, help='epsilon param for Adam', required=False)\n",
    "    parser.add_argument('-mx_it','--max_iter', type=int, default=50001, help='max # of training iterations', required=False)\n",
    "    parser.add_argument('-train_stats_every','--train_stats_every', type=int, default=100, help='skip train_stats_every iterations between recording training stats', required=False)\n",
    "    parser.add_argument('-eval_stats_every','--eval_stats_every', type=int, default=100, help='skip eval_stats_every iterations between recording evaluation stats', required=False)\n",
    "    parser.add_argument('-ndim_y','--ndim_y', type=int, default=256, help='dimensionality of y - topic indicator', required=False)\n",
    "    parser.add_argument('-ndim_x','--ndim_x', type=int, default=2, help='dimensionality of p(x) - data distribution', required=False)\n",
    "    parser.add_argument('-saveto','--saveto', type=str, default='', help='path prefix for saving results', required=False)\n",
    "    parser.add_argument('-gpu','--gpu', type=int, default=-2, help='if/which gpu to use (-1: all, -2: None)', required=False)\n",
    "    parser.add_argument('-hybrid','--hybridize', type=lambda x: (str(x).lower() == 'true'), default=False, help='declaritive True (hybridize) or imperative False', required=False)\n",
    "    parser.add_argument('-full_npmi','--full_npmi', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to compute NPMI for full trajectory', required=False)\n",
    "    parser.add_argument('-eot','--eval_on_test', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to evaluate on the test set (True) or validation set (False)', required=False)\n",
    "    parser.add_argument('-verb','--verbose', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to print progress to stdout', required=False)\n",
    "    parser.add_argument('-dirich_alpha','--dirich_alpha', type=float, default=1e-1, help='param for Dirichlet prior', required=False)\n",
    "    parser.add_argument('-adverse','--adverse', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to turn on adverserial training (MMD or GAN). set to False if only train auto-encoder', required=False)\n",
    "    parser.add_argument('-update_enc','--update_enc', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to update encoder for unlabed_train_op()', required=False)\n",
    "    parser.add_argument('-labeled_loss_lambda','--labeled_loss_lambda', type=float, default=1.0, help='param for Dirichlet noise for label', required=False)\n",
    "    parser.add_argument('-train_mode','--train_mode', type=str, default='mmd', help=\"set to mmd or adv (for GAN)\", required=False)\n",
    "    parser.add_argument('-kernel_alpha','--kernel_alpha', type=float, default=1.0, help='param for information diffusion kernel', required=False)\n",
    "    parser.add_argument('-recon_alpha','--recon_alpha', type=float, default=-1.0, help='multiplier of the reconstruction loss when combined with mmd loss', required=False)\n",
    "    parser.add_argument('-recon_alpha_adapt','--recon_alpha_adapt', type=float, default=-1.0, help='adaptively change recon_alpha so that [total loss = mmd + recon_alpha_adapt * recon loss], set to -1 if no adapt', required=False)\n",
    "    parser.add_argument('-dropout_p','--dropout_p', type=float, default=-1.0, help='dropout probability in encoder', required=False)\n",
    "    parser.add_argument('-l2_alpha','--l2_alpha', type=float, default=-1.0, help='alpha multipler for L2 regularization on latent vector', required=False)\n",
    "    parser.add_argument('-latent_noise','--latent_noise', type=float, default=0.0, help='proportion of dirichlet noise added to the latent vector after softmax', required=False)\n",
    "    parser.add_argument('-topic_decoder_weight','--topic_decoder_weight', type=lambda x: (str(x).lower() == 'true'), default=False, help='extract topic words based on decoder weights or decoder outputs', required=False)\n",
    "    parser.add_argument('-retrain_enc_only','--retrain_enc_only', type=lambda x: (str(x).lower() == 'true'), default=False, help='only retrain the encoder for reconstruction loss', required=False)\n",
    "    parser.add_argument('-l2_alpha_retrain','--l2_alpha_retrain', type=float, default=0.1, help='alpha multipler for L2 regularization on encoder output during retraining', required=False)\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "\n",
    "    if args['domain'] == 'twenty_news_sklearn':\n",
    "        from examples.domains.twenty_news_sklearn_wae import TwentyNews as Domain\n",
    "    elif args['domain'] == 'wikitext-103':\n",
    "        from examples.domains.wikitext103_wae import Wikitext103 as Domain\n",
    "    elif args['domain'] == 'nytimes-pbr':\n",
    "        from examples.domains.nyt_wae import Nytimes as Domain\n",
    "    elif args['domain'] == 'ag_news_csv':\n",
    "        from examples.domains.ag_news_wae import Agnews as Domain\n",
    "    elif args['domain'] == 'dbpedia_csv':\n",
    "        from examples.domains.dbpedia_wae import Dbpedia as Domain\n",
    "    elif args['domain'] == 'yelp_review_polarity_csv':\n",
    "        from examples.domains.yelp_polarity_wae import YelpPolarity as Domain\n",
    "    elif args['domain'] == 'lda_synthetic':\n",
    "        from examples.domains.lda_synthetic import LdaSynthetic as Domain\n",
    "    else:\n",
    "        raise NotImplementedError(args['domain'])\n",
    "\n",
    "    if args['model'] == 'dirichlet':\n",
    "#         from models.dirichlet import Encoder, Decoder, Discriminator_y\n",
    "        from dirichlet import Encoder, Decoder, Discriminator_y\n",
    "    else:\n",
    "        raise NotImplementedError(args['model'])\n",
    "\n",
    "    from compute_op import Unsupervised as Compute\n",
    "\n",
    "    assert args['latent_noise'] >= 0 and args['latent_noise'] <= 1\n",
    "    if args['description'] == '':\n",
    "        args['description'] = args['domain'] + '-' + args['algorithm'] + '-' + args['model']\n",
    "        if args['un_label_coeffs'][0] > 0 and args['un_label_coeffs'][1] == 0:\n",
    "            args['description'] += '-unsup'\n",
    "        elif args['un_label_coeffs'][0] > 0 and args['un_label_coeffs'][1] > 0:\n",
    "            args['description'] += '-semisup'\n",
    "        else:\n",
    "            args['description'] += '-sup'\n",
    "    elif args['description'].isdigit():\n",
    "        args['description'] = args['domain'] + '-' + args['algorithm'] + '-' + args['model'] + '-' + args['description']\n",
    "\n",
    "    if args['saveto'] == '':\n",
    "        args['saveto'] = 'examples/results/' + args['description'].replace('-','/')\n",
    "\n",
    "    saveto = args['saveto'] + '/' + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S/{}').format('')\n",
    "    if not os.path.exists(saveto):\n",
    "        os.makedirs(saveto)\n",
    "        os.makedirs(saveto + '/weights/encoder')\n",
    "        os.makedirs(saveto + '/weights/decoder')\n",
    "        os.makedirs(saveto + '/weights/discriminator_y')\n",
    "        os.makedirs(saveto + '/weights/discriminator_z')\n",
    "    shutil.copy(os.path.realpath('compute_op.py'), os.path.join(saveto, 'compute_op.py'))\n",
    "    shutil.copy(os.path.realpath('core.py'), os.path.join(saveto, 'core.py'))\n",
    "    shutil.copy(os.path.realpath('run.py'), os.path.join(saveto, 'run.py'))\n",
    "    shutil.copy(os.path.realpath('utils.py'), os.path.join(saveto, 'utils.py'))\n",
    "\n",
    "    # domain_file = args['domain']+'.py'\n",
    "    # shutil.copy(os.path.realpath('examples/domains/'+domain_file), os.path.join(saveto, domain_file))\n",
    "    model_file = args['model']+'.py'\n",
    "    shutil.copy(os.path.realpath('models/'+model_file), os.path.join(saveto, model_file))\n",
    "    args['saveto'] = saveto\n",
    "    with open(saveto+'args.txt', 'w') as file:\n",
    "        for key, val in args.items():\n",
    "            if val != '':\n",
    "                if isinstance(val, list) or isinstance(val, tuple):\n",
    "                    val = [str(v) for v in val]\n",
    "                    file.write('--'+str(key)+' '+' '.join(val)+'\\n')\n",
    "                else:\n",
    "                    file.write('--'+str(key)+' '+str(val)+'\\n')\n",
    "\n",
    "    if args['gpu'] >= 0 and gpu_exists(args['gpu']):\n",
    "        args['description'] += ' (gpu'+str(args['gpu'])+')'\n",
    "    else:\n",
    "        args['description'] += ' (cpu)'\n",
    "\n",
    "    pickle.dump(args, open(args['saveto']+'args.p','wb'))\n",
    "\n",
    "    return Compute, Domain, Encoder, Decoder, Discriminator_y, args\n",
    "\n",
    "\n",
    "def run_experiment(Compute, Domain, Encoder, Decoder, Discriminator_y, args):\n",
    "    print('\\nSaving to: '+args['saveto'])\n",
    "\n",
    "    model_ctx = gpu_helper(args['gpu'])\n",
    "\n",
    "    data = Domain(batch_size=args['batch_size'], data_path=args['data_path'], ctx=model_ctx, saveto=args['saveto'])\n",
    "    print('train dimension = ', data.data['train'].shape)\n",
    "    if type(data.data['train']) is np.ndarray:\n",
    "        mean_length = np.mean(np.sum(data.data['train'], axis=1))\n",
    "    else:\n",
    "        mean_length = mx.nd.mean(mx.nd.sum(data.data['train'], axis=1)).asscalar()\n",
    "    vocab_size = data.data['train'].shape[1]\n",
    "    if data.data['train_with_labels'] is not None:\n",
    "        print('train_with_labels dimension = ', data.data['train_with_labels'].shape)\n",
    "\n",
    "    if args['recon_alpha'] < 0:\n",
    "        args['recon_alpha'] = 1.0 / (mean_length * np.log(vocab_size))\n",
    "    print('Setting recon_alpha to {}'.format(args['recon_alpha']))\n",
    "\n",
    "    Enc = Encoder(model_ctx=model_ctx, batch_size=args['batch_size'], input_dim=args['ndim_x'], ndim_y=args['ndim_y'],\n",
    "                  n_hidden=args['enc_n_hidden'], n_layers=args['enc_n_layer'], nonlin=args['enc_nonlinearity'],\n",
    "                  weights_file=args['enc_weights'], freeze=args['enc_freeze'], latent_nonlin=args['latent_nonlinearity'])\n",
    "    Dec = Decoder(model_ctx=model_ctx, batch_size=args['batch_size'], output_dim=args['ndim_x'], ndim_y=args['ndim_y'],\n",
    "                  n_hidden=args['dec_n_hidden'], n_layers=args['dec_n_layer'], nonlin=args['dec_nonlinearity'],\n",
    "                  weights_file=args['dec_weights'], freeze=args['dec_freeze'], latent_nonlin=args['latent_nonlinearity'])\n",
    "    Dis_y = Discriminator_y(model_ctx=model_ctx, batch_size=args['batch_size'], ndim_y=args['ndim_y'],\n",
    "                            n_hidden=args['dis_n_hidden'], n_layers=args['dis_n_layer'],\n",
    "                            nonlin=args['dis_nonlinearity'], weights_file=args['dis_y_weights'],\n",
    "                            freeze=args['dis_freeze'], latent_nonlin=args['latent_nonlinearity'])\n",
    "    if args['enc_weights']:\n",
    "        Enc.load_parameters(args['enc_weights'], ctx=model_ctx)\n",
    "    else:\n",
    "        Enc.init_weights()\n",
    "    if args['dec_weights']:\n",
    "        Dec.load_parameters(args['dec_weights'], ctx=model_ctx)\n",
    "    else:\n",
    "        Dec.init_weights()\n",
    "    Dis_y.init_weights()\n",
    "    # load pre-trained document classifier\n",
    "    if args['hybridize']:\n",
    "        print('NOTE: Hybridizing Encoder and Decoder (Declaritive mode).')\n",
    "        Enc.hybridize()\n",
    "        Dec.hybridize()\n",
    "        Dis_y.hybridize()\n",
    "    else:\n",
    "        print('NOTE: Not Hybridizing Encoder and Decoder (Imperative mode).')\n",
    "\n",
    "    compute = Compute(data, Enc, Dec,  Dis_y, args)\n",
    "\n",
    "    N_train = data.data['train'].shape[0]\n",
    "\n",
    "    epochs = range(args['max_iter'])\n",
    "    if args['verbose']:\n",
    "        print(' ')\n",
    "        epochs = tqdm(epochs, desc=args['description'])\n",
    "\n",
    "    train_record = {'loss_discriminator':[], 'loss_generator':[], 'loss_reconstruction':[], 'latent_max_distr':[],\n",
    "                    'latent_avg_entropy':[], 'latent_avg':[], 'dirich_avg_entropy':[], 'loss_labeled':[]}\n",
    "    eval_record = {'NPMI':[], 'Topic Uniqueness':[], 'Top Words':[],\n",
    "                   'NPMI2':[], 'Topic Uniqueness2':[], 'Top Words2':[],\n",
    "                   'u_loss_train':[], 'l_loss_train':[],\n",
    "                   'u_loss_val':[], 'l_loss_val':[],\n",
    "                   'u_loss_test':[], 'l_loss_test':[],\n",
    "                   'l_acc_train':[], 'l_acc_val':[], 'l_acc_test':[]}\n",
    "\n",
    "    total_iterations_train = N_train // args['batch_size']\n",
    "    training_start_time = time.time()\n",
    "    i = 0\n",
    "    if args['retrain_enc_only']:\n",
    "        print('Retraining encoder ONLY!')\n",
    "        for i in epochs:\n",
    "            sum_loss_autoencoder = 0.0\n",
    "            epoch_start_time = time.time()\n",
    "            for itr in range(total_iterations_train):\n",
    "                loss_reconstruction = compute.retrain_enc(args['l2_alpha_retrain'])\n",
    "                sum_loss_autoencoder += loss_reconstruction\n",
    "            if args['verbose']:\n",
    "                # epochs.set_postfix({'L_Dis': loss_discriminator, 'L_Gen': loss_generator, 'L_Recon': loss_reconstruction})\n",
    "                print(\"Epoch {} done in {} sec - loss: a={:.5g} - total {} min\".format(\n",
    "                    i + 1, int(time.time() - epoch_start_time),\n",
    "                    sum_loss_autoencoder / total_iterations_train,\n",
    "                    int((time.time() - training_start_time) // 60)))\n",
    "    else:\n",
    "        for i in epochs:\n",
    "            sum_loss_generator = 0.0\n",
    "            sum_loss_discriminator = 0.0\n",
    "            sum_loss_autoencoder = 0.0\n",
    "            sum_discriminator_z_confidence_true = 0.0\n",
    "            sum_discriminator_z_confidence_fake = 0.0\n",
    "            sum_discriminator_y_confidence_true = 0.0\n",
    "            sum_discriminator_y_confidence_fake = 0.0\n",
    "            sum_loss_labeled = 0.0\n",
    "\n",
    "            latent_max_distr = np.zeros(args['ndim_y'])\n",
    "            latent_entropy_avg = 0.0\n",
    "            latent_v_avg = np.zeros(args['ndim_y'])\n",
    "            dirich_avg_entropy = 0.0\n",
    "\n",
    "            epoch_start_time = time.time()\n",
    "            for itr in range(total_iterations_train):\n",
    "                if args['train_mode'] == 'mmd':\n",
    "                    loss_reconstruction, loss_discriminator, latent_max, latent_entropy, latent_v, dirich_entropy = \\\n",
    "                        compute.unlabeled_train_op_mmd_combine(update_enc=args['update_enc'])\n",
    "                    loss_generator, \\\n",
    "                    discriminator_z_confidence_true, discriminator_z_confidence_fake, \\\n",
    "                    discriminator_y_confidence_true, discriminator_y_confidence_fake = 0,0,0,0,0\n",
    "                elif args['train_mode'] == 'adv':\n",
    "                    loss_discriminator, loss_generator, loss_reconstruction, \\\n",
    "                    discriminator_z_confidence_true, discriminator_z_confidence_fake, \\\n",
    "                    discriminator_y_confidence_true, discriminator_y_confidence_fake, \\\n",
    "                    latent_max, latent_entropy, latent_v, dirich_entropy = \\\n",
    "                        compute.unlabeled_train_op_adv_combine_add(update_enc=args['update_enc'])\n",
    "\n",
    "                sum_loss_discriminator += loss_discriminator\n",
    "                sum_loss_generator += loss_generator\n",
    "                sum_loss_autoencoder += loss_reconstruction\n",
    "                sum_discriminator_z_confidence_true += discriminator_z_confidence_true\n",
    "                sum_discriminator_z_confidence_fake += discriminator_z_confidence_fake\n",
    "                sum_discriminator_y_confidence_true += discriminator_y_confidence_true\n",
    "                sum_discriminator_y_confidence_fake += discriminator_y_confidence_fake\n",
    "\n",
    "                latent_max_distr += latent_max\n",
    "                latent_entropy_avg += latent_entropy\n",
    "                latent_v_avg += latent_v\n",
    "                dirich_avg_entropy += dirich_entropy\n",
    "\n",
    "            train_record['loss_discriminator'].append(sum_loss_discriminator / total_iterations_train)\n",
    "            train_record['loss_generator'].append(sum_loss_generator / total_iterations_train)\n",
    "            train_record['loss_reconstruction'].append(sum_loss_autoencoder / total_iterations_train)\n",
    "            train_record['latent_max_distr'].append(latent_max_distr / total_iterations_train)\n",
    "            train_record['latent_avg_entropy'].append(latent_entropy_avg / total_iterations_train)\n",
    "            train_record['latent_avg'].append(latent_v_avg / total_iterations_train)\n",
    "            train_record['dirich_avg_entropy'].append(dirich_avg_entropy / total_iterations_train)\n",
    "            train_record['loss_labeled'].append(sum_loss_labeled / total_iterations_train)\n",
    "            if args['verbose']:\n",
    "                # epochs.set_postfix({'L_Dis': loss_discriminator, 'L_Gen': loss_generator, 'L_Recon': loss_reconstruction})\n",
    "                print(\"Epoch {} done in {} sec - loss: g={:.5g}, d={:.5g}, a={:.5g}, label={:.5g} - disc_z: true={:.1f}%, fake={:.1f}% - disc_y: true={:.1f}%, fake={:.1f}% - total {} min\".format(\n",
    "                    i + 1, int(time.time() - epoch_start_time),\n",
    "                    sum_loss_generator / total_iterations_train,\n",
    "                    sum_loss_discriminator / total_iterations_train,\n",
    "                    sum_loss_autoencoder / total_iterations_train,\n",
    "                    sum_loss_labeled / total_iterations_train,\n",
    "                    sum_discriminator_z_confidence_true / total_iterations_train * 100,\n",
    "                    sum_discriminator_z_confidence_fake / total_iterations_train * 100,\n",
    "                    sum_discriminator_y_confidence_true / total_iterations_train * 100,\n",
    "                    sum_discriminator_y_confidence_fake / total_iterations_train * 100,\n",
    "                    int((time.time() - training_start_time) // 60)))\n",
    "                print('Latent avg entropy = {}, dirich_entropy={}'.format(\n",
    "                    train_record['latent_avg_entropy'][-1], train_record['dirich_avg_entropy'][-1]))\n",
    "                # print(train_record['latent_avg'][-1])\n",
    "            if i == (args['max_iter'] - 1) or (args['eval_stats_every'] > 0 and i % args['eval_stats_every'] == 0):\n",
    "                if args['recon_alpha_adapt'] > 0 and i == 0:\n",
    "                    compute.args['recon_alpha'] = train_record['loss_discriminator'][-1] / \\\n",
    "                                                  train_record['loss_reconstruction'][-1]\n",
    "                    compute.args['recon_alpha'] = abs(compute.args['recon_alpha']) * args['recon_alpha_adapt']\n",
    "                    print(\"recon_alpha adjusted to {}\".format(compute.args['recon_alpha']))\n",
    "\n",
    "                if args['domain'] == 'synthetic':\n",
    "                    enc_out = compute.test_synthetic_op()\n",
    "                    np.save(os.path.join(args['saveto'], \"enc_out_epoch{}\".format(i)), enc_out.asnumpy())\n",
    "                else:\n",
    "                    # extract topic words from decoder output:\n",
    "                    topic_words = get_topic_words_decoder_weights(Dec, data, model_ctx, decoder_weights=False)\n",
    "                    topic_uniqs = calc_topic_uniqueness(topic_words)\n",
    "                    eval_record['Topic Uniqueness'].append(np.mean(list(topic_uniqs.values())))\n",
    "                    topic_json = dict()\n",
    "                    for tp in range(len(topic_words)):\n",
    "                        topic_json[tp] = topic_words[tp]\n",
    "                    pmi_dict, npmi_dict = request_pmi(topic_dict=topic_json, port=1234)\n",
    "                    eval_record['NPMI'].append(np.mean(list(npmi_dict.values())))\n",
    "                    print(\"Topic Eval (decoder output): Uniq={:.5g}, NPMI={:.5g}\".format(\n",
    "                        eval_record['Topic Uniqueness'][-1], eval_record['NPMI'][-1]))\n",
    "                    eval_record['Top Words'].append(topic_json)\n",
    "                    print_topics(topic_json, npmi_dict, topic_uniqs, data)\n",
    "\n",
    "                    # extract topic words from decoder weight matrix:\n",
    "                    topic_words = get_topic_words_decoder_weights(Dec, data, model_ctx, decoder_weights=True)\n",
    "                    topic_uniqs = calc_topic_uniqueness(topic_words)\n",
    "                    eval_record['Topic Uniqueness2'].append(np.mean(list(topic_uniqs.values())))\n",
    "                    topic_json = dict()\n",
    "                    for tp in range(len(topic_words)):\n",
    "                        topic_json[tp] = topic_words[tp]\n",
    "                    pmi_dict, npmi_dict = request_pmi(topic_dict=topic_json, port=1234)\n",
    "                    eval_record['NPMI2'].append(np.mean(list(npmi_dict.values())))\n",
    "                    print(\"Topic Eval (decoder weight): Uniq={:.5g}, NPMI={:.5g}\".format(\n",
    "                        eval_record['Topic Uniqueness2'][-1], eval_record['NPMI2'][-1]))\n",
    "                    eval_record['Top Words2'].append(topic_json)\n",
    "                    print_topics(topic_json, npmi_dict, topic_uniqs, data)\n",
    "\n",
    "                    # evaluate train, validate and test losses for w/ w/o labels:\n",
    "                    u_loss_train, l_loss_train, u_loss_val, l_loss_val, u_loss_test, l_loss_test, l_acc_train, \\\n",
    "                    l_acc_val, l_acc_test = 0,0,0,0,0,0,0,0,0\n",
    "                    u_loss_train, l_loss_train, l_acc_train = compute.test_op(dataset='train')\n",
    "                    eval_record['u_loss_train'].append(u_loss_train)\n",
    "                    eval_record['l_loss_train'].append(l_loss_train)\n",
    "                    eval_record['l_acc_train'].append(l_acc_train)\n",
    "                    if data.data['valid'] is not None:\n",
    "                        u_loss_val, l_loss_val, l_acc_val = compute.test_op(dataset='valid')\n",
    "                        eval_record['u_loss_val'].append(u_loss_val)\n",
    "                        eval_record['l_loss_val'].append(l_loss_val)\n",
    "                        eval_record['l_acc_val'].append(l_acc_val)\n",
    "                    if data.data['test'] is not None:\n",
    "                        u_loss_test, l_loss_test, l_acc_test = compute.test_op(dataset='test')\n",
    "                        eval_record['u_loss_test'].append(u_loss_test)\n",
    "                        eval_record['l_loss_test'].append(l_loss_test)\n",
    "                        eval_record['l_acc_test'].append(l_acc_test)\n",
    "                    print(\"Train loss u-l-acc: {:.5g}-{:.5g}-{:.5g}, Val: {:.5g}-{:.5g}-{:.5g}, Test: {:.5g}-{:.5g}-{:.5g}\".format(\n",
    "                        u_loss_train, l_loss_train, l_acc_train, u_loss_val, l_loss_val, l_acc_val, u_loss_test, l_loss_test, l_acc_test))\n",
    "\n",
    "                    pickle.dump(train_record,open(args['saveto']+'train_record.p','wb'))\n",
    "                    pickle.dump(eval_record,open(args['saveto']+'eval_record.p','wb'))\n",
    "\n",
    "    # save final weights\n",
    "    Enc.save_parameters(args['saveto']+'weights/encoder/Enc_'+str(i))\n",
    "    Dec.save_parameters(args['saveto']+'weights/decoder/Dec_'+str(i))\n",
    "    Dis_y.save_parameters(args['saveto']+'weights/discriminator_y/Dis_y_'+str(i))\n",
    "\n",
    "    # save the latent features\n",
    "    compute.save_latent(args['saveto'])\n",
    "\n",
    "    if args['domain'] == 'lda_synthetic':\n",
    "        # save the decoder weight matrix\n",
    "        params = Dec.collect_params()\n",
    "        params = params['decoder0_dense0_weight'].data().transpose()\n",
    "        np.save(args['saveto']+'decoder_weight.npy', params.asnumpy())\n",
    "\n",
    "    # print_topic_with_scores(eval_record['Top Words'][-1])\n",
    "    print('Done! ' + args['description'])\n",
    "    print('\\nSaved to: '+args['saveto'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.dirichlet.py\n",
    "import numpy as np\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "# from core import ENet, DNet\n",
    "\n",
    "\n",
    "class Encoder(ENet):\n",
    "    '''\n",
    "    A gluon HybridBlock Encoder class\n",
    "    '''\n",
    "    def __init__(self, model_ctx, batch_size, input_dim, n_hidden=64, ndim_y=16, ndim_z=10, n_layers=0, nonlin=None,\n",
    "                 weights_file='', freeze=False, latent_nonlin='sigmoid', **kwargs):\n",
    "        '''\n",
    "        Constructor for encoder.\n",
    "        Args\n",
    "        ----\n",
    "        model_ctx: mxnet device context, No default\n",
    "          Which device to store/run the data and model on.\n",
    "        batch_size: integer, No default\n",
    "          The minibatch size.\n",
    "        input_dim: integer, No default\n",
    "          The data dimensionality that is input to the encoder.\n",
    "        n_hidden: integer or list, default 64\n",
    "          If integer, specifies the number of hidden units in\n",
    "          every hidden layer.\n",
    "          If list, each element specifies the number of hidden\n",
    "          units in each hidden layer.\n",
    "        output_dim: integer, default 10\n",
    "          The dimensionality of the latent space, z.\n",
    "        n_layers: integer, default 0\n",
    "          The number of hidden layers.\n",
    "        nonlin: string, default None\n",
    "          The nonlinearity to use in every hidden layer.\n",
    "        weights_file: string, default ''\n",
    "          The path to the file (mxnet params file or pickle file)\n",
    "          containing weights for each layer of the encoder.\n",
    "        freeze: boolean, default False\n",
    "          Whether to freeze the encoder weights (MIGHT BE BROKEN).\n",
    "        latent_nonlin: string, default 'sigmoid'\n",
    "          Which space to use for the latent variable:\n",
    "            if 'sigmoid': z in (0,1)\n",
    "            else: z in (-inf,inf)\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        encoder object.\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        if n_layers >= 0:\n",
    "            if isinstance(n_hidden, list):\n",
    "                n_hidden = n_hidden[0]\n",
    "                print('NOTE: Encoder ignoring list of hiddens because n_layer >= 0. Just using first element.')\n",
    "            n_hidden = n_layers*[n_hidden]\n",
    "        else:\n",
    "            n_layers = len(n_hidden)\n",
    "            print('NOTE: Encoder reading n_hidden as list.')\n",
    "\n",
    "        if nonlin == '':\n",
    "            nonlin = None\n",
    "        \n",
    "        in_units = input_dim\n",
    "        with self.name_scope():\n",
    "            self.main = nn.HybridSequential(prefix='encoder')\n",
    "            for i in range(n_layers):\n",
    "                self.main.add(nn.Dense(n_hidden[i], in_units=in_units, activation=nonlin))\n",
    "                in_units = n_hidden[i]\n",
    "            self.main.add(nn.Dense(ndim_y, in_units=in_units, activation=None))\n",
    "\n",
    "        self.model_ctx = model_ctx\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.ndim_y = ndim_y\n",
    "        self.ndim_z = ndim_z\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.nonlin = nonlin\n",
    "        self.latent_nonlin = latent_nonlin\n",
    "        self.weights_file = weights_file\n",
    "        self.freeze = freeze\n",
    "        self.dist_params = [None]\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        '''\n",
    "        Passes the input through the encoder.\n",
    "        Args\n",
    "        ----\n",
    "        F: mxnet.nd or mxnet.sym, No default\n",
    "          This will be passed implicitly when calling hybrid forward.\n",
    "        x: NDarray or mxnet symbol, No default\n",
    "          The input to the encoder.\n",
    "        Returns\n",
    "        -------\n",
    "        dist_params: list\n",
    "          A list of the posterior parameters as NDarrays, each being of size batch_size x z_dim.\n",
    "        samples: NDarray\n",
    "          The posterior samples as a batch_size x z_dim NDarray.\n",
    "        '''\n",
    "\n",
    "        y = self.main(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Decoder(DNet):\n",
    "    '''\n",
    "    A gluon HybridBlock Decoder class with Multinomial likelihood, p(x|z).\n",
    "    '''\n",
    "    def __init__(self, model_ctx, batch_size, output_dim, ndim_y=16,  n_hidden=64, n_layers=0, nonlin='',\n",
    "                 weights_file='', freeze=False, latent_nonlin='', **kwargs):\n",
    "        '''\n",
    "        Constructor for Multinomial decoder.\n",
    "        Args\n",
    "        ----\n",
    "        model_ctx: mxnet device context, No default\n",
    "          Which device to store/run the data and model on.\n",
    "        batch_size: integer, No default\n",
    "          The minibatch size.\n",
    "        n_hidden: integer or list, default 64\n",
    "          If integer, specifies the number of hidden units in\n",
    "          every hidden layer.\n",
    "          If list, each element specifies the number of hidden\n",
    "          units in each hidden layer.\n",
    "        output_dim: integer, No default\n",
    "          The dimensionality of the latent space, z.\n",
    "        n_layers: integer, default 0\n",
    "          The number of hidden layers.\n",
    "        nonlin: string, default 'sigmoid'\n",
    "          The nonlinearity to use in every hidden layer.\n",
    "        weights_file: string, default ''\n",
    "          The path to the file (mxnet params file or pickle file)\n",
    "          containing weights for each layer of the encoder.\n",
    "        freeze: boolean, default False\n",
    "          Whether to freeze the encoder weights (MIGHT BE BROKEN).\n",
    "        latent_nonlin: string, default 'sigmoid'\n",
    "          Which space to use for the latent variable:\n",
    "            if 'sigmoid': z in (0,1)\n",
    "            else: z in (-inf,inf)\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        Multinomial decoder object.\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        if n_layers >= 0:\n",
    "            if isinstance(n_hidden, list):\n",
    "                n_hidden = n_hidden[0]\n",
    "                print('NOTE: Decoder ignoring list of hiddens because n_layer >= 0. Just using first element.')\n",
    "            n_hidden = n_layers*[n_hidden]\n",
    "        else:\n",
    "            n_layers = len(n_hidden)\n",
    "            print('NOTE: Decoder reading n_hidden as list.')\n",
    "\n",
    "        if nonlin == '':\n",
    "            nonlin = None\n",
    "\n",
    "        in_units = n_hidden[0]\n",
    "        with self.name_scope():\n",
    "            self.main = nn.HybridSequential(prefix='decoder')\n",
    "            self.main.add(nn.Dense(n_hidden[0], in_units=ndim_y, activation=None))\n",
    "\n",
    "        self.model_ctx = model_ctx\n",
    "        self.batch_size = batch_size\n",
    "        self.ndim_y = ndim_y\n",
    "        self.n_hidden = n_hidden\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.nonlin = nonlin\n",
    "        self.latent_nonlin = latent_nonlin\n",
    "        self.weights_file = weights_file\n",
    "        self.freeze = freeze\n",
    "\n",
    "    def hybrid_forward(self, F, y):\n",
    "        '''\n",
    "        Passes the input through the decoder.\n",
    "        Args\n",
    "        ----\n",
    "        F: mxnet.nd or mxnet.sym, No default\n",
    "          This will be passed implicitly when calling hybrid forward.\n",
    "        x: NDarray or mxnet symbol, No default\n",
    "          The input to the decoder.\n",
    "        Returns\n",
    "        -------\n",
    "        dist_params: list\n",
    "          A list of the multinomial parameters as NDarrays, each being of size batch_size x z_dim.\n",
    "        samples: NDarray\n",
    "          The multinomial samples as a batch_size x z_dim NDarray (NOT IMPLEMENTED).\n",
    "        '''\n",
    "        out = self.main(y)\n",
    "        return out\n",
    "\n",
    "    def y_as_topics(self, eps=1e-10):\n",
    "        y = np.eye(self.ndim_y)\n",
    "        return mx.nd.array(y)\n",
    "\n",
    "class Discriminator_y(ENet):\n",
    "    '''\n",
    "    A gluon HybridBlock Discriminator Class for y\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model_ctx, batch_size, output_dim=2, ndim_y=16, n_hidden=64, n_layers=0, nonlin='sigmoid',\n",
    "                 weights_file='', freeze=False, latent_nonlin='sigmoid', apply_softmax=False, **kwargs):\n",
    "        '''\n",
    "        Constructor for Discriminator Class for y.\n",
    "        Args\n",
    "        ----\n",
    "        model_ctx: mxnet device context, No default\n",
    "          Which device to store/run the data and model on.\n",
    "        batch_size: integer, No default\n",
    "          The minibatch size.\n",
    "        n_hidden: integer or list, default 64\n",
    "          If integer, specifies the number of hidden units in\n",
    "          every hidden layer.\n",
    "          If list, each element specifies the number of hidden\n",
    "          units in each hidden layer.\n",
    "        output_dim: integer, No default\n",
    "          The dimensionality of the latent space, z.\n",
    "        n_layers: integer, default 0\n",
    "          The number of hidden layers.\n",
    "        nonlin: string, default 'sigmoid'\n",
    "          The nonlinearity to use in every hidden layer.\n",
    "        weights_file: string, default ''\n",
    "          The path to the file (mxnet params file or pickle file)\n",
    "          containing weights for each layer of the encoder.\n",
    "        freeze: boolean, default False\n",
    "          Whether to freeze the encoder weights (MIGHT BE BROKEN).\n",
    "        latent_nonlin: string, default 'sigmoid'\n",
    "          Which space to use for the latent variable:\n",
    "            if 'sigmoid': z in (0,1)\n",
    "            else: z in (-inf,inf)\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        Multinomial Discriminator object.\n",
    "        '''\n",
    "        super(Discriminator_y, self).__init__()\n",
    "\n",
    "        if n_layers >= 0:\n",
    "            if isinstance(n_hidden, list):\n",
    "                n_hidden = n_hidden[0]\n",
    "                print('NOTE: Decoder ignoring list of hiddens because n_layer >= 0. Just using first element.')\n",
    "            n_hidden = n_layers * [n_hidden]\n",
    "        else:\n",
    "            n_layers = len(n_hidden)\n",
    "            print('NOTE: Decoder reading n_hidden as list.')\n",
    "\n",
    "        if latent_nonlin != 'sigmoid':\n",
    "            print('NOTE: Latent z will be fed to decoder in logit-space (-inf,inf).')\n",
    "        else:\n",
    "            print('NOTE: Latent z will be fed to decoder in probability-space (0,1).')\n",
    "\n",
    "        if nonlin == '':\n",
    "            nonlin = None\n",
    "\n",
    "        in_units = ndim_y\n",
    "        with self.name_scope():\n",
    "            self.main = nn.HybridSequential(prefix='discriminator_y')\n",
    "            for i in range(n_layers):\n",
    "                self.main.add(nn.Dense(n_hidden[i], in_units=in_units, activation=nonlin))\n",
    "                in_units = n_hidden[i]\n",
    "            self.main.add(nn.Dense(output_dim, in_units=in_units, activation=None))\n",
    "\n",
    "        self.model_ctx = model_ctx\n",
    "        self.batch_size = batch_size\n",
    "        self.ndim_y = ndim_y\n",
    "        self.n_hidden = n_hidden\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.nonlin = nonlin\n",
    "        self.latent_nonlin = latent_nonlin\n",
    "        self.weights_file = weights_file\n",
    "        self.freeze = freeze\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def hybrid_forward(self, F, y):\n",
    "        '''\n",
    "        Passes the input through the decoder.\n",
    "        Args\n",
    "        ----\n",
    "        F: mxnet.nd or mxnet.sym, No default\n",
    "          This will be passed implicitly when calling hybrid forward.\n",
    "        x: NDarray or mxnet symbol, No default\n",
    "          The input to the decoder.\n",
    "        '''\n",
    "        logit = self.main(y)\n",
    "        if self.apply_softmax:\n",
    "            return F.softmax(logit)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "twenty_news",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9c2a161ea003>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCompute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDomain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDiscriminator_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-218b8717ca36>\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomains\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlda_synthetic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLdaSynthetic\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mDomain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'domain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'dirichlet'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: twenty_news"
     ]
    }
   ],
   "source": [
    "Compute, Domain, Encoder, Decoder, Discriminator_y, args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
