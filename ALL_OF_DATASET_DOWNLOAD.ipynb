{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download 파헤처기\n",
    "- module path, data path 따로 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Preprocessing script 얻기\n",
    "- module_path로 script file을 shutil로 copy해주는 부분 존재함\n",
    "- 경로는 내부적으로 대게 `{USER_PATH}\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\`에 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import prepare_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path, hash_, resolved_file_path = prepare_module(\n",
    "    path=\"./wlda/wikitext.py\",\n",
    "    cache_dir=\"data\",\n",
    "    return_resolved_file_path=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets_modules.datasets.wikitext.8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af.wikitext'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./wlda/wikitext.py'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같은 `wikitext.json` 파일도 생성되고, script도 동일하게 복사한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original file path': './wlda/wikitext.py',\n",
       " 'local file path': 'C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\modules\\\\datasets_modules\\\\datasets\\\\wikitext\\\\24181ecf0c80364527ddc5234d7e27fd0266aa176c88772db341f191e86e6e14\\\\wikitext.py'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"original file path\": \"./wlda/wikitext.py\", \n",
    "    \"local file path\": \"C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\modules\\\\datasets_modules\\\\datasets\\\\wikitext\\\\24181ecf0c80364527ddc5234d7e27fd0266aa176c88772db341f191e86e6e14\\\\wikitext.py\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Processing Script에서 Main Class 가져오기\n",
    "- 얘의 로직은 여러 번 시행해봤기 때문에 알거라고 믿음\n",
    "- tutorial 작성은 나중에 고려해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import import_main_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_cls = import_main_class(module_path, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets_modules.datasets.wikitext.8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af.wikitext.Wikitext"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. DatasetBuilder 객체화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_instance = builder_cls(\n",
    "    cache_dir=\"data\",\n",
    "    name=\"wikitext-103-v1\",\n",
    "    data_dir=None, # 얜 무엇? 이미 있으면 pass하는건가\n",
    "    data_files=None, # 얘도 무엇? 궁금\n",
    "    hash=hash_, # 얜 빼도 될 듯. locally 작업 도중엔\n",
    "    features=None, # 얜 없으면 script에서 feature 넣어주는거 사용할 듯?\n",
    "    **config_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 객체가 생김과 동시에 `cache_dir`이 생성되었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets_modules.datasets.wikitext.8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af.wikitext.Wikitext at 0x15b71914b38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'wikitext',\n",
       " 'hash': '8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af',\n",
       " 'config': WikitextConfig(name='wikitext-103-v1', version=1.0.0, data_dir=None, data_files=None, description='raw level dataset. The raw tokens before the addition of <unk> tokens. They should only be used for character level work or for creating newly derived datasets.'),\n",
       " 'config_id': 'wikitext-103-v1',\n",
       " 'info': DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\n', citation='@InProceedings{wikitext,\\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\\n    year={2016}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='wikitext', config_name='wikitext-103-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1272551, num_examples=62, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=535694801, num_examples=29444, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1134973, num_examples=60, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076, 'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}, download_size=190229076, post_processing_size=None, dataset_size=538102325, size_in_bytes=728331401),\n",
       " '_cache_dir_root': 'data',\n",
       " '_cache_dir': 'data\\\\wikitext\\\\wikitext-103-v1\\\\1.0.0\\\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af',\n",
       " '_writer_batch_size': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datasets_modules.datasets.wikitext.8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af.wikitext.Wikitext,\n",
       " datasets.builder.GeneratorBasedBuilder,\n",
       " datasets.builder.DatasetBuilder,\n",
       " object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_cls.__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init에선 무슨일이 벌어질까? 우선 `GeneratorBaseBuilder`에선 바로 상위 `DatasetBuilder`의 init으로 넘어간다. 그 이후, `_writer_batch_size`를 할당한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance._writer_batch_size is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.DEFAULT_WRITER_BATCH_SIZE is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DatasetBuilder` 클래스를 살펴보자. 해당 Builder 클래스는 아래 세 가지 핵심 메서드로 구성된다고 한다.\n",
    "1. `datasets.DatasetBuilder.info`\n",
    "2. `datasets.DatasetBuilder.download_and_prepare`\n",
    "3. `datasets.DatasetBuilder.as_dataset`\n",
    "\n",
    "이 중 2와 3은 Step 4, 5와 매칭되는 것이며, 현재는 객체 생성만 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 객체의 이름은 본래 cls의 이름을 `camelcase_to_snakecase` 메서드를 통해 snakecase로 변환하여 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wikitext'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikitext'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hash도 바로 할당해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 config을 세팅해주는데, `~.BUILDER_CONFIG_CLASS`의 `__init__`의 signature를 검사하여 features가 있다면 config_kwargs에 넣어준다.\n",
    "- `WikitextConfig`을 비교해주는건진 잘 모르겠다.\n",
    "- 아래보면 그냥 `BUILDER_CONFIG_CLASS`와 비교하는 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.builder.BuilderConfig"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.BUILDER_CONFIG_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'self': <Parameter \"self\">,\n",
       "              'name': <Parameter \"name:str='default'\">,\n",
       "              'version': <Parameter \"version:Union[str, datasets.utils.version.Version, NoneType]='0.0.0'\">,\n",
       "              'data_dir': <Parameter \"data_dir:str=None\">,\n",
       "              'data_files': <Parameter \"data_files:Union[Dict, List]=None\">,\n",
       "              'description': <Parameter \"description:str=None\">})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(\n",
    "    builder_instance.BUILDER_CONFIG_CLASS.__init__).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"features\" in inspect.signature(\n",
    "    builder_instance.BUILDER_CONFIG_CLASS.__init__).parameters and None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 경우는 features가 None인 경우. 여기서 클래스의 `BUILDER_CONFIGS: List`를 검사하여 config을 할당하는 모양."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WikitextConfig(name='wikitext-103-v1', version=1.0.0, data_dir=None, data_files=None, description='raw level dataset. The raw tokens before the addition of <unk> tokens. They should only be used for character level work or for creating newly derived datasets.')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info는 아래 `get_exported_dataset_info` 메서드를 통해 기본 `DatasetInfo` 객체를 받아온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified \\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\n', citation='@InProceedings{wikitext,\\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\\n    year=2016\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='wikitext', config_name='wikitext-103-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1272551, num_examples=62, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=535694801, num_examples=29444, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1134973, num_examples=60, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076, 'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = builder_instance.get_exported_dataset_info()\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음, 객체에 정의된 `_info`메서드를 통해 info를 업데이트시켜준다.\n",
    "\n",
    "```python\n",
    "class Wikitext(datasets.GeneratorBasedBuilder):\n",
    "    ...\n",
    "    def _info(self):\n",
    "        features = datasets.Features(\n",
    "            {\n",
    "                \"text\": datasets.Value(\"string\")\n",
    "            }\n",
    "        )\n",
    "        return datasets.DatasetInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            features=features,\n",
    "            supervised_keys=None,\n",
    "            homepage=_URL,\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.update(builder_instance._info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\n', citation='@InProceedings{wikitext,\\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\\n    year={2016}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='wikitext', config_name='wikitext-103-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1272551, num_examples=62, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=535694801, num_examples=29444, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1134973, num_examples=60, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076, 'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에 이것저것 setting해준 다음, 만일 features가 정의가 안되어있었다면, 입력인자의 features로 넣어줘도 무방하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.builder_name = builder_instance.name\n",
    "info.config_name = builder_instance.config.name\n",
    "info.version = builder_instance.config.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음엔 `data_dir`을 준비한다!\n",
    "- `cache_dir`이 존재하면 사용\n",
    "- 아니라면 `datasets.config.HF_DATASETS_CACHE`을 사용\n",
    "- {USER_PATH}로 확장하여 data_dir을 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\n', citation='@InProceedings{wikitext,\\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\\n    year={2016}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='wikitext', config_name='wikitext-103-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1272551, num_examples=62, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=535694801, num_examples=29444, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1134973, num_examples=60, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076, 'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\n', citation='@InProceedings{wikitext,\\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\\n    year={2016}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='wikitext', config_name='wikitext-103-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1272551, num_examples=62, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=535694801, num_examples=29444, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1134973, num_examples=60, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076, 'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}, download_size=190229076, post_processing_size=None, dataset_size=538102325, size_in_bytes=728331401)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/jinma/.cache/huggingface/datasets')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import config\n",
    "\n",
    "config.HF_DATASETS_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\datasets'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cache_dir_root = os.path.expanduser(config.HF_DATASETS_CACHE)\n",
    "_cache_dir_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cache_dir_root = os.path.expanduser(\"data\")\n",
    "_cache_dir_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "root는 그냥 위처럼 단순 root이며, 실제 저장위치는 hash와 이것 저것 버전 등을 입혀서 저장해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\wikitext\\\\wikitext-103-v1\\\\1.0.0\\\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cache_dir = builder_instance._build_cache_dir()\n",
    "_cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 객체 생성시, `_cache_dir_root`에 해당하는 부분의 폴더만 생성해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(_cache_dir_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 혹시 file이 존재한다면, info 파일을 해당 directory에서 읽어온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Data Download\n",
    "- Builder 클래스의 핵심 메서드 중 2번째! `download_and_prepare`를 사용할 것\n",
    "- 해당 과정을 요약하자면,\n",
    "    - Cache가 있다면 데이터셋 재사용\n",
    "    - 없으면 웹에서 다운로드 실시 (hub or url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과정과 결과를 조금 더 상세하게 쓰자면,\n",
    "- cache 검사, 만약 존재하지 않으면 아래 과정으로, 존재한다면 바로 값 반환\n",
    "- raw dataset 다운로드 (hub or url)\n",
    "- `_split_generators` 메서드 실행\n",
    "- split별로 line별 `_generate_examples`로 sample을 arrow dataset에 넣음\n",
    "\n",
    "결과값은\n",
    "- LICENSE\n",
    "- arrow 파일 객체\n",
    "- dataset_info 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils.file_utils import url_or_path_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./wlda/wikitext.py'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./wlda'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_or_path_parent(resolved_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GenerateMode.REUSE_DATASET_IF_EXISTS: 'reuse_dataset_if_exists'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DownloadConfig, DownloadManager, GenerateMode\n",
    "\n",
    "download_mode = GenerateMode(None or GenerateMode.REUSE_DATASET_IF_EXISTS)\n",
    "download_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_infos = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\downloads'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_config = DownloadConfig(    \n",
    "    cache_dir=os.path.join(builder_instance._cache_dir_root, \"downloads\"),\n",
    "    force_download=False,\n",
    "    use_etag=False,\n",
    "    use_auth_token=False,\n",
    ")  # We don't use etag for data files to speed up the process\n",
    "\n",
    "download_config.cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_instance.config.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_manager = DownloadManager(\n",
    "    dataset_name=builder_instance.name,\n",
    "    download_config=download_config,\n",
    "    data_dir=builder_instance.config.data_dir, # None, 얜 setting X\n",
    "    base_path=url_or_path_parent(resolved_file_path), # ./wlda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_manager.manual_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_manager.downloaded_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\data_wikitext_wikitext-103-v1_1.0.0_8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af.lock'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prevent parallel disk operations\n",
    "lock_path = os.path.join(\n",
    "    builder_instance._cache_dir_root, \n",
    "    builder_instance._cache_dir.replace(os.sep, \"_\") + \".lock\"\n",
    ")\n",
    "lock_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils.filelock import FileLock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import contextlib\n",
    "from datasets import utils\n",
    "from datasets.utils.file_utils import is_remote_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cache 파일이 없거나 재사용하지 않을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.arrow_reader import (\n",
    "    ArrowReader, HF_GCP_BASE_URL, DatasetNotOnHfGcs, MissingFilesOnHfGcs\n",
    ")\n",
    "from datasets.info import DatasetInfo\n",
    "from datasets.splits import SplitDict\n",
    "from datasets.utils.info_utils import verify_checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.utils.filelock._Acquire_ReturnProxy at 0x15b781d3588>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelock = FileLock(lock_path)\n",
    "filelock.acquire() # ENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_exists: True\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "실험을 위해서 파일을 제거해주세요",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-d641d5e4fb0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata_exists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuilder_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"data_exists: {data_exists}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata_exists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"실험을 위해서 파일을 제거해주세요\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: 실험을 위해서 파일을 제거해주세요"
     ]
    }
   ],
   "source": [
    "data_exists = os.path.exists(builder_instance._cache_dir)\n",
    "print(f\"data_exists: {data_exists}\")\n",
    "assert not data_exists, \"실험을 위해서 파일을 제거해주세요\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not is_remote_url(builder_instance._cache_dir_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영웅왕, 디스크 크기는 충분한가?\n",
    "not utils.has_sufficient_disk_space(\n",
    "    builder_instance.info.size_in_bytes or 0, \n",
    "    # 'C:\\\\Users\\\\jinma\\\\.cache\\\\huggingface\\\\datasets'\n",
    "    directory=builder_instance._cache_dir_root\n",
    ") # IOError 발생 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print is intentional: we want this to always go to stdout so user has\n",
    "# information needed to cancel download/preparation if needed.\n",
    "# This comes right before the progress bar.\n",
    "print(\n",
    "    f\"Downloading and preparing dataset {builder_instance.info.builder_name}/{builder_instance.info.config_name} \"\n",
    "    f\"(download: {utils.size_str(builder_instance.info.download_size)}, generated: {utils.size_str(builder_instance.info.dataset_size)}, \"\n",
    "    f\"post-processed: {utils.size_str(builder_instance.info.post_processing_size)}, \"\n",
    "    f\"total: {utils.size_str(builder_instance.info.size_in_bytes)}) to {builder_instance._cache_dir}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': ' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\n',\n",
       " 'citation': '@InProceedings{wikitext,\\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\\n    year={2016}\\n}\\n',\n",
       " 'homepage': 'https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/',\n",
       " 'license': '',\n",
       " 'features': {'text': {'dtype': 'string', 'id': None, '_type': 'Value'}},\n",
       " 'post_processed': None,\n",
       " 'supervised_keys': None,\n",
       " 'builder_name': 'wikitext',\n",
       " 'config_name': 'wikitext-103-v1',\n",
       " 'version': {'version_str': '1.0.0',\n",
       "  'description': None,\n",
       "  'major': 1,\n",
       "  'minor': 0,\n",
       "  'patch': 0},\n",
       " 'splits': {'test': {'name': 'test',\n",
       "   'num_bytes': 1272551,\n",
       "   'num_examples': 62,\n",
       "   'dataset_name': 'wikitext'},\n",
       "  'train': {'name': 'train',\n",
       "   'num_bytes': 535694801,\n",
       "   'num_examples': 29444,\n",
       "   'dataset_name': 'wikitext'},\n",
       "  'validation': {'name': 'validation',\n",
       "   'num_bytes': 1134973,\n",
       "   'num_examples': 60,\n",
       "   'dataset_name': 'wikitext'}},\n",
       " 'download_checksums': {'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076,\n",
       "   'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}},\n",
       " 'download_size': 190229076,\n",
       " 'post_processing_size': None,\n",
       " 'dataset_size': 538102325,\n",
       " 'size_in_bytes': 728331401}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdict(builder_instance.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `incomplete_dir` contextmanager!\n",
    "\n",
    "```python\n",
    "@contextlib.contextmanager\n",
    "def incomplete_dir(dirname):\n",
    "    \"\"\"Create temporary dir for dirname and rename on exit.\"\"\"\n",
    "    if is_remote_url(dirname):\n",
    "        yield dirname\n",
    "    else:\n",
    "        tmp_dir = dirname + \".incomplete\"\n",
    "        os.makedirs(tmp_dir, exist_ok=True)\n",
    "        try:\n",
    "            yield tmp_dir\n",
    "            if os.path.isdir(dirname):\n",
    "                shutil.rmtree(dirname)\n",
    "            os.rename(tmp_dir, dirname)\n",
    "        finally:\n",
    "            if os.path.exists(tmp_dir):\n",
    "                shutil.rmtree(tmp_dir)\n",
    "```\n",
    "\n",
    "- `utils.py_utils.temporary` contextmanager!\n",
    "\n",
    "```python\n",
    "@contextlib.contextmanager\n",
    "def temporary_assignment(obj, attr, value):\n",
    "    \"\"\"Temporarily assign obj.attr to value.\"\"\"\n",
    "    original = getattr(obj, attr, None)\n",
    "    setattr(obj, attr, value)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        setattr(obj, attr, original)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\wikitext\\\\wikitext-103-v1\\\\1.0.0\\\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirname = builder_instance._cache_dir\n",
    "dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_remote_url(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = dirname + \".incomplete\"\n",
    "os.makedirs(tmp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\wikitext\\\\wikitext-103-v1\\\\1.0.0\\\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af.incomplete'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter @ incomplete_dir\n",
    "tmp_data_dir = tmp_dir\n",
    "tmp_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: data\\wikitext\\wikitext-103-v1\\1.0.0\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af\n",
      "temp    : data\\wikitext\\wikitext-103-v1\\1.0.0\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af.incomplete\n"
     ]
    }
   ],
   "source": [
    "# >> Enter @ temporary_assignment\n",
    "original = getattr(builder_instance, \"_cache_dir\", None)\n",
    "setattr(builder_instance, \"_cache_dir\", tmp_data_dir)\n",
    "\n",
    "print(f\"\"\"\n",
    "original: {original}\n",
    "temp    : {builder_instance._cache_dir}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.arrow_reader import DatasetNotOnHfGcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurs!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    builder_instance._download_prepared_from_hf_gcs(\n",
    "        dl_manager._download_config)\n",
    "except DatasetNotOnHfGcs as e:\n",
    "    print(\"Error occurs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, datasets.splits.SplitDict)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self._download_and_prepare\n",
    "split_dict = SplitDict(dataset_name=builder_instance.name)\n",
    "split_dict, type(split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, dict)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_generators_kwargs = builder_instance._make_split_generators_kwargs({})\n",
    "split_generators_kwargs, type(split_generators_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 download 발생! dl_manager의 `download_and_extract` 메서드를 활용\n",
    "- downloads에 파일 다운로드함!\n",
    "- extract 위치도 바꿔버리면...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SplitGenerator(name='test', gen_kwargs={'data_file': 'data\\\\downloads\\\\extracted\\\\b23f382c6349d2566715ce27126d0fdda673c06ff84c853c01187a45c59ca52c\\\\wikitext-103\\\\wiki.test.tokens', 'split': 'test'}, split_info=SplitInfo(name='test', num_bytes=0, num_examples=0, dataset_name=None)),\n",
       " SplitGenerator(name='train', gen_kwargs={'data_file': 'data\\\\downloads\\\\extracted\\\\b23f382c6349d2566715ce27126d0fdda673c06ff84c853c01187a45c59ca52c\\\\wikitext-103\\\\wiki.train.tokens', 'split': 'train'}, split_info=SplitInfo(name='train', num_bytes=0, num_examples=0, dataset_name=None)),\n",
       " SplitGenerator(name='validation', gen_kwargs={'data_file': 'data\\\\downloads\\\\extracted\\\\b23f382c6349d2566715ce27126d0fdda673c06ff84c853c01187a45c59ca52c\\\\wikitext-103\\\\wiki.valid.tokens', 'split': 'valid'}, split_info=SplitInfo(name='validation', num_bytes=0, num_examples=0, dataset_name=None))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_generators = builder_instance._split_generators(dl_manager)\n",
    "split_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.config.data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076,\n",
       "  'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.info.download_checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076,\n",
       "  'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_manager.get_recorded_sizes_checksums()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checksums verification\n",
    "verify_checksums(\n",
    "    builder_instance.info.download_checksums,\n",
    "    dl_manager.get_recorded_sizes_checksums(),\n",
    "    \"dataset source files\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.features.Features"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(builder_instance.info.features) # Dict의 subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(builder_instance.info.features, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.arrow_writer import ArrowWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build Split\n",
    "for split_generator in split_generators:\n",
    "    if str(split_generator.split_info.name).lower() == \"all\":\n",
    "        raise ValueError(\n",
    "            \"`all` is a special split keyword corresponding to the \"\n",
    "            \"union of all splits, so cannot be used as key in \"\n",
    "            \"._split_generator().\"\n",
    "        )\n",
    "    split_dict.add(split_generator.split_info)\n",
    "    # 아래 메서드는 GeneratorBasedBuilder에 있음\n",
    "    # **************** _prepare_split ******************\n",
    "    split_info = split_generator.split_info\n",
    "    fname = \"{}-{}.arrow\".format(builder_instance.name, split_generator.name)\n",
    "    fpath = os.path.join(builder_instance._cache_dir, fname) # incomplete\n",
    "    \n",
    "    # >> 여기서 최상위 객체의 `_generate_examples` 메서드가 사용되는구나~\n",
    "    generator = builder_instance._generate_examples(**split_generator.gen_kwargs)\n",
    "    not_verbose = False\n",
    "#     with ArrowWriter(features=builder_instance.info.features,\n",
    "#                      path=fpath,\n",
    "#                      writer_batch_size=builder_instance._writer_batch_size) as writer:\n",
    "    writer = ArrowWriter(features=builder_instance.info.features,\n",
    "                         path=fpath,\n",
    "                         writer_batch_size=builder_instance._writer_batch_size)\n",
    "    try:\n",
    "        for key, record in utils.tqdm(\n",
    "            generator, unit=\" examples\", total=split_info.num_examples,\n",
    "            leave=False, disable=not_verbose\n",
    "        ):\n",
    "            # 아래 메서드는 다음 두 가지 기능을 수행\n",
    "            # (1) 아래 경우, python object로 변환하여 반환, 아니면 그냥 pass\n",
    "            #     ndarray, torch.Tensor, tf.Tensor, pd.Series, pd.DataFrame\n",
    "            #     dict, list, tuple\n",
    "            # (2) 감싸진 객체를 전부 검사하면서 내려감!\n",
    "            #     dict, list, tuple, Sequence, str, ClassLabel, ...\n",
    "            #     위 객체들이면 recursive하게 계속 검사함\n",
    "            #     값이 나올 경우 return\n",
    "            example = builder_instance.info.features.encode_example(record)\n",
    "            writer.write(example)\n",
    "    finally:\n",
    "        num_examples, num_bytes = writer.finalize()\n",
    "        \n",
    "    split_generator.split_info.num_examples = num_examples\n",
    "    split_generator.split_info.num_bytes = num_bytes\n",
    "    # **************** _prepare_split ******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils.info_utils import verify_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verify_infos:\n",
    "    verify_splits(builder_instance.info.splits, split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_instance.info.splits = split_dict\n",
    "builder_instance.info.download_size = dl_manager.downloaded_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync info\n",
    "builder_instance.info.dataset_size = sum(\n",
    "    split.num_bytes for split in builder_instance.info.splits.values())\n",
    "builder_instance.info.download_checksums = dl_manager.get_recorded_sizes_checksums()\n",
    "builder_instance.info.size_in_bytes = builder_instance.info.dataset_size + \\\n",
    "                                      builder_instance.info.download_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\data_wikitext_wikitext-103-v1_1.0.0_8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af.incomplete.lock'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\n",
    "    builder_instance._cache_dir_root, \n",
    "    builder_instance._cache_dir.replace(os.sep, \"_\") + \".lock\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_instance._save_info() # LICENSE와 dataset_info.json 저장!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Exit @ temporary_assignment\n",
    "setattr(builder_instance, \"_cache_dir\", original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETE\n"
     ]
    }
   ],
   "source": [
    "# Exit @ incomplete_dir\n",
    "if os.path.isdir(dirname):\n",
    "    print(\"DELETE\")\n",
    "    shutil.rmtree(dirname)\n",
    "os.rename(tmp_dir, dirname)\n",
    "if os.path.exists(tmp_dir):\n",
    "    shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download post processing resources\n",
    "builder_instance.download_post_processing_resources(dl_manager)\n",
    "# 기본구현은 {}, no mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to data\\wikitext\\wikitext-103-v1\\1.0.0\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Dataset {builder_instance.name} downloaded and prepared to \"\n",
    "    f\"{builder_instance._cache_dir}. Subsequent calls will reuse this data.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cache 파일이 있어서 재사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_instance.download_post_processing_resources(dl_manager) # 아무것도 안함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Dataset Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'wikitext',\n",
       " 'hash': '8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af',\n",
       " 'config': WikitextConfig(name='wikitext-103-v1', version=1.0.0, data_dir=None, data_files=None, description='raw level dataset. The raw tokens before the addition of <unk> tokens. They should only be used for character level work or for creating newly derived datasets.'),\n",
       " 'config_id': 'wikitext-103-v1',\n",
       " 'info': DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\n', citation='@InProceedings{wikitext,\\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\\n    year={2016}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='wikitext', config_name='wikitext-103-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1272551, num_examples=62, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=535694801, num_examples=29444, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1134973, num_examples=60, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076, 'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}, download_size=190229076, post_processing_size=None, dataset_size=538102325, size_in_bytes=728331401),\n",
       " '_cache_dir_root': 'data',\n",
       " '_cache_dir': 'data\\\\wikitext\\\\wikitext-103-v1\\\\1.0.0\\\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af',\n",
       " '_writer_batch_size': None}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder_instance.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset for splits\n",
    "keep_in_memory = False # is_small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 'test', 'train': 'train', 'validation': 'validation'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = {s: s for s in builder_instance.info.splits}\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = tuple([tuple])\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train', 'validation']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterable = list(split.values()) if isinstance(split, dict) else split\n",
    "iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "function = partial(\n",
    "    builder_instance._build_single_dataset,\n",
    "    run_post_process=True,\n",
    "    ignore_verifications=False,\n",
    "    in_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(functools.partial(<bound method DatasetBuilder._build_single_dataset of <datasets_modules.datasets.wikitext.8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af.wikitext.Wikitext object at 0x0000015B71914B38>>, run_post_process=True, ignore_verifications=False, in_memory=False),\n",
       "  ['test', 'train', 'validation'],\n",
       "  (tuple,),\n",
       "  0,\n",
       "  False)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_kwds = []  # We organize the splits ourselve (contiguous splits)\n",
    "for index in range(1):\n",
    "    div = len(iterable) // 1\n",
    "    mod = len(iterable) % 1\n",
    "    start = div * index + min(index, mod)\n",
    "    end = start + div + (1 if index < mod else 0)\n",
    "    split_kwds.append((function, iterable[start:end], types, index, False))\n",
    "split_kwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _single_map_nested\n",
    "function, data_struct, types, rank, disable_tqdm = split_kwds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not isinstance(data_struct, dict) and not isinstance(data_struct, types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train', 'validation']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = data_struct\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_infos = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(split, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.splits import Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arrow_table': pyarrow.Table\n",
       " text: string,\n",
       " 'data_files': [{'filename': 'data\\\\wikitext\\\\wikitext-103-v1\\\\1.0.0\\\\8ae2a41908b3b12285d41e5b92b82eb1837e7053db277a34d471f19c5e0888af\\\\wikitext-test.arrow',\n",
       "   'skip': 0,\n",
       "   'take': 62}],\n",
       " 'info': DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\n', citation='@InProceedings{wikitext,\\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\\n    year={2016}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='wikitext', config_name='wikitext-103-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1272551, num_examples=62, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=535694801, num_examples=29444, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1134973, num_examples=60, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076, 'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}, download_size=190229076, post_processing_size=None, dataset_size=538102325, size_in_bytes=728331401),\n",
       " 'split': NamedSplit('test')}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _as_dataset\n",
    "dataset_kwargs = ArrowReader(builder_instance._cache_dir, builder_instance.info).read(\n",
    "    name=builder_instance.name,\n",
    "    instructions=Split(\"test\"),\n",
    "    split_infos=builder_instance.info.splits.values(),\n",
    "    in_memory=False,\n",
    ")\n",
    "dataset_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['arrow_table', 'data_files', 'info', 'split'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_kwargs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.arrow_dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 62\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset(**dataset_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build base dataset\n",
    "ds = builder_instance._as_dataset(\n",
    "    split=Split(split[0]),\n",
    "    in_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_post_process\n",
    "print(builder_instance._post_processing_resources(split))\n",
    "resources_paths = {}\n",
    "builder_instance._post_process(ds, resources_paths) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 62\n",
       "})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
