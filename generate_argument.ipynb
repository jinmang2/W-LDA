{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom : str = field(\n",
      "    default=\"wikitext-103\", metadata={\"help\": \"\"})\n",
      "desc : str = field(\n",
      "    default=\"wikitext103_paper\", metadata={\"help\": \"\"})\n",
      "mod : str = field(\n",
      "    default=\"dirichlet\", metadata={\"help\": \"\"})\n",
      "ndim_x : int = field(\n",
      "    default=20000, metadata={\"help\": \"\"})\n",
      "opt : str = field(\n",
      "    default=\"Adam\", metadata={\"help\": \"\"})\n",
      "e_nh : int = field(\n",
      "    default=100, metadata={\"help\": \"\"})\n",
      "e_nl : int = field(\n",
      "    default=-1, metadata={\"help\": \"\"})\n",
      "e_nonlin : str = field(\n",
      "    default=\"softrelu\", metadata={\"help\": \"\"})\n",
      "d_nh : int = field(\n",
      "    default=20000, metadata={\"help\": \"\"})\n",
      "d_nl : int = field(\n",
      "    default=-1, metadata={\"help\": \"\"})\n",
      "dis_nh : int = field(\n",
      "    default=50, metadata={\"help\": \"\"})\n",
      "dis_nl : int = field(\n",
      "    default=-1, metadata={\"help\": \"\"})\n",
      "verb : bool = field(\n",
      "    default=True, metadata={\"help\": \"\"})\n",
      "eval_stats_every : int = field(\n",
      "    default=10, metadata={\"help\": \"\"})\n",
      "adverse : bool = field(\n",
      "    default=True, metadata={\"help\": \"\"})\n",
      "update_enc : bool = field(\n",
      "    default=True, metadata={\"help\": \"\"})\n",
      "train_mode : str = field(\n",
      "    default=\"mmd\", metadata={\"help\": \"\"})\n",
      "ndim_y : int = field(\n",
      "    default=50, metadata={\"help\": \"\"})\n",
      "bs : int = field(\n",
      "    default=360, metadata={\"help\": \"\"})\n",
      "lr : float = field(\n",
      "    default=0.002, metadata={\"help\": \"\"})\n",
      "kernel_alpha : int = field(\n",
      "    default=1, metadata={\"help\": \"\"})\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in \"\"\"\n",
    "-dom wikitext-103\n",
    "-desc wikitext103_paper\n",
    "-mod dirichlet\n",
    "-ndim_x 20000\n",
    "-opt Adam\n",
    "-e_nh 100 100\n",
    "-e_nl -1\n",
    "-e_nonlin softrelu\n",
    "-d_nh 20000\n",
    "-d_nl -1\n",
    "-dis_nh 50 50\n",
    "-dis_nl -1\n",
    "-verb True\n",
    "-eval_stats_every 10\n",
    "-adverse True\n",
    "-update_enc True\n",
    "-train_mode mmd\n",
    "-ndim_y 50\n",
    "-bs 360\n",
    "-lr 0.002\n",
    "-kernel_alpha 1\"\"\".strip().split(\"\\n\"):\n",
    "    j = i.split(\" \")\n",
    "    key = j[0][1:]\n",
    "    value = j[1]\n",
    "    try:\n",
    "        type_ = eval(value)\n",
    "    except:\n",
    "        type_ = \"\"\n",
    "        value = '\"' + value + '\"'\n",
    "    type_ = re.compile(\"[a-z]+\").findall(repr(type(type_)))[-1]\n",
    "    value = f\"field(\\n    default={value},\" + ' metadata={\"help\": \"\"})'\n",
    "    res.append(key + f\" : {type_}\" + \" = \" + value)\n",
    "    \n",
    "print(\"\\n\".join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_argument = \"\"\"parser.add_argument('-dom','--domain', type=str, default='twenty_news', help='domain to run', required=False)\n",
    "parser.add_argument('-data','--data_path', type=str, default='', help='file path for dataset', required=False)\n",
    "parser.add_argument('-max_labels','--max_labels', type=int, default=100, help='max number of topics to specify as labels for a single training document', required=False)\n",
    "parser.add_argument('-max_labeled_samples','--max_labeled_samples', type=int, default=10, help='max number of labeled samples per topic', required=False)\n",
    "parser.add_argument('-label_seed','--label_seed', type=lambda x: int(x) if x != 'None' else None, default=None, help='random seed for subsampling the labeled dataset', required=False)\n",
    "parser.add_argument('-mod','--model', type=str, default='dirichlet', help='model to use', required=False)\n",
    "parser.add_argument('-desc','--description', type=str, default='', help='description for the experiment', required=False)\n",
    "parser.add_argument('-alg','--algorithm', type=str, default='standard', help='algorithm to use for training: standard', required=False)\n",
    "parser.add_argument('-bs','--batch_size', type=int, default=256, help='batch_size for training', required=False)\n",
    "parser.add_argument('-opt','--optim', type=str, default='Adam', help='encoder training algorithm', required=False)\n",
    "parser.add_argument('-lr','--learning_rate', type=float, default=1e-4, help='learning rate', required=False)\n",
    "parser.add_argument('-l2','--weight_decay', type=float, default=0., help='weight decay', required=False)\n",
    "parser.add_argument('-e_nh','--enc_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for encoder or list of hiddens for each layer', required=False)\n",
    "parser.add_argument('-e_nl','--enc_n_layer', type=int, default=1, help='# of hidden layers for encoder, set to -1 if passing list of n_hiddens', required=False)\n",
    "parser.add_argument('-e_nonlin','--enc_nonlinearity', type=str, default='sigmoid', help='type of nonlinearity for encoder', required=False)\n",
    "parser.add_argument('-e_weights','--enc_weights', type=str, default='', help='file path for encoder weights', required=False)\n",
    "parser.add_argument('-e_freeze','--enc_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the encoder weights', required=False)\n",
    "parser.add_argument('-lat_nonlin','--latent_nonlinearity', type=str, default='', help='type of to use prior to decoder', required=False)\n",
    "parser.add_argument('-d_nh','--dec_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for decoder or list of hiddens for each layer', required=False)\n",
    "parser.add_argument('-d_nl','--dec_n_layer', type=int, default=0, help='# of hidden layers for decoder', required=False)\n",
    "parser.add_argument('-d_nonlin','--dec_nonlinearity', type=str, default='', help='type of nonlinearity for decoder', required=False)\n",
    "parser.add_argument('-d_weights','--dec_weights', type=str, default='', help='file path for decoder weights', required=False)\n",
    "parser.add_argument('-d_freeze','--dec_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the decoder weights', required=False)\n",
    "parser.add_argument('-d_word_dist','--dec_word_dist', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to init decoder weights with training set word distributions', required=False)\n",
    "parser.add_argument('-dis_nh','--dis_n_hidden', type=int, nargs='+', default=[128], help='# of hidden units for encoder or list of hiddens for each layer', required=False)\n",
    "parser.add_argument('-dis_nl','--dis_n_layer', type=int, default=1, help='# of hidden layers for encoder, set to -1 if passing list of n_hiddens', required=False)\n",
    "parser.add_argument('-dis_nonlin','--dis_nonlinearity', type=str, default='sigmoid', help='type of nonlinearity for discriminator', required=False)\n",
    "parser.add_argument('-dis_y_weights','--dis_y_weights', type=str, default='', help='file path for discriminator_y weights', required=False)\n",
    "parser.add_argument('-dis_z_weights','--dis_z_weights', type=str, default='', help='file path for discriminator_z weights', required=False)\n",
    "parser.add_argument('-dis_freeze','--dis_freeze', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to freeze the encoder weights', required=False)\n",
    "parser.add_argument('-include_w','--include_weights', type=str, nargs='*', default=[], help='weights to train on (default is all weights) -- all others are kept fixed; Ex: E.z_encoder D.decoder', required=False)\n",
    "parser.add_argument('-eps','--epsilon', type=float, default=1e-8, help='epsilon param for Adam', required=False)\n",
    "parser.add_argument('-mx_it','--max_iter', type=int, default=50001, help='max # of training iterations', required=False)\n",
    "parser.add_argument('-train_stats_every','--train_stats_every', type=int, default=100, help='skip train_stats_every iterations between recording training stats', required=False)\n",
    "parser.add_argument('-eval_stats_every','--eval_stats_every', type=int, default=100, help='skip eval_stats_every iterations between recording evaluation stats', required=False)\n",
    "parser.add_argument('-ndim_y','--ndim_y', type=int, default=256, help='dimensionality of y - topic indicator', required=False)\n",
    "parser.add_argument('-ndim_x','--ndim_x', type=int, default=2, help='dimensionality of p(x) - data distribution', required=False)\n",
    "parser.add_argument('-saveto','--saveto', type=str, default='', help='path prefix for saving results', required=False)\n",
    "parser.add_argument('-gpu','--gpu', type=int, default=-2, help='if/which gpu to use (-1: all, -2: None)', required=False)\n",
    "parser.add_argument('-hybrid','--hybridize', type=lambda x: (str(x).lower() == 'true'), default=False, help='declaritive True (hybridize) or imperative False', required=False)\n",
    "parser.add_argument('-full_npmi','--full_npmi', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to compute NPMI for full trajectory', required=False)\n",
    "parser.add_argument('-eot','--eval_on_test', type=lambda x: (str(x).lower() == 'true'), default=False, help='whether to evaluate on the test set (True) or validation set (False)', required=False)\n",
    "parser.add_argument('-verb','--verbose', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to print progress to stdout', required=False)\n",
    "parser.add_argument('-dirich_alpha','--dirich_alpha', type=float, default=1e-1, help='param for Dirichlet prior', required=False)\n",
    "parser.add_argument('-adverse','--adverse', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to turn on adverserial training (MMD or GAN). set to False if only train auto-encoder', required=False)\n",
    "parser.add_argument('-update_enc','--update_enc', type=lambda x: (str(x).lower() == 'true'), default=True, help='whether to update encoder for unlabed_train_op()', required=False)\n",
    "parser.add_argument('-labeled_loss_lambda','--labeled_loss_lambda', type=float, default=1.0, help='param for Dirichlet noise for label', required=False)\n",
    "parser.add_argument('-train_mode','--train_mode', type=str, default='mmd', help=\"set to mmd or adv (for GAN)\", required=False)\n",
    "parser.add_argument('-kernel_alpha','--kernel_alpha', type=float, default=1.0, help='param for information diffusion kernel', required=False)\n",
    "parser.add_argument('-recon_alpha','--recon_alpha', type=float, default=-1.0, help='multiplier of the reconstruction loss when combined with mmd loss', required=False)\n",
    "parser.add_argument('-recon_alpha_adapt','--recon_alpha_adapt', type=float, default=-1.0, help='adaptively change recon_alpha so that [total loss = mmd + recon_alpha_adapt * recon loss], set to -1 if no adapt', required=False)\n",
    "parser.add_argument('-dropout_p','--dropout_p', type=float, default=-1.0, help='dropout probability in encoder', required=False)\n",
    "parser.add_argument('-l2_alpha','--l2_alpha', type=float, default=-1.0, help='alpha multipler for L2 regularization on latent vector', required=False)\n",
    "parser.add_argument('-latent_noise','--latent_noise', type=float, default=0.0, help='proportion of dirichlet noise added to the latent vector after softmax', required=False)\n",
    "parser.add_argument('-topic_decoder_weight','--topic_decoder_weight', type=lambda x: (str(x).lower() == 'true'), default=False, help='extract topic words based on decoder weights or decoder outputs', required=False)\n",
    "parser.add_argument('-retrain_enc_only','--retrain_enc_only', type=lambda x: (str(x).lower() == 'true'), default=False, help='only retrain the encoder for reconstruction loss', required=False)\n",
    "parser.add_argument('-l2_alpha_retrain','--l2_alpha_retrain', type=float, default=0.1, help='alpha multipler for L2 regularization on encoder output during retraining', required=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain : str = field(\n",
      "    default='twenty_news', metadata={\"help\": \"domain to run\"})\n",
      "data_path : str = field(\n",
      "    default='', metadata={\"help\": \"file path for dataset\"})\n",
      "max_labels : int = field(\n",
      "    default=100, metadata={\"help\": \"max number of topics to specify as labels for a single training document\"})\n",
      "max_labeled_samples : int = field(\n",
      "    default=10, metadata={\"help\": \"max number of labeled samples per topic\"})\n",
      "label_seed : bool = field(\n",
      "    default=None, metadata={\"help\": \"random seed for subsampling the labeled dataset\"})\n",
      "model : str = field(\n",
      "    default='dirichlet', metadata={\"help\": \"model to use\"})\n",
      "description : str = field(\n",
      "    default='', metadata={\"help\": \"description for the experiment\"})\n",
      "algorithm : str = field(\n",
      "    default='standard', metadata={\"help\": \"algorithm to use for training: standard\"})\n",
      "batch_size : int = field(\n",
      "    default=256, metadata={\"help\": \"batch_size for training\"})\n",
      "optim : str = field(\n",
      "    default='Adam', metadata={\"help\": \"encoder training algorithm\"})\n",
      "learning_rate : float = field(\n",
      "    default=1e-4, metadata={\"help\": \"learning rate\"})\n",
      "weight_decay : float = field(\n",
      "    default=0., metadata={\"help\": \"weight decay\"})\n",
      "enc_n_hidden : int = field(\n",
      "    default=[128], metadata={\"help\": \"# of hidden units for encoder or list of hiddens for each layer\"})\n",
      "enc_n_layer : int = field(\n",
      "    default=1, metadata={\"help\": \"# of hidden layers for encode\"})\n",
      "enc_nonlinearity : str = field(\n",
      "    default='sigmoid', metadata={\"help\": \"type of nonlinearity for encoder\"})\n",
      "enc_weights : str = field(\n",
      "    default='', metadata={\"help\": \"file path for encoder weights\"})\n",
      "enc_freeze : bool = field(\n",
      "    default=False, metadata={\"help\": \"whether to freeze the encoder weights\"})\n",
      "latent_nonlinearity : str = field(\n",
      "    default='', metadata={\"help\": \"type of to use prior to decoder\"})\n",
      "dec_n_hidden : int = field(\n",
      "    default=[128], metadata={\"help\": \"# of hidden units for decoder or list of hiddens for each layer\"})\n",
      "dec_n_layer : int = field(\n",
      "    default=0, metadata={\"help\": \"# of hidden layers for decoder\"})\n",
      "dec_nonlinearity : str = field(\n",
      "    default='', metadata={\"help\": \"type of nonlinearity for decoder\"})\n",
      "dec_weights : str = field(\n",
      "    default='', metadata={\"help\": \"file path for decoder weights\"})\n",
      "dec_freeze : bool = field(\n",
      "    default=False, metadata={\"help\": \"whether to freeze the decoder weights\"})\n",
      "dec_word_dist : bool = field(\n",
      "    default=False, metadata={\"help\": \"whether to init decoder weights with training set word distributions\"})\n",
      "dis_n_hidden : int = field(\n",
      "    default=[128], metadata={\"help\": \"# of hidden units for encoder or list of hiddens for each layer\"})\n",
      "dis_n_layer : int = field(\n",
      "    default=1, metadata={\"help\": \"# of hidden layers for encode\"})\n",
      "dis_nonlinearity : str = field(\n",
      "    default='sigmoid', metadata={\"help\": \"type of nonlinearity for discriminator\"})\n",
      "dis_y_weights : str = field(\n",
      "    default='', metadata={\"help\": \"file path for discriminator_y weights\"})\n",
      "dis_z_weights : str = field(\n",
      "    default='', metadata={\"help\": \"file path for discriminator_z weights\"})\n",
      "dis_freeze : bool = field(\n",
      "    default=False, metadata={\"help\": \"whether to freeze the encoder weights\"})\n",
      "include_weights : str = field(\n",
      "    default=[], metadata={\"help\": \"weights to train on (default is all weights) -- all others are kept fixed; Ex: E.z_encoder D.decoder\"})\n",
      "epsilon : float = field(\n",
      "    default=1e-8, metadata={\"help\": \"epsilon param for Adam\"})\n",
      "max_iter : int = field(\n",
      "    default=50001, metadata={\"help\": \"max # of training iterations\"})\n",
      "train_stats_every : int = field(\n",
      "    default=100, metadata={\"help\": \"skip train_stats_every iterations between recording training stats\"})\n",
      "eval_stats_every : int = field(\n",
      "    default=100, metadata={\"help\": \"skip eval_stats_every iterations between recording evaluation stats\"})\n",
      "ndim_y : int = field(\n",
      "    default=256, metadata={\"help\": \"dimensionality of y - topic indicator\"})\n",
      "ndim_x : int = field(\n",
      "    default=2, metadata={\"help\": \"dimensionality of p(x) - data distribution\"})\n",
      "saveto : str = field(\n",
      "    default='', metadata={\"help\": \"path prefix for saving results\"})\n",
      "gpu : int = field(\n",
      "    default=-2, metadata={\"help\": \"if/which gpu to use (-1: al\"})\n",
      "hybridize : bool = field(\n",
      "    default=False, metadata={\"help\": \"declaritive True (hybridize) or imperative False\"})\n",
      "full_npmi : bool = field(\n",
      "    default=False, metadata={\"help\": \"whether to compute NPMI for full trajectory\"})\n",
      "eval_on_test : bool = field(\n",
      "    default=False, metadata={\"help\": \"whether to evaluate on the test set (True) or validation set (False)\"})\n",
      "verbose : bool = field(\n",
      "    default=True, metadata={\"help\": \"whether to print progress to stdout\"})\n",
      "dirich_alpha : float = field(\n",
      "    default=1e-1, metadata={\"help\": \"param for Dirichlet prior\"})\n",
      "adverse : bool = field(\n",
      "    default=True, metadata={\"help\": \"whether to turn on adverserial training (MMD or GAN). set to False if only train auto-encoder\"})\n",
      "update_enc : bool = field(\n",
      "    default=True, metadata={\"help\": \"whether to update encoder for unlabed_train_op()\"})\n",
      "labeled_loss_lambda : float = field(\n",
      "    default=1.0, metadata={\"help\": \"param for Dirichlet noise for label\"})\n",
      "train_mode : str = field(\n",
      "    default='mmd', metadata={\"help\": \"set to mmd or adv (for GAN)\"})\n",
      "kernel_alpha : float = field(\n",
      "    default=1.0, metadata={\"help\": \"param for information diffusion kernel\"})\n",
      "recon_alpha : float = field(\n",
      "    default=-1.0, metadata={\"help\": \"multiplier of the reconstruction loss when combined with mmd loss\"})\n",
      "recon_alpha_adapt : float = field(\n",
      "    default=-1.0, metadata={\"help\": \"adaptively change recon_alpha so that [total loss = mmd + recon_alpha_adapt * recon loss\"})\n",
      "dropout_p : float = field(\n",
      "    default=-1.0, metadata={\"help\": \"dropout probability in encoder\"})\n",
      "l2_alpha : float = field(\n",
      "    default=-1.0, metadata={\"help\": \"alpha multipler for L2 regularization on latent vector\"})\n",
      "latent_noise : float = field(\n",
      "    default=0.0, metadata={\"help\": \"proportion of dirichlet noise added to the latent vector after softmax\"})\n",
      "topic_decoder_weight : bool = field(\n",
      "    default=False, metadata={\"help\": \"extract topic words based on decoder weights or decoder outputs\"})\n",
      "retrain_enc_only : bool = field(\n",
      "    default=False, metadata={\"help\": \"only retrain the encoder for reconstruction loss\"})\n",
      "l2_alpha_retrain : float = field(\n",
      "    default=0.1, metadata={\"help\": \"alpha multipler for L2 regularization on encoder output during retraining\"})\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for line in parse_argument.split(\"\\n\"):\n",
    "    arguments = \"\".join(line.split(\"add_argument(\")[1:])[:-1]\n",
    "    _, key = arguments.split(\",\")[0], arguments.split(\",\")[1]\n",
    "    type_ = arguments.split(\"type=\")[1].split(\",\")[0]\n",
    "    if \"lambda\" in type_:\n",
    "        type_ = \"bool\"\n",
    "    default = arguments.split(\"default=\")[1].split(\",\")[0]\n",
    "    help_msg = arguments.split(\"help=\")[1].split(\",\")[0]\n",
    "    \n",
    "    txt = key[3:-1] + f\" : {type_}\" + \" = field(\\n    \"\n",
    "    txt += f'default={default}, metadata={{\"help\": \"{help_msg[1:-1]}\"}})'\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수정한 arguments\n",
    "```\n",
    "label_seed\n",
    "e_nh\n",
    "e_freeze\n",
    "d_nh\n",
    "dis_nh\n",
    "include_w\n",
    "d_freeze\n",
    "d_word_dist\n",
    "hybrid\n",
    "full_npmi\n",
    "eot\n",
    "verb\n",
    "adverse\n",
    "update_enc\n",
    "topic_decoder_weight\n",
    "retrain_enc_only\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
